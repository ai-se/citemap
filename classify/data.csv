Conference|Year|Title|Authors|Keywords|Abstract|Category|Decision|Raw Decision
MSR|2015|Enhanced and more secure AODV routing protocol to avoid black hole attack in MANET|vijay chhari,rajesh singh,s dhakad|Mobile adhoc network component,DOS,single blcak hole attack,collaborative black hole attack|Wireless networks are gaining popularity now days, as the users require wireless connectivity irrespective of their geographic position. There is an increasing threat of attacks on MANETs. Black hole attack is a security threat in which the packet is redirected to a node that actually does not exist in the network. It’s an analogy to the black hole in the universe in which things disappear. In black hole attack malicious node uses its routing protocol in order to advertise itself for having the shortest path to the destination node or to the packet it wants to intercept. MANETs should have a secure way for transmitting packet or information over a network which is quite challenging and vital issue. In this paper, a review on different existing techniques for detection of black hole attacks with their defects is presented.|Research Paper|reject|desk reject
MSR|2015|Performance Analysis of Configured Clustered Network of Routing Information Protocol (RIP) under node and link failure|purvee kashyap,sandeep gupta,priya pathak|RIP,MANET,OPNET Modeler,Clustered Network,Adhoc networks,Protocols,Network Performance|In this study an attempt has been made to analyze and compare the compare the performance of the intra domain distance vector RIP routing information protocol using OPNET modeler with respect to increasing number of failed nodes and links in the network. In present study, a comparison of RIP protocol under clustered network has been made on the basis of throughput, delay and network load, by increasing number of failed nodes and links in the network. Simulation results are given to validate the efficiency of our architecture in detecting distortions and showing the deviations from conventional.|Research Paper|reject|desk reject
MSR|2015|An Empirical Study of End-user Programmers in the Computer Music Community|gregory burlet,abram hindle|end users,visual programming languages,software repository analysis,clone detection|Computer musicians are a community of end-user programmers who often use visual programming languages such as Max/MSP or Pure Data to realize their musical compositions. This research study conducts a multifaceted analysis of the software development practices of computer musicians when programming in these visual music-oriented languages. A statistical analysis of project metadata harvested from software repositories hosted on GitHub reveals that in comparison to the general population of software developers, computer musicians' repositories have less commits, less frequent commits, more commits on weekends, yet similar numbers of bug reports and similar numbers of contributing authors. Analysis of source code in these repositories reveals that the vast majority of code can be reconstructed from duplicate fragments. Finally, these results are corroborated by a survey of computer musicians and interviews with individuals in this end-user community. Based on this analysis and feedback from computer musicians we find that there are many avenues where software engineering can be applied to help aid this community of end-user programmers.|Research Paper|accept|accept
MSR|2015|A historical analysis of Debian package incompatibilities|tom mens,maelick claes,roberto di cosmo,jerome vouillon|package dependency,package incompatibility,empirical software engineering,software repository mining,historical analysis,software distribution,survival analysis|Users and developers of software distributions are often confronted with installation problems due to conflicting packages. A prototypical example of this are the Linux distributions such as Debian. Conflicts between packages have been studied under different points of view in the literature, in particular for the Debian operating system, but little is known about how these package conflicts evolve over time. This article present an extensive analysis of the evolution of package incompatibilities, spanning a decade of the life of the Debian stable and testing distributions for its most popular architecture, i386. Using the technique of survival analysis, this empirical study sheds some light on the origin and evolution of package incompatibilities, and provides the basis for building indicators that may be used to improve the quality of package-based distributions.|Research Paper|accept|accept
MSR|2015|Mining Recruiting Services Repositories for Software Engineering Job Requirements|jens ehlers|Software Engineering Skills,Job Requirements Study,Software Engineering Curricula|The demand of skilled software engineers by industry is higher than the supply from academic graduate programs. An effective software engineering (SE) education which fits the expertise needed by companies is striven for by both sides, academia and industry. Online job ads are adequate indicators of demanded skill sets for future SE professionals, who seek for building up a valuable profile in a competitive global job market. The contribution of this paper is an empirical study providing an evaluation of more than 80,000 job postings of leading recruiting services to identify which skills are currently essential. We cluster skills by category, quantify their demand, and show up trends in time, by region, and by industry branch. The results of this survey are of interest for educators developing new curricula, job seekers, and recruiters. Particularly, students can derive vital implications for a profitable specialization in their study program.|Practice Paper|reject|reject
MSR|2015|Does Code Structure Decay?|daniela steidl,florian deissenboeck|Software Evolution,Software Growth,Software Quality,Quality Decay,Empirical Studies,Mining History|Software systems evolve over time and are often maintained for decades. Without effective counter measures, it is generally assumed that their code quality gradually decays. In this paper, we revisit this claim with a more specific focus and ask: Does the code structure in terms of its organization in methods and files actually decay? We conduct an empirical case study mining the evolution of methods and files of nine Java open source systems to get a deeper understand how methods and files grow over time and to derive whether the overall code structure decays. Although revealing that modified methods and files do grow over time, we show that there is no trend of general structural decay as most software systems grow by adding new code instead of modifying existing code.|Research Paper|reject|reject
MSR|2015|BPMiner: Mining Developers' Behavior Patterns from Screen-Captured Task Videos|jing li,lingfeng bao,zhenchang xing,xin peng,xinyu wang,wenyun zhao|screen-captured task videos,HCI data,behavior pattern,computer vision,online search behavior,proof-of-concept prototype|Many user studies of software development use screen-capture software to record developers' behavior for post-mortem analysis. However, extracting behavioral patterns from screen-captured videos requires manual transcription and coding of videos, which is often tedious and error-prone. Automatically extracting Human-Computer Interaction (HCI) data from screen-captured videos and systematically analyzing behavioral data will help researchers analyze developers' behavior in software development more effectively and efficiently. In this paper, we present BPMiner, a novel behavior analysis approach to mine developers' behavior patterns from screen-captured videos using computer vision techniques and exploratory sequential pattern analysis. We have implemented a proof-of-concept prototype of BPMiner, and applied the BPMiner prototype to study the developers' online search behavior during software development. We conducted a case study to evaluate the BPMiner prototype with 29 hours of screen-captured task videos. The case study suggest that the BPMiner approach can open up new ways to study developers' behavior in software development.|Research Paper|reject|reject
MSR|2015|Issues faced during Requirement elicitation and management in offshore software development|aftab ahmed|Requirements Engineering,offshore RE,Stake holders,Requirement Change Management|Requirement elicitation is a difficult process when carried locally, even tough when done offshore. The issues and problems faced by requirement engineers in offshore offices include communication and collaboration problems due to distributed projects, cross functional stake holders, time zone and organizational boundaries. These problems lead to decision making, project execution power and small groups having stake holder’s needs. Change management in requirement engineering through continuous collaboration and relationship with the customer so requirements can be elicited and verified. In the last section a model of training will be proposed to overcome these challenges.|Research Paper|reject|desk reject
MSR|2015|Anvaya: An Algorithm and Case-Study on Improving the Goodness of Software Process Models generated by Mining Event-Log Data in Issue Tracking Systems|prerna juneja,divya kundra,ashish sureka|Issue Tracking Systems (ITS),Process Mining,Software Process Models,Clustering,Process Model Discovery|Issue Tracking Systems (ITS) such as Bugzilla can be viewed as Process Aware Information Systems (PAIS) generating event-logs during the life-cycle of a bug report. Process Mining consists of mining event logs generated from PAIS for process model discovery, conformance and enhancement. We apply process map discovery techniques to mine event trace data generated from ITS of open source Firefox browser project to generate and study process models. Bug life-cycle consists of diversity and variance. Therefore, the process models generated from the event-logs are spaghetti-like with large number of edges, inter-connections and nodes. Such models are complex to analyse and difficult to comprehend by a process analyst. We improve the Goodness (fitness and structural complexity) of the process models by splitting the event-log into homogeneous subsets by clustering structurally similar traces. We adapt the K-Medoid clustering algorithm with two different distance metrics: Longest Common Sub sequence (LCS) and Dynamic Time Warping (DTW). We evaluate the goodness of the process models generated from the clusters using complexity and fitness metrics. Process models generated after clustering have high degree of fitness and less structural complexity and thus are easier to comprehend compared with the process model generated from the entire event-log. We study back-forth & self-loops, bug reopening, and bottleneck in the clusters obtained and show that clustering enables better analysis. We also propose an algorithm to automate the clustering process -the algorithm takes as input the event log and returns the best cluster set.|Research Paper|reject|reject
MSR|2015|Predicting Defect Prone Modules in Web Applications|serdar bicer,banu diri|Defect prediction,Software metrics,Software quality|Simplest approach in software testing is exhaustive testing, i.e. testing all possibilities in a given code piece. However, that is impossible due to time and budget limitations. For this reason, software managers often use learning based oracles to predict defect proneness of their products. Software defect prediction techniques are used by project managers to efficiently allocate their precious resources in testing phase. Predicting defect proneness of software products has been an active research area in software engineering domain in recent years. Researchers have been using static code metrics, code churn metrics, developer networks, and module networks as inputs to their proposed models until now. However, domain specific characteristics of software has not been taken into account. In this research, we aimed to improve defect prediction performance for web applications by utilizing their characteristics.|Research Paper|reject|reject
MSR|2015|What’s up with refactoring? Mining refactoring discussions on Stack Overflow|alessandro murgia,daan janssens,javier perez,serge demeyer|Stack Overflow,Refactoring,Practitioners,Topic Modeling|Refactoring is a recommended practice in the software engineering domain. As a consequence practitioners commonly gather in online communities such as Stack Overflow to discuss and share opinions about refactoring. In this paper, we mine Stack Overflow’s discussions related to refactoring. We present the recurring topics, show the relevance of refactoring terms in informal discussions and evaluate the relevance of code snippets for understanding refactoring. Finally, we show the potential of practitioner’s buzz about refactoring tools.|Research Paper|reject|reject
MSR|2015|Cumulative Code Churn: Impact on Maintainability|csaba farago,peter hegedus,rudolf ferenc|Code churn,ISO/IEC 25010,Source code maintainability,Wilcoxon test|It has been shown already that the source code of software systems erodes during development, which results in high maintenance costs in the long term. But can we somehow narrow down where exactly this erosion happens? Is it possible to infer the future erosion based on past code changes? Do modifications performed on frequently changing code (i.e. high-churn code) have worse effect on software quality than those affecting less frequently modified code? In this study we investigated these questions and found that code churn indeed increases the pace of code erosion. We calculated cumulative code churn values and maintainability changes for every commit of three open-source and one proprietary software system. With the help of Wilcoxon rank test we compared the code churn values of the files in commits resulting maintainability increase with those of decreasing the maintainability. Two out of the four tests showed very strong (p-values 0.00081 and 0.00921), one of them strong (p-value 0.022) and one positive but not significant (p-value 0.16) results. These results support our preliminary assumption that modifying high-churn code is more likely to decrease the overall maintainability of a software system, which can be thought of as the generalization of the already known phenomena that code churn results in higher number of defects. This result could be utilized in identifying the "hot spots" of the source code from maintainability point of view. A possible IDE plug-in, which indicates the risk of decreasing the maintainability of the source code, could help the architect and warn the developers.|Research Paper|reject|reject
MSR|2015|Role Distribution and Evolution in Github Project Teams|zhongjie wang,dewayne e. perry|FLOSS projects,role distributions,role transformations,multi-role contributions|Globally-distributed development teams of FLOSS projects evolve rapidly. In a team, different roles make different contributions to the project, and it has been demonstrated that keeping a rational role distribution is of great significance to the potential growth and expansion of the project. To identify the underlying patterns of FLOSS project team structures and role transformations, we performed an empirical study on 89 popular Github projects, in which we analyzed the distribution of 9 roles and identified four clusters of FLOSS projects, each having distinct characteristics on team structure (characterized by the ratio of 9 roles) and role transformations (characterized by the occurring frequency, the duration and the Times of Activeness). The direct and indirect contributions that a developer makes in different roles of one project are accumulated and visualized to characterize the continuity of his or her participation. We found there are no obvious correlations among the contributions of multiple roles that a developer concurrently plays. We identified six co-contribution patterns when developers are concurrently dedicated to multiple projects, and a majority of developers contribute to a single project and a small number of developers contribute concurrently to at most two projects. This study should help FLOSS project managers gain a deep understanding of dynamic role distributions and role transformations in teams, so as to take initiatives to improve their team structures.|Practice Paper|reject|reject
MSR|2015|Development Emails Content Analyzer:  An Approach for Development Emails Sentences Classification|andrea di sorbo,sebastiano panichella,corrado visaggio,gerardo canfora,harald c. gall|Unstructured Data Mining,Development Emails,Natural Language Processing,Empirical Study|Communication among developers plays an important role in any successful software project. Developers rely on emails to exchange relevant information for accomplishing maintenance and development tasks. For this reason, past works proposed approaches to generate summaries of emails content and to link emails content with source code artifacts. However, all these works are not able to link the “intent” of emails with the corresponding source code artifacts. We argue that helping developers to discern the content of email messages that best fit his/her information need is a relevant task. In this paper, we define a set of sentences categories in the context of software development (namely, Feature Request, Opinion Asking, Problem Discovery, Solution Proposal, Information Seeking, and Information Giving) and propose an approach, based on Natural Language Processing, which automatically assigns each email sentence to one of these categories. An empirical study involving two open source projects, Qt and Ubuntu, shows that the proposed approach is able to correctly classify development emails content with a precision of 90% and recall of 70%, on average. The approach also outperforms Machine Learning strategies, previously used by other authors for classifying bug reports, in terms of both precision and recall. The proposed approach can be used for improving the performances of summarization strategies by selecting the more informative text sections (thus avoiding textual noise) in development messages.|Research Paper|reject|reject
MSR|2015|Packages Evolution of Open Source Enterprise Resource Planning Systems: An Empirical Study|muhammad waseem,muhammad usman,ikram asghar|Empirical study,software evolution,software maintenance,packages evolution|Software evolution is one of the key research area in software engineering. Previous studies have indicated that software may evolve from a different perspective with the passage of time for migrating and upgrading. It may evolve on architecture, design and code level for systems and subsystems. The objective of this study is to empirically investigate the trends of packages evolution in open source enterprise resource planning (ERPs) on system and subsystem level. We have answered the following three questions. First, do the growth patterns of open source ERPs are in conformance with the results of similar studies of other open source systems? Second, do the overall packages increase or decrease? Third, what are the dominating change factors in packages evolution? The results are based on empirical data from three open source ERPs which are adempiere, open bravo and apache OFBiz.  We extract the data in terms of added, deleted and modified packages through ‘JDiff’ by comparing subsequent releases. latter on data have been analyzed on the system and subsystem level. This result of our data demonstrate the major growth is limited to few subsystems, overall packages are increased and dominating change factor is modification of the  packages. The first two results are similar to the previous studies while the third one differs from them.|Practice Paper|reject|reject
MSR|2015|Code Growth of Industrial Software Systems: An Exploratory Study|magiel bruntink,eric bouwers|software growth,industrial data,software measurement|It is known that software projects need to evolve in order to stay relevant, and that the volume of code of evolving systems typically increases. An increase in volume requires additional resources for maintenance and evolution, which makes this property important for project owners. So what is a typical growth rate for a software project? And what factors influence this growth rate? While such data have been reported at scale for open-source software, data on industrial software is scarce. This paper aims to fill this gap by analyzing the growth data of 317 industrial software projects that have been monitored following a standardized process. The analysis of this dataset provides preliminary answers to the questions stated above. In addition, our data show that an existing growth rate metric, the Compound Annual Growth Rate, can result in under- and overestimations of growth, in particular in project years beyond the first.|Research Paper|reject|reject
MSR|2015|Using Developer-Interaction Trails to Triage Change Requests|motahareh bahrami zanjani,huzefa h. kagdi,christian bird|Bug triage,Developer recommendation,Mining interaction history|The paper presents an approach, namely iHDev, to recommend developers who are most likely to implement incoming change requests. The basic premise of iHDev is that the developers who interacted with the source code relevant to a given change request are most likely to best assist with its resolution. A machine-learning technique is first used to locate source-code entities relevant to the textual description of a given change request. iHDev then mines interaction trails (i.e., Mylyn sessions) associated with these source-code entities to recommend a ranked list of developers. iHDev integrates the interaction trails in a unique way to perform its task, which was not investigated previously. An empirical study on open source systems Mylyn and Eclipse Project was conducted to assess the effectiveness of iHDev. A number of change requests were used in the evaluated benchmark. Recall for top one to five recommended developers and Mean Reciprocal Rank (MRR) values are reported. Furthermore, a comparative study with two previous approaches that use commit histories and/or the source-code authorship information for developer recommendation was performed. Results show that iHDev could provide a recall gain of up to 127.27% with equivalent or improved MRR values by up to 112.5%.|Research Paper|reject|reject
MSR|2015|Co-evolution of infrastructure and Source Code - An Empirical Study|yujuan jiang,bram adams|infrastructure-as-code,OpenStack,configuration,co-evolution,change coupling,Puppet,Chef,bug,test|Infrastructure-as-code automates the process of configuring and setting up the environment (e.g., servers, VMs and databases) in which a software system will be tested and/or deployed, through textual specification files in a language like Puppet or Chef. Since the environment is instantiated automatically by the infrastructure languages’ tools, no manual intervention is necessary apart from maintaining the infrastructure specification files. The amount of work involved with such maintenance, as well as the size and complexity of infrastructure specification files, have not yet been studied empirically. Through an empirical study of the version control system of 265 OpenStack projects, we find that infrastructure files are large and churn frequently, which could indicate a potential of introducing bugs. Furthermore, we found that the infrastructure code files are coupled tightly with the other files in a project, especially test files, which implies that testers often need to change infrastructure specifications when making changes to the test framework and tests.|Research Paper|accept|accept
MSR|2015|Characteristics of Useful Code Reviews: An Empirical Study|amiangshu bosu,michaela greiler,christian bird|code review,effectiveness,usefulness,reviewer characteristics,modern code review|Over the past decade, both open source and commercial software projects have adopted contemporary peer code review practices as a quality control mechanism. Prior research has shown that developers spend a large amount of time and effort performing code reviews. Therefore, identifying factors that lead to useful code reviews can benefit projects by increasing code review effectiveness and quality. In a three-stage mixed research study, we qualitatively investigated what aspects of code reviews make them useful to developers, used our findings to build and verify a classification model that can distinguish between useful and not useful code review feedback, and finally we used this classifier to classify review comments enabling us to empirically investigate factors that lead to more effective code review feedback. In total, we analyzed 1.5 millions review comments from five Microsoft projects and uncovered many factors that affect the usefulness of review feedback. For example, we found that the proportion of useful comments made by a reviewer increases dramatically in the first year that he or she is at Microsoft but tends to plateau afterwards. In contrast, we found that the more files that are in a change, the lower the proportion of comments in the code review that will be of value to the author of the change. Based on our findings, we provide recommendations for practitioners to improve effectiveness of code reviews.|Research Paper|reject|reject
MSR|2015|A Look on GitHub Projects Attractiveness through Social Inferences. An Empirical Study|anton ryabchikov|GitHub,Contribution Attractiveness,Path Modeling|The last years have seen a shift of open source software (OSS) development towards social coding forges. Characterized by high transparency of activities and low contribution barriers, they foster competition among projects for the development resources. Projects striving for success in highly competitive environments need a better understanding of what signals facilitate their attractiveness for contributors. Although a number of recent works discusses community members’ inferences stimulated by various project-related signals, little has been done on quantitative exploration of the effect of these inferences on OSS project attractiveness. In this paper we bridge this gap by constructing a causal relationship model of project attractiveness and by verifying it through a case study of 2631 projects, extracted from GitHub social coding forge. We argue, based on model analysis, that project "attractiveness" is well represented by the set of visually perceptible project indicators - repository forks, notification subscribers, stargazers and pull requests. The results obtained empirically demonstrate statistically significant positive impact of exposed contribution opportunities (reflected by the number of issues and commit comments), development activity (the size of the development teams and their contributions) and project owner status (the number of followers) on GitHub project attractiveness for contributors. Our findings show that, compared to other factors, inferred contribution opportunities have the most prominent effect on the response construct.|Research Paper|reject|reject
MSR|2015|The App Sampling Problem for App Store Mining|william martin,mark harman,yue jia,federica sarro,yuanyuan zhang|app store analysis,review,data mining,sample bias,topic modelling,lda,app sampling problem|There has been much recent work on user review mining and analysis for requirements engineering. However, most of this work uses subsets of recent review data. We investigate the effects of sampling bias, and techniques for its amelioration in App Store Mining and Analysis, where sampling bias is often unavoidable. We mine 106,891 requests from 2,729,103 user reviews and investigate the properties of apps and reviews from 3 different partitions: the sets with fully complete review data, partially complete review data, and no review data at all. We find that app metrics such as price, rating, and download rank are significantly different between the three completeness levels. We show that correlation analysis can find trends in the data that prevail across the partitions, offering one possible approach to App Store Analysis in the presence of sampling bias.|Research Paper|accept|accept
MSR|2015|Python: Where the Mutants Hide|joshua campbell,abram hindle,jose nelson amaral|python,language model,n-grams,syntax errors,naturalness,parsing,testing,mutation|Dynamic scripting programming languages present a unique challenge to software engineering tools that depend on static analysis. Dynamic languages do not benefit from the full lexical and syntax analysis provided by compilers and static analysis tools. Prior work exploited a statically typed language (Java) and a simple n-gram language model to find syntax-error locations in programs. This work investigates whether n-gram- based error location on source code written in a dynamic language is effective without static analysis or compilation. Unnatural- Code.py is a syntax-error locator developed for the Python pro- gramming language. The UnnaturalCode.py approach is effective on Python code, but faces significantly more challenges than its Java counterpart did. UnnaturalCode.py generalizes the success of previous statically-typed approaches to a dynamically-typed language. This paper also presents a novel tool for random-edit mutation-based automatic exploration of program code variants. The random-edit mutation tool is used to analyze both the implementation of the Python language and UnnaturalCode.py.|Research Paper|reject|reject
MSR|2015|Contrasting exception handling code across languages: An analysis of GitHub repositories|benjamin jakobus,alessandro garcia,eiji adachi barbosa,carlos jose pereira de lucena|exception handling,programming languages,exceptions|Exception handling mechanisms have been introduced into programming languages in an effort to help deal with runtime irregularities. These mechanisms typically provide constructs for sectioning code into exception scopes (e.g. Java try blocks) and exception handlers (e.g. Java catch blocks). Whilst exception handling mechanisms have been the focus of much research over the past years, empirical studies have only focused on characterizing exception handling code of Java and C# programs. There exists little empirical evidence on the impact of various programming languages on exception handling code in software projects. Moreover, to date there exists no study which has examined the structure of exception scopes. We address this shortcoming by examining the differences and commonalities of both exception handlers and exception scopes across a wider range of languages. To this end, we analysed 50 software repositories, containing code developed in  C++, JavaScript, PHP, Java and C#. More than 9 million lines of code and over twenty thousand exception handling blocks were analyzed. Our findings show that the majority of exception handlers are very simplistic, regardless of language. However, our analysis also revealed that there exist differences in the frequency, structure and length of exception scopes and exception handling code across languages. This observation suggests that it is not always possible to generalize the results of past findings across projects developed in different languages.|Research Paper|reject|reject
MSR|2015|Multi-layer configuration errors - Empirical study on Wordpress|mohammed sayagh,bram adams|Multi-layer systems,Software configuration,Wordpress,Empirical study|Software can be adapted to different situations and platforms by changing its configuration values. However, incorrect configurations can lead to configuration errors that are hard to resolve, especially in the case of multi layer systems, where configuration choices in each layer might contradict each other or not be visible. This paper performs a case study on a multi-layer system consisting of Wordpress plugins, Wordpress, and php. Our analyses show that these layers use an important number of configuration options, which evolves across time. They also show that each layer uses a considerable percentage of configuration options defined by lower layers, and that a high percentage of Wordpress and php configuration options are used by different plugins at the same time.|Research Paper|reject|reject
MSR|2015|Extracting normative processes from open source software repositories - A case study of Python Enhancement Proposals (PEPs)|bastin tony roy savarimuthu,khanh hoa dam,daniel avery,aditya k. ghose|norms,normative process,norm mining,norm extraction,process mining|The process by which norms are initially suggested, discussed, supported and ratified to become a policies – the normative process – is often complex, but is crucial to the governing of open organizations like the Open Source Software Development (OSSD) communities. Individuals require a good understanding of the normative processes that exist in those communities to enable smoother collaboration across geographically distributed and diverse members. In many cases, normative processes are not explicitly prescribed, and even if they exist, do not reflect the current practice of an OSSD community. In this paper, we propose to mine archived project documentation to extract normative processes. We particularly focus on extracting the normative processes used in the Python development community to create Python Enhancement Proposals (PEPs). Our findings demonstrate how a process describing the developmental stages of PEPs made available to the community (the prescribed process) is different to the process extracted from archived data (the extracted process). Also, it demonstrates the need for making the fine-grained normative processes of the three types of PEPs available to the community. Insights from our analysis of the extracted normative processes can lead to interesting research on how the nature of normative processes affect the success and failure of an OSSD community.|Research Paper|reject|reject
MSR|2015|Why Power Laws? An Explanation from Fine-Grained Code Changes|zhongpeng lin,jim whitehead|power law,simulation,software evolution,self-organized criticality,preferential attachment,fine-grained code changes|Throughout the years, empirical studies have found power law distributions in various measures across many software systems. However, to this day, surprisingly little is known about how they are produced. We offer an explanation from the perspective of fine-grained code changes in this paper. A model based on preferential attachment and self-organized criticality is proposed to simulate software evolution. The experiment shows that the simulation is able to render power law distributions out of the fine-grained code changes, suggesting preferential attachment and self-organized criticality may be the underlying forces for the power law distributions in software systems.|Research Paper|accept|accept
MSR|2015|Homophily and Behavioral Influence in OSS Socio-Technical Networks|david kavaler,vladimir filkov|social networks,networks and behavior,network evolution,behavior evolution,knowledge flow,homophily,influence,apache,open source software,stochastic actor-oriented model,siena|Working with others and being mindful of others' work is part and parcel of software development, especially in Open Source Software (OSS) projects. Developers' social and technical activities in those project occur in context of  the similar activities of all their teammates, as they all change and evolve together. Dynamic socio-technical task networks can summarize such interleaved activities at a systemic level, thus enabling epidemiological studies of both the link changes and the behavioral effects. Static analysis approaches are frequently inadequate to study these networks as OSS projects change dramatically in both social and technical activity over time. On the other hand, ad-hoc dynamic approaches are often only loosely supported by theory and can yield misleading findings. Borrowing from social network analysis, here we adapt for our purposes the stochastic actor-oriented models for studying the evolution of network properties. This modeling technique provides methods to study the interplay between behavior influence and network architecture as they change over time. Using the R package SIENA we apply these models to the study of link evolution and file ownership and productivity behavior in three Apache Software Foundation projects. We find evidence of selection in link evolution in all projects. On the other hand, we find no evidence for the spread of influence of either ownership or productivity behaviors through the networks. This work illustrates the great potential of dynamical network models for studying influences affecting the software process.|Research Paper|reject|reject
MSR|2015|Characterization and prediction of issue-related risks in software projects|morakot choetikertikul,khanh hoa dam,truyen tran,aditya k. ghose|Risk management,Issue tracking system,Machine learning,Data mining|Identifying risks relevant to a software project and planning measures to deal with them are critical to the success of the project. Current practices in risk assessment mostly rely on high-level, generic guidance or the subjective judgements of experts. In this paper, we propose a novel approach to risk assessment using historical data associated with a software project. Specifically, our approach identifies patterns of past events that caused project delays, and uses this knowledge to identify risks in the current state of the project. A set of risk factors characterizing “risky” software tasks (in the form of issues) were extracted from five open source projects: Apache, Duraspace, JBoss, Moodle, and Spring. In addition, we performed feature selection using a sparse logistic regression model to select risk factors with good discriminative power. Based on these risk factors, we built predictive models to predict if an issue will cause a project delay. Our predictive models are able to predict both the risk impact (i.e. the extend of the delay) and the likelihood of a risk occurring. The evaluation results demonstrate the effectiveness of our predictive models, achieving on average 48%–81% precision, 23%–90% recall, 29%–71% F-measure, and 70%–92% Area Under the ROC Curve. Our predictive models also have low error rates: 0.39–0.75 for Macro- averaged Mean Cost-Error and and 0.7–1.2 for Macro-averaged Mean Absolute Error.|Research Paper|accept|accept
MSR|2015|Recommendations for Extract Method Refactoring Based on Machine Learning|ayaka imazato,yoshiki higo,keisuke hotta,shinji kusumoto|Refactoring,Extract Method,Machine Learning,Software Maintenance|Refactoring is a technique to alter internal structure of software without changing its external behavior. Refactoring is an important technique to improve maintainability of software, and developers often use this technique during development process. Previously, a number of research have focused on refactoring. Some of them proposed techniques to recommend refactoring opportunities for developers. Recommendation of refactoring opportunities helps to reduce developers' time and effort to find locations to be refactored. However, there are no specific criteria for developers to determine where they should refactor because the criteria differ from project to project and from developer to developer. In this study, we propose a technique to recommend refactoring opportunities in source code by using machine learning techniques. Machine learning techniques enables to recommend flexibly refactoring opportunities in accordance with the characteristics of target projects and developers. Our proposed technique learns information on the features of refactorings conducted in the past. Then, based on these information, it recommends refactoring opportunities in given source code for developers. To evaluate our proposed technique, we investigated four research questions with five open source projects. As a result, we confirmed that the proposed technique was able to recommend refactoring opportunities with high accuracy.|Research Paper|reject|reject
MSR|2015|Towards Extracting Requirement Models from Bug Repositories|daniel avery,khanh hoa dam,bastin tony roy savarimuthu,aditya k. ghose|mining bug reports,requirements mining,requirements interdependencies|Bug-tracking and issue-tracking systems contain insightful information about both functionalities (functional requirements) and qualities (non-functional requirements) of a software. These are in the form of reporting a requirement not being met, or requesting a change to existing requirement, or requesting to add requirements. Such information can be mined to extract a requirement model for a software which can be used to reason about duplications (as done by existing work on detection of duplicate bug reports), inconsistencies, and requirement interdependencies. This paper proposes a technique to extract requirements in the form of normative statements from issue/bug reports. This is a step towards formalizing and extracting a requirement model from issue/bug reports.|Research Paper|reject|reject
MSR|2015|Are Suggestions about Coupled Changes Interesting? - Software Repository Mining Case Study|jasmin ramadani,stefan wagner|Data Mining,Software Repositories,Coupled Changes,Interestingness|Software repositories include a large amount of information that can be used for bug fixing or maintenance. It can be hard to successfully perform these tasks if the developer does not have much experience in software development. We apply data mining to analyze the source code history and discover coupling between file changes. We investigate three repositories to find files that frequently change together to support the software developers during these tasks. We complement to the coupled files information coming from the version control system, the issue tracking system and the project documentation. In our study, we contrast our findings with feedback from the developers about how interesting our findings are for them. The study results show that small repositories are not suitable for successful analysis. Coupled changes and most of the additional attributes we present are interesting for developers with various level of experience and not only for the unexperienced. They also suggest other additional issues to be relevant, e.g. the context of the coupled changes and the way they are presented, which we did not cover in this study.|Research Paper|reject|reject
MSR|2015|Trend Analysis of Key Technologies on Community Question & Answering Sites|abhishek kumar singh,debasish sena,naresh kumar nagwani,sudhakar pandey|Trend Analysis,Fuzzy Trend Analysis,ARIMA,Community Question and Answering|In recent era, users have been increasingly sharing their questions and answers on community question answering sites. Most community question answering sites aim towards providing useful and relevant answers to the questions. These community sites are mainly used by software developers in the area of programming languages and key technologies of software development. Trend analysis of these community sites help in understanding the technological evolutions, its popularity and key features used in recent software development. Trend analysis also benefits software developers to understand recent technologies and their related problems faced by other users. A question in Q&A site mainly consists of three attributes namely title, body and tags. Tag is one of the very important attribute in a question present in a community question answering site. Tag provides the terms associated with a particular question and technology. In this work trend analysis is carried using time series analysis techniques over the tags mentioned for the questions. Time series analysis works on ordered sequence of values of a variable spaced equally in time intervals. In this paper ARIMA time series model and fuzzy time series model are used for trend analysis of the weekly data of key programming technologies such as c#, java, php and python. Primary findings indicate that the fuzzy time series analysis provides more accurate technology trends in comparison to the ARIMA time series model.|Research Paper|reject|reject
MSR|2015|Code Ownership and Software Quality:  A Replication Study|michaela greiler,kim sebastian herzig,jacek czerwonka|code ownership,software quality,bug prediction|In a traditional sense, ownership determines rights and duties in regard to an object, for example a property. The owner of source code usually refers to the person that invented the code. However, larger code artefacts, such as files, are usually composed by multiple engineers contributing to the entity over time through a series of changes. Frequently, the person with the highest contribution, e.g. the most number of code changes, is defined as the code owner and takes responsibility for it. Thus, code ownership relates to the knowledge engineers have about code. Lacking responsibility and knowledge about code can reduce code quality. In an earlier study, Bird et al. [1] showed that Windows binaries that lacked clear code ownership were more likely to be defect prone. However recommendations for large artefacts such as binaries are usually not actionable. E.g. changing the concept of binaries and refactoring them to ensure strong ownership would violate system architecture principles. A recent replication study by Foucault et al. [2] on open source software replicate the original results and lead to doubts about the general concept of ownership impacting code quality. In this paper, we replicated and extended the previous two ownership studies [1, 2] and reflect on their findings. Further, we define several new ownership metrics to investigate the dependency between ownership and code quality on file and directory level for 4 major Microsoft products. The results confirm the original findings by Bird et al. [1] that code ownership correlates with code quality. Using new and refined code ownership metrics we were able to classify source files that contained at least one bug with a median precision of 0.74 and a median recall of 0.38. On directory level, we achieve a precision of 0.76 and a recall of 0.60.|Practice Paper|accept|accept
MSR|2015|A Software Defect-Proneness Prediction Framework: A new approach using genetic algorithms to generate learning schemes|juan murillo-morera|Software metrics,Learning schemes,Evolutionary algorithms,Fault prediction models,Software quality|BACKGROUND: Recently, software defect prediction is an important research topic in the software engineering field. The demand for development of good quality software has seen a rapid growth in the last few years. The software measurement data collected during the software development process include valuable information about software projects status, progress, quality, performance, and evolution. The software fault prediction in the early phases of software development can help and guide software practitioners to focus the available testing resources on the weaker areas during the software development. The accurate prediction of defect prone software modules can help the software testing effort, reduce costs, and improve the software testing process by focusing on fault-prone module. OBJECTIVE: This research aims to propose a software defect-proneness prediction framework. The framework for software defect prediction was characterized by 1) Learning Scheme Generator using genetic algorithms 2) Propose models with quality and high performance according to metrics derived from the confusion matrix such as: balance and AUC. METHOD: The framework is comprised of 1) scheme learning generator. This component evaluates performance of the learning schemes and suggests the best option according to each data set analyzed, 2) defect predictor component builds models according to the evaluated learning scheme and predicts software defects with new data agreed to the constructed model. Twelve data sets was analyzed from NASA MDP repository. RESULTS: According to the results, there were different learning schemes for different data sets. The proposed framework is more effective and less prone to bias than previous approaches because it was followed through a computacional strategy com- bining different learning schemes but automatically through evolutionary programming. CONCLUSIONS: The framework has included more combinations of learning schemes than other proposals, which means that there are more possibilities to find better learning schemes for each data set. Finally, the better learning schemes were analyzed respect to the performance using balance and AUC metrics.|Research Paper|reject|reject
MSR|2015|Towards Correlating Search on Google and Asking on Stack Overflow|chunyang chen,zhenchang xing|Stack Overflow Tags,Google Trend,Correlation,Delay|Search engines and Community Question and Answer (CQA) sites are two common ways for people to seek information on the Internet. It is often suggested that the crowdsource knowledge in CQA sites can satisfy the information needs of web users searching about the knowledge. The assumption is that questions people ask on CQA sites and information people search using search engines overlap largely, and the knowledge accumulated in CQA sites continues to align with the information needs of web users searching about the knowledge. This paper reports our empirical study of the correlations of the 185 popular programming topics people search on Google and ask on Stack Overflow using query statistics from Google Trend over a 11 year span and posts from Stack Overflow data dump over a 5 year span. Our study shows that programming topics people search on Google and ask on Stack Overflow indeed align well over years. We also identify the differences between long-live techniques and emerging new techniques, and between general and specific programming topics. Inspired by our empirical results, we discuss applications that can harness Stack Overflow for studying the information needs of web users and enhancing search results.|Research Paper|reject|reject
MSR|2015|Predicting Software Field Reliability|pete rotella,sunita chulani,devesh goyal|software release reliability,prediction,modeling,testing,release quality,error analysis,customer experience|The objective of the work described is to accurately predict, as early as possible in the software lifecycle, how reliably a new software release will behave in the field. The initiative is based on a set of innovative mathematical models that have consistently shown a high correlation between key in-process metrics and our primary customer experience metric, SWDPMH (Software Defects per Million Hours [usage] per Month). We have focused on the three primary dimensions of testing – incoming, fixed, and backlog bugs. All of the key predictive metrics described here are empirically-derived, and in specific quantitative terms have not previously been documented in the software engineering/quality literature. A key part of this work is the empirical determination of the precision of the measurements of the primary predictive variables, and the determination of the prediction (outcome) error. These error values enable teams to accurately gauge bug finding and fixing progress, week by week, during the primary test period.|Practice Paper|reject|reject
MSR|2015|Which Algorithms Do We Need?: Mining TopCoder Problem Statements|nachai limsettho,shin fujiwara,hideaki hata,akito monden,ken-ichi matsumoto|Automated programming,Topic modeling,Data mining,classification,Software engineering|Abstract— Background: Understanding requirements is an essential process in software development. It is the first step toward automated programming, but also a big hurdle for many programmers. To challenge this research problem, we collected and analyzed more than 2,000 problem statements in TopCoder, the hosting service of competitive programming competitions, as complete and compact requirements. Aims: In this study we adopt NLP techniques to classify problem statements into categories of required algorithms, and to recommend previously solved similar problem statements to refer. We aim to make a system that provides the properties and characteristics of the target problem statements. Method: The previous problem statements are transformed into topic membership vectors, and then used to build classification models. In addition, a list of related problems is suggested using the distance in a topic space between the target and the previously solved problems statements.  Results: Our system can automatically categorize problem statements with reasonable performance (Hamming score 0.865 and Jaccard’s distance 0.808). In 79.89% of the list of top 5 related previous problem statements, there exists more than one correct problem statement. Conclusion: This system contributes to a small step forward in automated programming. It also has a practical use, aiding inexperienced programmers to access information and examples much easier than before, which contributes to improving their working performance.|Research Paper|reject|reject
MSR|2015|Gitana: a SQL-based Git Repository Inspector|valerio cosentino,javier luis canovas izquierdo,jordi cabot|Git,SQL,Source Control Management system,Relational database|Source code repositories are often managed using Source Control Management (SCM) systems. While they efficiently track the evolution of the code in the repository, they lack browsing and querying mechanisms beyond basic command line support. In particular, this is the case for Git, one of the most popular SCM systems. Despite its popularity, there exist few tools that complement Git with query functionalities and those that exist only focus on providing predefined statistics that cannot be tailored to the specific users' needs. In this paper, we propose an approach that, given a Git repository, exports (and digests) its data to a relational database in order to ease browsing and querying activities with standard SQL syntax and tools (familiar to a broader audience). To ensure efficiency, an incremental propagation mechanism refreshes the database content with the latest modifications. We also provide a JSON exporter to facilitate the analysis of the repository data with other technologies. We have implemented our approach in Gitana, an open-source tool available on GitHub. Gitana has been evaluated through a case study on several open-source projects.|Research Paper|reject|reject
MSR|2015|Studying the Needed Effort for Identifying Duplicate Issues|mohamed sami rakha,weiyi shang,ahmed e. hassan|Software Issue Reports,Duplicate Issues Detection,Mining Software Repositories|A considerable amount of software engineering research has examined duplicate issue reports. Thus far, duplicate reports have been considered as a hindrance to developers and a drain on their resources. As a result, prior research in this area has focused on proposing automated approaches to accurately identify duplicate reports. However, manual inspection of duplicate reports remains the norm in practice. To the best of our knowledge, there exists no prior studies that study the effort that is needed for identifying duplicate issue reports. In this paper, we empirically study the effort that is needed for manually-identifying duplicate reports in four open source projects, i.e., Firefox, SeaMonkey, Bugzilla and Eclipse-Platform. Our results show that: (i) More than 50% of the duplicate reports are identified within half a day. Most of the duplicate issue reports are identified without any discussion and with the involvement of very few people; (ii) A classification model built using a set of factors extracted from  duplicate issue reports classifies duplicates  according to the effort that is needed to identify them with a precision of 0.59 to 0.76, a recall of 0.22  to 0.96, and an ROC area of 0.68 to 0.80; and (iii) Factors that capture the developer awareness about the duplicate issue's peers (i.e, other duplicates of that issue) and textual similarity of a new report to prior reports are the most influential factors in our models. Our findings highlight the need for effort aware evaluation for approaches that are built for identifying duplicate issue reports, since the identification of a considerable number of duplicates (over 50%) appears to be a relatively trivial task for developers.|Research Paper|reject|reject
MSR|2015|Combining aspect mining techniques in two levels of granularity: a case study in source code analysis|ingrid marcal,rogerio garcia,danilo eler,celso olivete junior,ronaldo correia|Aspect mining,Crosscutting Concern,Software Repository,Combining techniques|Aspect mining aims to identify crosscutting concerns in source code, some mining techniques focus on specific data from software repository. Several techniques have been proposed and show good results, but the identification of false positive is still a problem, since it decrease their precision. In addition, despite large number of mining tools and techniques in literature, few researches explore their combination to improve and refine their results. In this paper we combine two aspect mining techniques of different granularity level: the first one at file level (Commit Frequency Analysis - CFA), the second one at methods level (Fan-in analysis). The CFA restricts the set of methods to be inspected by Fan-in analysis to those methods implemented in classes with greater chances to contain crosscutting concerns. We conducted experiments to assess the effectiveness of our approach on mining a specific type of crosscutting concern (Consistent Behavior). The experiments were conducted using the JHotDraw 6 software repository. We show that combining CFA and Fan-in analysis decreases the number of false-positives for the Consistent Behavior crosscutting concern, improving the precision, and reducing the effort to analyze the mining results manually.|Research Paper|reject|reject
MSR|2015|Organizational volatility and post-release defects: A replication case study using data from Google Chrome|samuel mugnaini donadelli,yue cai zhu,peter c. rigby|Replication,Software Quality,Turnover|The quality of software projects is affected by developer turnover.  Mockus studied organizational volatility in the context a large switching software project at Avaya. We replicate his model of the impact of organizational volatility on post-release defects.  At the time of Mockus's study, Avaya was experimenting with outsourcing and layoffs were prevalent. In contrast, we study volatility on the Chrome web-browser, which is growing rapidly in terms of popularity and team size.  Where possible, we use the same factors as Mockus: the number of co-owners, the number of developers joining and leaving the organization, the number of co-changing directories, developer experience and, instead of LOCs, the churn.  Our investigation is conducted at the directory instead of the file level.  The control variables, including churn, number of co-owners, and expertise all conform with the consensus in the literature that more changes, more co-owners, and lower expertise lead to an increase in customer reported post-release defects.  After normalizing by the highly correlated number of co-owners, the number of developers who leave and join both reduce the number of post-release defects.  We discuss this unexpected result.|Research Paper|accept|accept
MSR|2015|A preliminary examination of dormant files on software projects|samuel mugnaini donadelli,yue cai zhu,peter c. rigby|Software quality assurance,Legacy systems,Software evolution|Code that does not evolve becomes a hidden threat to the quality of a system.  In this paper, we study the influence of files that have not changed for more than one year (dormant files). We examine two projects: Evolution, a legacy system with more than 15 years of history and, Google Chrome, with 6 years of history. We find that Evolution has a high number of dormant files and that dormant files tend to be larger than active files.  Chrome, in contrast, has relatively few dormant files and dormant files tend to be smaller than active files. We also find that developers who take over dormant files on both systems are experienced developers and not newcomers to the project.  Since they are experienced, we suspect that it may be easier for them to understand these dormant files.  We observe that when a file has not been changed in the last 2 to 4 months, it is more likely to become dormant. We find that half of the dormant files remain dormant while the other half become active. The period of time for a dormant file to become active is 457 days for Chrome and 623 days for Evolution. Bugs lurk in 5% and 17% of dormant files for Chrome and Evolution, respectively. On Chrome active files contain many more bugs, however, for Evolution, almost half of the bugs are contained in dormant files.|Research Paper|reject|reject
MSR|2015|Automatically Mining Negative Code Examples from Software Developer Q & A Forums|ryan serva,zachary r. senzer,lori l. pollock,k. vijay-shanker|mining negative code examples,sentiment analysis,question and answer forums|In addition to learning good practices and reusing code from mining code examples, programmers can be supported in their learning and code improvement processes through negative, or poorly written, code examples. While it is challenging to identify negative code examples automatically from within source code, we leverage a key insight that the natural language in questions that include code examples posted on forums can provide adequate clues. In this paper, we describe a sentiment analysis-based technique for mining negative code examples from developer question and answer forums along with a technique to automatically mine negative sentiment indicators commonly used by developers, which are used to drive the sentiment-based technique.|Research Paper|reject|reject
MSR|2015|Unveiling Exception Handling Bug Hazards in Android based on GitHub and Google Code Issues|roberta coelho,lucas almeida,georgios gousios,arie van deursen|Exception Handling,Android Platform,Repository Mining,Google Code,GitHub|This paper reports on a study mining the exception stack traces included in 159,048 issues reported on Android projects hosted in GitHub (482 projects) and Google Code (157 projects). The goal of this study is to investigate whether stack trace information can reveal bug hazards related to exception handling code that may lead to a decrease in application robustness. Overall 6,005 exception stack traces were extracted, and subjected to source code and bytecode analysis. The outcomes of this study include the identification of the following bug hazards: (i) unexpected cross-type exception wrappings (for instance, trying to handle an instance of OutOfMemoryError ``hidden'' in a checked exception) which can make the exception-related code more complex and negatively impact the application robustness; (ii) undocumented runtime exceptions thrown by both the Android platform and third party libraries; and (iii) undocumented checked exceptions thrown by the Android Platform. Such undocumented exceptions make difficult, and most of the times infeasible for the client code to protect against ``unforeseen'' situations that may happen while calling third-party code. This study provides further insights on such bug hazards and the robustness threats they impose to Android apps as well as to other systems based on the Java exception model.|Research Paper|accept|accept
MSR|2015|On Selection of the Number of Topics for the Latent Dirichlet Allocation Model|andriy miranskyy,denise woit|LDA,Topic analysis,Topic selection,Text mining|Context: Extraction of topics from text corpuses helps improve Software Engineering processes. Latent Dirichlet Allocation (LDA) is often used to perform such analysis. However, calibration of the models is computationally expensive, especially if iterating over a large number of topics. Our goal is to create a simple formula allowing analysts to estimate the number of topics, so that the top X topics include the desired proportion of documents under study. Method: We derived the formula from the empirical analysis of two SE-related text corpuses. Results: We show that this simple power law based formula can be used to estimate the number of topics with high accuracy (root-mean-square error ranges from 0.008 to 0.1). Conclusions: We believe that practitioners can use our formula to expedite LDA analysis. The formula is also of interest to theoreticians, as it suggests that different SE text corpuses have similar underlying properties.|Research Paper|reject|reject
MSR|2015|Toward Reusing Code Changes|yoshiki higo,akio ohtani,shinpei hayashi,hideaki hata,shinji kusumoto|Change reuse,Source Code Analysis,Code clone|Existing techniques have succeeded to help developers implement new code.  However, they are insufficient to help them change existing code.  Previous studies have proposed techniques to support bug fixes but other kinds of code changes such as function enhancement and refactoring are not supported by them.  In this paper, we propose a novel system that helps developers change existing code.  Unlike existing techniques, our system can support any kinds of code changes if similar code changes occurred in the past.  Our research is still on very early stage and we have not have any implementation or any prototype yet.  This paper introduces our research purpose, an outline of our system, and how our system is different from existing techniques.|Research Paper|accept|accept
MSR|2015|Automatic Visualization of Data Dynamics on Mobile Apps via User Reviews|cuiyun gao,baoxiang wang,yangfan zhou,michael r. lyu|mobile apps,dynamics,unsupervised method,user reviews|User reviews on mobile apps are a wealth of valuable repositories for the developers and have been extensively studied in academia recently. Similar to other types of software repositories, the history of app reviews could directly facilitate the bug-fixing or feature-designing process for app development. Previous studies were mostly focused on extracting static topics and ranking reviews using the whole dataset collected. However, mobile apps are basically utility-driven with the characteristics of strong timeliness. User reviews are typically effective in a specific period of time. In this paper, we extract the principal topics of user reviews from a dynamic perspective and visualize the dynamics based on ThemeRiver, a tool for analyzing data with time. As far as we know, this paper is the first to visualize dynamic topics of user reviews on mobile apps. Moreover, the existing research tasks usually demand researchers an immense amount of time and efforts to interpret the extracted topics. In our framework, we automate the topic-labeling process by employing an optimization method and a neural network method respectively. The generated labels are fed to the visualization process. To verify the effectiveness of our framework, we conduct experiments on four popular apps and the results demonstrate the consistency of our approach with respect to the reality.|Research Paper|reject|reject
MSR|2015|Inferring Source Code Identifier Naming Styles Using Hidden Markov Model|latifa guerrouj,sebastiano panichella,giuliano antoniol,foutse khomh,yann-gael gueheneuc|Source code lexicon,Identifier styles,Hidden Markov Models,Divergence,Context.|Recent and past studies indicate that source code lexicon plays an important role in program comprehension and software quality. One specific aspect of source code lexicon is the style used to name various program elements; developers often use different styles of identifiers (i.e., name of methods, variables, attributes, etc.) when writing the source code of software systems, for example making use of abbreviations and acronyms or their concatenation using different kind of separators (e.g., CamelCase, underscore, etc). In some cases, the style being used by a developer adheres to the project’s overall style, while other developers may bring their own style of coding identifiers based on their knowledge and experience. This paper proposes the use of Hidden Markov Models (HMM), to infer naming styles used for methods, variables, attributes, and parameters. Then, it investigates whether the identifier naming style of a specific developer deviates from the overall project identifier coding style. A study conducted on the history of three open-source projects: PostgreSQL, ANT, and Hibernate highlighted that different program elements are written using different naming styles by developers who either adhere to or diverge from the overall naming style of the project. A qualitative analysis performed on the change logs of diverging developers show that developers mainly diverge when adding new features, removing, or enhancing existing ones. Divergence happens also, but less frequently, in the context of bug-fixing activities, renaming, and (re)documentation tasks.|Research Paper|reject|reject
MSR|2015|An Empirical Study on the Energy Impact of Logging for Mobile Applications|silvia di nardo,abram hindle,zhen ming jiang|execution logs,software evolution,dynamic analysis,energy|Execution-logs are debug statements that developers insert into their code. Execution-logs play an important role in monitoring and diagnosing the health of software applications. Logging uses resources and can have an impact on application performance: excessive logging could induce slower execution. Mobile platforms are limited by their battery energy resources, thus understanding the energy consumption of logging is more relevant to mobile than desktop computing.  Unfortunately, there are no works devoted to studying the impact of logging on mobile applications. An empirical study investigating the energy cost of logging in Android applications was performed using Green Miner, an automated energy test-bed for mobile applications. More than 200 versions of four popular open source Android applications (Calculator, FeedEx, Firefox and VLC) were tested under logging enabled and disabled. In addition, a controlled experiment was performed with varying rate of logging and size of log messages. Each of the experiment was conducted multiple times to ensure rigorous measurement. The total amount of testing was more than 250 hours. This studies found that infrequent logging does not have a large impact on energy consumption. Heavy logging impacts energy consumption: the rate of logging and disk flushes were two statistically significant factors of energy consumption attributable to logging. System and OS level logs are highly correlated with the energy consumption of mobile applications, whereas application level logs are not. The log behaviour of existing applications can be used to predict energy consumption in other applications.|Research Paper|reject|reject
MSR|2015|Ecosystems in GitHub and a Method for Ecosystem Identification using Reference Coupling|kelly blincoe,francis harrison,daniela e. herlea|Software Ecosystems,Technical Dependencies,Reference Coupling,GitHub|Software projects are not developed in isolation, and recent research has shifted to studying software ecosystems, communities of projects that depend on each other and are developed together. However, challenges arise when trying to identify technical dependencies at the ecosystem level. In this paper, we propose a new method for detecting technical dependencies between projects known as reference coupling which are established through user-specified cross-references between projects. We use our method to identify ecosystems in GitHub-hosted projects and identify several characteristics of the identified ecosystems. We find that most ecosystems are centered around one project and are interconnected with other ecosystems. The predominant type of ecosystems are those that develop tools to support software development. We also found that the project owners’ social behaviour aligns well with the technical dependencies within the ecosystem, but project contributors’ social behaviour does not align with these dependencies. We conclude with a discussion on future research that is enabled by our reference coupling method.|Research Paper|accept|accept
MSR|2015|A Method to Detect License Inconsistencies in Large-Scale Open Source Projects|yuhao wu,yuki manabe,tetsuya kanda,daniel m. german,katsuro inoue|Free open source software,Software licenses,License inconsistency,Code clone,Empirical study|The reuse of free and open source software (FOSS) components is becoming more and more popular. They usually contain one or more software licenses describing the requirements and conditions which should be followed when been reused. Licenses are usually written in the header of source code files as program comments. Removing or modifying the license header by re-distributors will result in the inconsistency of license with its ancestor, and may potentially cause license infringement. But to the best of our knowledge, no research has been devoted to investigate such kind of license infringements nor license inconsistencies. In this paper, we describe and categorize different types of license inconsistencies and propose a feasible method to detect them. Then we apply this method to Debian 7.5 and present the license inconsistencies found in it. With a manual analysis, we summarized various reasons behind these license inconsistencies, some of which imply license infringement and require the attention from the developers. This analysis also exposes the difficulty to discover license infringements, highlighting the usefulness of finding and maintaining source code provenance.|Research Paper|accept|accept
MSR|2015|Investigating the Quality of Code Review: Why Do Reviewers Miss Bugs?|oleksii kononenko,yaxin cao,latifa guerrouj,olga baysal|code review,code review quality,bug-inducing changes,mining software repositories,Mozilla,empirical study|Code review is an essential element of any mature software development project; it aims at evaluating code contributions submitted by developers. In principle, code review should improve the quality of code changes (patches) before they are committed to the project’s master repository. In practice, the execution of this process can allow bugs to get in unnoticed. In this paper, we report an empirical study investigating code review of a large open source project. We explore the relationship between reviewers’ code inspections and factors, both personal and temporal in nature, that might affect the quality of such inspections.We applied the SZZ algorithm to detect bug-inducing changes that were then linked to the code review information extracted from the issue tracking system. We found that the reasons why reviewers miss bugs are related to both their personal characteristics, as well as the technical properties of the patches under review.|Research Paper|reject|reject
MSR|2015|Annotations as maintenance tasks in similarity management|thomas schmorleiz,ralf lammel|Variability,Similarity management,Software product lines,Clone detection,Annotation,Change propagation|The notion of mining software repositories is instantiated in the sense of analyzing and managing similarities for collections of software variants that evolve over time. Variants may have been created by cloning and may have evolved independently afterwards (‘clone and own’). We present an analysis that determines how similarities between variants have evolved over time, thereby possibly identifying code changes that should be propagated. Similarities or their evolutions can be annotated by a developer to express maintenance tasks such as ‘Restore Equality’ or ‘Increase Similarity’. We present a complete process that integrates analysis, code changes, version control, annotation, automated change propagation, and manual actions for similarity improvement. We apply the resulting approach in a case study of 36 Haskell-based variants for a simple human-resources management system and reveal and eliminate several hundred cases of unintended reduction of sharing or similarity.|Research Paper|reject|reject
MSR|2015|An enhanced Graph-based infrastructure for Software Search Engines|colin atkinson,marcus schumacher|code search engines,index structure,Graph-based infrastructure|The first generation of software search engines such as Merobase, Sourcerer etc. showed that it is possible to support reasonably sophisticated searches over large bodies of software components using indices based on full-text search engines (most commonly Lucene). However, the “tricks” these engines use to map code structure to flat text are not only inflexible, they do not scale well to components composed of multiple program modules (e.g. interfaces, classes etc.) As a result, beyond plain string matching, they are only able to support a limited and a priori fixed set of query types, and are rarely, if ever able, to find components composed of more than one code module. In this paper we present an index representation approach which is able to support the key information bound up in source code in a more accurate, flexible way, and thus efficiently support a much wider range of searches on components composed of multiple program modules.|Research Paper|accept|accept
MSR|2015|Enhancing Branch Prediction using Software Evolution|saikat dutta,moumita das,ansuman banerjee|Branch Prediction,Software Evolution,Program Slicing,Weakest Precondition|Software evolution has been extensively studied in the past decade for various properties and interesting patterns. In this work, we study the effect of  evolution on branch prediction techniques. Typically for any program, at the hardware level, all dynamic branch prediction strategies, learn the branch behaviors at run time and later re-use them to predict the direction of future branches. The duration of the learning curve depends heavily on the kind of technique used and also the complexity of the program at hand. We propose that saving the branch outcome profile from an older version and reusing it in a new version can significantly reduce this overhead and improve performance. In this paper, we discuss the effect of program evolution on the performance of branch prediction, study how the individual branches get affected during evolution, suggest several new methods to reuse the branch behavior information from a previous version on similar as well as different test cases, and share our results on various software repositories. Preliminary results indicate our intuitions are well justified.|Research Paper|reject|reject
MSR|2015|Finding documentation bugs with static analysis|maria kechagia,diomidis spinellis|API,documentation,stack traces,exception handling,software quality,debugging|Application programming interfaces (APIs) can be evaluated in terms of their documentation and their underlying implementation. Although there are many metrics and techniques to evaluate software quality, there is scant work about the impact that APIs’ documentation has on client applications’ in-use quality, such as execution failures—crashes. We examine the source code and documentation of the Android API to determine inconsistencies between exceptions that static analysis can find in the source code and exceptions that exist in the documentation. We argue that some undocumented exceptions can affect the in-use quality of client applications and show associated examples. To support our conjecture, we conduct an empirical study by using a set of stack traces from Android application crashes. Our results show that indeed some undocumented exceptions that static analysis can find in the source code can decrease the in-use quality of client applications. In addition, we observe that exceptions that are highly specific (e.g. FileNotFoundException, ArrayIndexOutOfBoundsException), in contrast to their more general parent classes (Exception, RuntimeException), are not a major cause of reduced in-use quality of client applications. Finally, we find that from the manifested exceptions in a stack trace one can distinguish a crash caused by an API implementation error from a crash caused by an application programming error.|Research Paper|reject|reject
MSR|2015|Global vs. Local Models for Cross-project Defect Predicton|steffen herbold,alexander trautsch,jens grabowski|local models,defect prediction,cross-project|Altough significant effort has been spent by reseachers, the performance of defect prediction in a cross-project setting, i.e. with data that does not come from the same project context, is still unsatisfactory. A recent proposal for the improvement of defect predictions are local models. With local models, the available data is first clustered into homogenuous regions and afterwards separate predictors are trained for each homogenuous region. Since the main problem of cross-project defect prediction is data heterogenity, the idea of local models is promising. However, the past studies only focused on the within-project context and did not yet consider the cross-project context. Within this paper, we apply local models for cross-project defect prediction. In a large case study, we evalute the performance of local models and evaluate their advantages and drawbacks for cross-project predictions.|Research Paper|reject|reject
MSR|2015|Partitioning Composite Code Changes to Facilitate Code Review|yida tao,sunghun kim|Software code changes,Code review,Code analysis|Developers expend significant effort on reviewing source code changes, hence the comprehensibility of code changes directly affects development productivity. Our recent study has suggested that composite code change, which mix multiple development issues together, are typically difficult to review. Unfortunately, our manual inspection of 453 open source code changes reveals a non-trivial occurrence (up to 29%) of such composite changes. In this paper, we propose a heuristic-based approach to automatically partition composite changes, such that each sub-changes in the partition is more cohesive and self-contained. Our quantitative and qualitative evaluation results are promising in demonstrating the potential benefits of our approach for facilitating the code review process of composite code changes.|Research Paper|accept|accept
MSR|2015|The More the Merrier? Assessing the Effect of Multiple Developers and Revisions on Software  Artefacts|marco moscatiello,andrea capiluppi,lerina aversano,maria tortorella|Software processes,Change patterns,Structural quality,Open Source development|The types of interaction between software developers have been extensively studied, as well as evaluating the stability, quality and reusability of source code using metrics. The objective of this research is to add a third dimension to  the debate, and to compare the structural attributes of code resulting from collaborating authors, with the same attributes developed by single developers. To achieve this comparison, we propose a case study for which all the atomic revisions of a Java software system are analysed; the number of developers, per revision and OO class, is also analysed, to define whether each revision is single- or multi-authored. We cluster the source code in either: (i) single-authored with one revision; (ii) single-authored with many revisions; or (iii) multi-authored with many revisions. We found that the attributes of the OO classes change based on the number of authors working on them, and on the number of revisions: classes modified just once are in general smaller than those modified several times and by more authors. More importantly, the presence of more developers is related to a higher chance to observe fluctuations in the structural metrics: what's more important, not always classes modified by more developers show more structural quality.|Practice Paper|reject|reject
MSR|2015|Matching GitHub developer profiles to job advertisments|claudia hauff,georgios gousios|social coding,github,job recommendation|GitHub is a social coding platform that enables developers to efficiently work on projects, connect with other developers, collaborate and generally ``be seen'' by the community. This visibility also extends to prospective employers and HR personel who may use GitHub to learn more about a developer's skills and interests. We propose a pipeline that automatizes this process and automatically suggests matching job advertisements to developers, based on signals extracting from their activities on GitHub.|Research Paper|accept|accept
MSR|2015|Classification and theme detection for Github software repositories|konstantin tyapochkin,sergey nikolenko,alexey smirnov|GitHub,repository classification,repository theme detection,mining software repositories|Github is the most popular software repository hosting, containing millions of repositories. The git repositories on Github are supplemented with textual information such as repository descriptions and readme files. In this work, we tackle the problem of automatically classifying repositories with respect to their main themes (e.g., machine learning, gamedev, or bioinformatics) and technologies used in the repository (e.g. adobe flash, google api or video processing). In this study we propose a method for detecting key words and word combinations from repository descriptions at Github automatically, without a labeled data set and using expert evaluation only for already detected keywords. This method allows to classify repositories to a hierarchy of several wide themes further subdivided into numerous more narrow themes, understand which repositories are created only for testing and self-education purposes, and uncover popular currently trending areas of development.|Research Paper|reject|reject
MSR|2015|Summarizing Complex Development Artifacts by Mining Heterogenous Data|luca ponzanelli,andrea mocci,michele lanza|summarization,heterogeneous data,stack overflow|Summarization is hailed as a promising approach to reduce the amount of information that must be taken in by the person who wants to understand development artifacts, such as pieces of code, bug reports, emails, etc. However, existing approaches treat artifacts as pure textual entities, disregarding the heterogeneous and partially structured nature of most artifacts, which contain intertwined pieces of distinct type, such as source code, diffs, stack traces, human language, etc. We present a novel approach to augment existing summarization techniques (such as LexRank) to deal with the heterogeneous and multidimensional nature of complex artifacts. Our preliminary results on heterogeneous artifacts suggest our approach outperforms the current text-based approaches.|Research Paper|accept|accept
MSR|2015|Software Remodularization Recommenders: Are we Going in the Right Direction?|ivan candela,gabriele bavota,barbara russo,rocco oliveto|software remodularization,empirical study,recommenders|Design erosion plagues a lot of software systems. Refactoring, and in particular, remodularization operations can be performed to repair the design of a software system and remove the erosion caused by software evolution. Various approaches have been proposed to support developers during the remodularization of a software system. Such approaches are based on the underlying assumption that developers pursue an optimal balance between quality metrics, such as cohesion and coupling. Thus, a remodularization recommender proposes a solution that implicitly provides a (near) optimal balance between such quality metrics. However, there is still a lack of empirical evidence that such a balance is actually pursued by developers when designing and evolving their software systems. This paper presents a large study that aims at finding empirical evidence of the assumption  which remodularization approaches proposed so far are based on. The analysis involved 100 Java open-source software systems and the results achieved have been used to distill a set of lesson learned that should be considering to design effective remodularization recommenders.|Research Paper|reject|reject
MSR|2015|Structured Gotos are (Slightly) Harmful|yossi gil,eli sennesh|unstructured jumps,evidence-based language design,empirical defect measurement|The GOTO keyword is traditionally "considered harmful," so much so that programming languages should simply do without it.  We take an evidence-based approach to this issue by counting unstructured jumps in Java code as a software-evolution metric and examining the evolution of 26 open-source code corpora in relation to that metric.  We compare unstructured jumps with code length in tokens and compressed size as statistical predictors of defect proneness.  Our results show that the unstructured-jump metric has predictive power independent of code size.|Research Paper|reject|reject
MSR|2015|Using Big Data and Data Mining Techniques on GitHub Archives to improve Task Assignment Process|shahab bayati|GitHub Mining,Big Data,Recommender System,Clustering,Task Assignment|The available data in online software engineering repositories contain valuable and actionable information which can be discovered through MSR processes. As these repositories get bigger in sizes and the types of data gets more complex, the Big Data techniques can be applied for having better performance in MSR researches. In this paper general Big Data approaches are used to overcome the Internet-scale repositories and GitHub as the largest collaborative development environment is selected to be analyzed.  Machine learning algorithm are applied on open source projects in GitHub to improve the process of task assignment in GitHub by recommending tasks and sources for the right developers and other similar cases.|Research Paper|reject|reject
MSR|2015|Automatically Prioritizing Pull Requests|erik van der veen,georgios gousios,andy zaidman|pull requests,recommendation,prioritisation,GitHub|In previous work, we observed that in the pull-based development model integrators face challenges with regard to prioritizing work in the face of multiple concurrent pull requests. We present the design and initial implementation of a prototype pull request prioritisation tool called PRioritizer. PRioritizer works like a priority inbox for pull requests, recommending the top 3 pull requests the project owner should focus on. An preliminary user study showed that PRioritizer provides functionality that GitHub is currently lacking, even though users needed more insight into how the priority ranking is established to make PRioritizer  really useful.|Research Paper|accept|accept
MSR|2015|Ramp-up Journey of New Hires: Do strategic practices of software companies influence productivity?|ayushi rastogi,suresh thummalapenta,thomas zimmermann,nachiappan nagappan,jacek czerwonka|Ramp-up Journey,New Hires,Productivity|Software companies regularly recruit skilled and talented employees to meet evolving business requirements. Although companies expect early contributions, new hires often take several weeks to reach the same productivity level as existing employees. We refer to this transition of new hires from novices to experts as ramp-up journey. There can be various factors such as lack of technical skills or lack of familiarity with the process that influence the ramp-up journey of new hires. The goal of our work is to identify those factors and study their influence on the ramp-up journey. We expect the results from this study to help identify the need of various types of assistance to new hires to ramp-up faster. As a first step towards our goal, this paper explores the impact of two strategic practices, namely distributed development and internship on the ramp-up journey of new hires. Our results show that, new hires in proximity to the core development team and new hires with prior internship experience perform better than others in the beginning. In the overall ramp-up journey, the effect of the two factors attenuate, yet nevertheless better compared to their counterparts. Product teams can use this information to pay special attention to non-interns and use better tools for distributed, cooperative work to help new hires ramp-up faster.|Research Paper|reject|reject
MSR|2015|Retrieving Diverse Opinions from Software Reviews|emitza guzman,omar aly,bernd brugge|mining user reviews,diversity retrieval,sentiment analysis|Users can have conflicting opinions and different experiences when using software and user reviews serve as a channel in which users can document their opinions and experiences. To develop and evolve software that is usable and relevant for a diverse group of users, different opinions and experiences need to be taken into account. In this paper we present DIVERSE, a feature and sentiment centric retrieval approach which automatically provides developers with a diverse sample of user reviews that is representative of the different opinions and experiences mentioned in the whole set of reviews. We evaluated the diversity retrieval performance of our approach on reviews from seven apps from two different app stores. We compared the reviews retrieved by DIVERSE with random and feature-based retrieval approaches and found that DIVERSE outperforms both with a statistical significant difference. Additionally, a controlled experiment revealed that DIVERSE can help developers save time when analyzing user reviews and was considered useful for detecting conflicting opinions and software evolution.|Research Paper|reject|reject
MSR|2015|On the Identification of Duplicates in Q&A Sites|mathias ellmann,walid maalej|data mining,text mining,stack overflow,knowledge sharing,software development|Stack overflow is one of the most popular online communities to ask and answer developer questions. Every day about 3700 questions are posted on this Q&A site. Two percent of these questions are manually identified as duplicates, i.e. questions which have been posted, discussed, and answered previously at least in part. A tool support for the duplicate identification might reduce the question answering time, allow to reuse knowledge from previous posts, and save time of active community members spent for the manual identification of duplicates. In this paper, we empirically study duplicates in stack overflow with the aim of getting insights into how to automatically identify them. We analyzed the structure, frequency, and topics of duplicates and compared them with the original posts and unique posts. We also analyzed the content of duplicates using text similarly algorithms. We found that about 20% of original posts have 2 or more (up to 558) duplicates. Original questions are significantly longer and include much more answers than duplicate questions. Some topics such as asp.net seem to be rarer in duplicates than in unique questions. On average, there is about 20% of syntactic (term) and 40% of semantic overlap between the text in the original and duplicate questions. Latent Semantic Indexing applied on questions and tags is able to correctly predict about 50% of the duplicates within a top 20 results.|Research Paper|reject|reject
MSR|2015|Query-Level Replication of Mining Studies Through User-Supported Ontology Matching|thomas kowark,keven richly,ralf teusner,matthias uflacker,hasso plattner|study replication,query formulation,ontology matching,analysis platform|The replication of studies in mining software repositories (MSR) on datasets stemming from different projects is crucial for assessing the general validity of the gathered findings. In order to replicate a specific study, it is either necessary to import a new dataset into the originally used tool or to rewrite the queries of the initial experiment in a different tool. The former requires considerable programming efforts to provide data import capabilities for the abundance of available project support tools. The latter is equally labour-intensive, as no commonly accepted data format for storing software repository data is available and, hence, each query requires manual translation. In this paper, we present how the semi-automatic ontology matching concept of the RepMine system simplifies the query translation between different representations of software repository data. Our system constructs ontologies that describe the data schemas of the different mining tools. Based on these ontologies, the corresponding queries issued within the respective tools are expressed using a graph-based abstraction. These graphs are transformed by means of automatic ontology matching tools, as well as manual user input in order to reflect conceptually identical queries on different datasets. Through this approach, we are able to overcome a vast majority of the different impediments for replication that potentially emerge when using state-of-the-art mining systems. Furthermore, we present a step-by-step replication of an exemplary mining study to showcase that our approach reduces the required manual labour in comparison to a replication using current tools. We conclude by outlining how an implementation of RepMine as a shared platform for the MSR community could not only reduce individual efforts but distribute work and benefits across all of its users.|Research Paper|reject|reject
MSR|2015|Extracting Facts from Performance Tuning History of Scientific Applications for Predicting Effective Optimization Patterns|masatomo hashimoto,masaaki terai,toshiyuki maeda,kazuo minami|performance tuning of scientific applications,change pattern identification,tuning pattern prediction,fine-grained source code differencing|To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts such as changing compiler options and transforming the programs without changing the semantics.  Those attempts followed by performance evaluations are repeated until satisfactory results are attained.  The task of performance tuning requires a great deal of time and effort.  On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored based on their intuition or good sense of tuning.  We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications and makes it available to decision makers.  However, in general, tuning histories do not explicitly contain facts about what kind of program transformation contributed to the better performance.  For reconstructing missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from a set of source code variants.  Once required facts are extracted, they are stored and queried for further data mining to predict effective tuning patterns for given source code.  This paper reports on an experiment of tuning pattern identification followed by constructing prediction models conducted for a few scientific applications tuned for the K computer.|Practice Paper|accept|accept
MSR|2015|Expert Finding in Q&A Social Networks for Bug Triaging|ali sajedi badashian,abram hindle,eleni stroulia|Bug triaging,Bug assignment,Mining Software Repositories,Expertise retrieval|Expert finding is the process of finding appropriate people. One of the applications to expert finding is bug triaging and assignment in which a list of developers should be ranked for working on a bug report. Developers in this ranked list, then, will be picked up for fixing the bug by the project manager. While in previous methods just the contents of the bugs and developers’ prior activities are examined, we look for additional source of expertise to find the evidences of expertise of developers and apply it in assigning developers to the bugs. Our novel inter-network model ranks the potential triagers of a coding platform based on their expertise as inferred from their work in Q&A software platforms as well as prior bug fixing activities. Specifically, we considered the contributions of the users in Stack Overflow –a Q&A web site– and apply the results to assign bugs to developers in GitHub –a code repository. We assessed the approach in an experiment on 7144 bug reports in top 20 projects of \gh~. We also presented the Mean Reciprocal Rank (MRR) evaluation measure and reported the results based on MRR as well as the traditional top-k recommendations. Then we compared the accuracy of our approach with the accuracies of the previous works. Considering both evaluation metrics, our approach outperforms other state-of-the-art works.|Research Paper|reject|reject
MSR|2015|Will they like this? Evaluating Code Contributions With Language Models|vincent hellendoorn,premkumar t. devanbu,alberto bacchelli|code review,language models,pull request,OSS,github|Popular open-source software projects receive and review contributions from a diverse array of developers, many of whom have little to no prior involvement with the project. A recent survey reported that reviewers consider conformance to the project's code style to be one of the top priorities when evaluating code contributions on Github. We propose to quantitatively evaluate the existence and effects of this phenomenon. To this aim we use language models, which were shown to accurately capture stylistic aspects of code. We find that rejected changesets do contain code significantly less similar to the project than accepted ones; furthermore, the less similar changesets are more likely to be subject to thorough review. Armed with these results we further investigate whether new contributors learn to conform to the project style and find that experience is positively correlated with conformance to the project's code style.|Research Paper|accept|accept
MSR|2015|Investigating Code Review Practices in Defective Files: An Empirical Study of the Qt System|patanamon thongtanunam,shane mcintosh,ahmed e. hassan,hajimu iida|Code Review,Software Quality,Open Source Software|Software code review is a well-established software quality practice. Recently, Modern Code Review (MCR) has been widely adopted in both open source and industrial projects. To evaluate the impact that characteristics of MCR practices have on software quality, this paper comparatively studies MCR practices in defective and clean source code files. We investigate defective files along two perspectives: 1) files that will eventually have defects (i.e., future-defective files) and 2) files that have historically been defective (i.e., risky files). Through an empirical study of 11,736 reviews of changes to 24,486 files from the Qt open source system, we find that both future-defective files and risky files tend to be reviewed less rigorously than their clean counterparts. We also find that the concerns addressed during the code reviews of both defective and clean files tend to enhance evolvability, i.e., ease future maintenance (like documentation), rather than focus on functional issues (like incorrect program logic). Our findings suggest that although functionality concerns are rarely addressed during code review, the rigor of the reviewing process that is applied to a source code file throughout a development cycle shares a link with its defect proneness.|Research Paper|accept|accept
MSR|2015|Mining Software Repositories to Enable License Compliance Analysis and Verification|christopher vendome,denys poshyvanyk|Software Licensing,Software Provenance,Mining Software Repositories|Software license compliance presents a difficult, yet serious problem for developers. While software licensing aims to facilitate the freedom to reuse, to distribute, and to modify software artifacts, the licenses impose different legal restrictions. These differences and constraints may lead to incompatibility issues between certain licenses, which means for developers that certain systems or components cannot be used together. To further complicate compliance, both licensing of a system may change during evolution and licenses themselves can be updated into a new version. We propose an approach for mining licensing information from both source code and byte-code to ensure license compliance of all components and licenses within a system. First, we establish provenance of the binary components of a system in order to identify their license(s) since they would be lost during compilation. Subsequently, we identify the licenses of both binaries linked to source code and the original source code of the system. The compliance engine analyzes not only the component-license pairs, but it also considers the way in which the code was integrated into a given system. If a potential license incompatibility is identified, the approach aims to provide a feedback by analyzing both the architecture of the system and functional clones so that the developer can more easily remedy the license incompatibility issues.|Research Paper|reject|reject
MSR|2015|Don't Multitask! The Role of Developer's Confusion in Defect Prediction|dario di nucci,fabio palomba,sandro siravo,gabriele bavota,rocco oliveto,andrea de lucia|Defect Prediction,Confusion metrics,Software Complexity|The complexity of software systems has grown exponentially in the last years, pushing developers to face more challenging tasks during evolution and maintenance activities. The developers’ ability to cope with a task might be hindered when it impacts several software components possibly spread across different subsystems. In this paper, we propose two measures capturing the structural and semantic distance between the components a developer is working on in a given time period. We call these measures structural and semantic confusion and we hypothesize that they have an impact on the likelihood of introducing defects during code change activities. To validate our conjecture, we use the deﬁned measures to build a bug prediction model. We evaluated our model on ﬁve open source systems and compared it with two competitive techniques: the ﬁrst is a prediction model exploiting the number of developers working on code component in a given time period as predictor variable while the second is the Basic Code Change Model proposed by Hassan and using code change entropy information. The achieved results show the superiority of our model with respect to the two competitive approaches, and the orthogonality of the deﬁned confusion measures with respect to standard predictors commonly used in the literature.|Research Paper|reject|reject
MSR|2015|Does Sentiment Influence Code Review Activity? A Method to Analyze Sentiment and its Effects|taufan h. ardhinata,amiangshu bosu,barbara russo,jeffrey carver|Peer Code Review,Sentiment Analysis,Text mining|While, the sentiment (i.e. general positive or negative tone) of the conversation between two people can influence the quality of their relationship, it is not clear whether that sentiment is related with the outcome of professional activities, like peer code review. This paper presents a method for mining code repositories to gain a better understanding of the relationship between sentiment and code review outcomes. Using a dictionary customized for the peer code review domain, we mined the repositories of three Open Source project to determine the sentiment in the conversations between developers. We then correlated that sentiment to number of mutual reviews, review time, and acceptance rate. Our findings show that negative sentiment increases time to review and reduces acceptance rate of code review requests.|Research Paper|reject|reject
MSR|2015|Recommending Posts Concerning API Issues in Developer Q&A Sites|wei wang,haroon malik,michael w. godfrey|application program interfaces,software ecosystems,online Q&A|API design is known to be a challenging craft, as API designers must balance their elegant ideals against “real-world” concerns, such as utility, performance, backwards compatibility, and unforeseen emergent uses.  However, to date, there is no principled method to collect or analyze API usability information that incorporates input from typical developers. In practice, developers often turn to Q&A websites such as stackoverflow.com (SO) when seeking expert advice on API use; the popularity of such sites has thus led to a very large volume of unstructured information that can be searched with diligence for answers to specific questions. The collected wisdom within such sites could, in principle, be of great help to API designers to better support developer needs, if only it could be collected, analyzed, and distilled for practical use. In this paper, we present a methodology that combines several techniques, including social network analysis and topic mining, to recommend SO posts that are likely to concern API design-related issues.  To establish a comparison baseline, we introduce two more recommendation approaches: a reputation-based recommender and a random recommender.  We have found that when applied to Q&A discussion of two popular mobile platforms, Android and iOS, our approach achieves up to 93 % accuracy and is more stable with its recommendations when compared to the two baseline techniques.|Research Paper|accept|accept
MSR|2015|Are Bullies more Productive? Empirical Study of Affectiveness vs. Issue Fixing Time|marco ortu,bram adams,giuseppe destefanis,parastou tourani,michele marchesi,roberto tonelli|Minig software repository,affective analysis,software productivity|Human Affectiveness, i.e., the emotional state of a person, plays a crucial role in many domains where they can make or break a team’s ability to produce successful products. Software development is a collaborative activity as well, yet there is little information on how affectiveness impacts software productivity. In order to measure this impact, this paper analyzes the sentiment, emotions and politeness of developers in more than 500K comments posted in the Apache projects' JIRA Issue Tracking System. We found that the more positive developers are (expressing positive emotions such as JOY and LOVE in their comments),  the shorter the issue fixing time is likely to be. Contrarily, negative emotions such as SADNESS, have a negative impact on the issue fixing time. Politeness plays a more complex role and we empirically analyze its impact on developers' productivity.|Research Paper|accept|accept
MSR|2015|Are These Bugs Really “Normal”?|ripon saha,julia l. lawall,sarfraz khurshid,dewayne e perry|Bug Severity,Bug Tracking System,Mining Software Repository|Understanding the severity of reported bugs is important in both research and practice. In particular, a number of recently proposed mining-based software engineering techniques predict bug severity, bug report quality, and bug-fix time, according to this information.  Many bug tracking systems provide a field "severity" offering options such as "severe", "normal", and "minor", with "normal" as the default.  However, there is a widespread perception that for many bug reports the label "normal" may not reflect the actual severity, because reporters may overlook setting the severity or may not feel confident enough to do so.  In many cases, researchers ignore "normal" bug reports, and thus overlook a large percentage of the reports provided. On the other hand, treating them all together risks mixing reports that have very diverse properties.  In this study, we investigate the extent to which "normal" bug reports actually have the "normal" severity. We find that many Normal bug reports in practice are not normal. Furthermore, this misclassification can have a significant impact on the accuracy of mining-based tools and studies that rely on bug report severity information.|Research Paper|accept|accept
MSR|2015|Do Onboarding Programs Work?|adriaan labuschagne,reid holmes|onboarding,OSS,mentorship|Open source software systems rely on community source code contributions to fix bugs and develop new features. Unfortunately, it is often difficult to become an effective contributor on open-source projects, because of the complexity of the tools required to develop and test new patches, and the challenge of breaking into an already-formed social organization. To help new contributors learn their development practices, OSS projects have created onboarding programs that, for example, identify easy ‘first bugs’ and mentor new developers’ contributions. However, we found that developers who join an organization through these programs are about 50% less likely to transition into long-term community members than developers who do not use these programs. Measuring the impact of these programs is important, as coordinating and staffing onboarding projects is expensive; this paper demonstrates that onboarding programs are not as effective at transitioning new developers into long-term contributors as might be hoped, although developers who do succeed through these programs find them valuable.|Research Paper|accept|accept
MSR|2015|Mining Component Repositories for Installability Issues|pietro abate,roberto di cosmo,louis gesbert,fabrice le fessant,ralf treinen,stefano zacchiroli|software components,software packages,installability,dependency solving,quality assurance,component repositories|Component repositories play an increasingly relevant role in software life-cycle management, from software distribution to end-user, to deployment and upgrade management. Software components shipped via such repositories are equipped with rich metadata that describe their relationship (e.g., dependencies and conflicts) with other components. In this practice paper we show how to use a tool, distcheck, that uses component metadata to identify all the components in a repository that cannot be installed (e.g., due to unsatisfiable dependencies), provides detailed information to help developers understanding the cause of the problem, and fix it in the repository. We report about detailed analyses of several repositories: the Debian distribution, the OPAM package collection, and Drupal modules. In each case, Distcheck is able to efficiently identify not installable components and provide valuable explanations of the issues. Our experience provides solid ground for generalizing the use of distcheck to other component repositories.|Practice Paper|accept|accept
MSR|2015|Exploring the Interaction between Stemmers and Text Models for IR Based Bug Localization|wen yi,shivani rao,avinash c. kak|Stemming,Latent Semantic Indexing,Unigram,Vector Space Model,Bug Localization,Information Retrieval,Software Engineering|Stemming algorithms are important to the operation of information retrieval algorithms that are used in IR-based bug localization frameworks. The general practice among researchers and others is to use one of the well-known stemming algorithms without paying much attention to its appropriateness to how the retrieval is carried out. In this paper we argue that there exists a “tension” between stemmers, on the one hand, and the text models used for making retrievals, on the other. Whereas the purpose of a stemming algorithm is to generalize the meaning of the words retained in a model by reducing their variants to common roots, the purpose of a text model is to exploit the discriminatory power of the words in order to make the documents distinguishable. Therefore, it is unrealistic to assume that the same stemming algorithm would work equally well for all different text models. This is borne out by our experiments. Our work demonstrates that, for any given text model, not all well-known stemmers lead to improved retrieval accuracy. Furthermore, for each text model, there is likely to be a particular stemmer that works the best. Our experimental investigations are based on the following five stemmers: KStem, MStem, Paice, Porter, and Snowball. The text models we have used are Unigram, VSM, and LSI. Our quantitative results are based on AspectJ and JodaTime libraries.|Research Paper|reject|reject
MSR|2015|Do Bugs Foreshadow Vulnerabilities? A Study of the Chromium Project|felivel camilo,andrew meneely,meiyappan nagappan|bugs,vulnerabilities,mining,chromium|As developers face ever-increasing pressure to engineer secure software, researchers are building an understanding of security-sensitive bugs (i.e. vulnerabilities). Research into mining software repositories has greatly increased our understanding of software quality via empirical study of bugs. However, conceptually vulnerabilities are different from bugs: they represent abusive functionality as opposed to wrong or insufficient functionality commonly associated with traditional, non-security bugs. In this study, we performed an in-depth analysis of the Chromium project to empirically examine the relationship between bugs and vulnerabilities. We mined 374,686 bugs and 703 post-release vulnerabilities over five Chromium releases that span six years of development. Using logistic regression analysis, we examined how various categories of pre-release bugs (e.g. stability, compatibility, etc.) are associated with post-release vulnerabilities. While we found statistically significant correlations between pre-release bugs and post-release vulnerabilities, we also found the association to be weak. Number of features, SLOC, and number of pre-release security bugs are, in general, more closely associated with post-release vulnerabilities than any of our non-security bug categories. In a separate analysis, we found that the files with highest defect density did not intersect with the files of highest vulnerability density. These results indicate that bugs and vulnerabilities are empirically dissimilar groups, warranting the need for more research targeting vulnerabilities specifically.|Research Paper|accept|accept
MSR|2015|An Empirical Study of Highly-Distributed Bugs in Mozilla Projects|le an,foutse khomh|bug triaging,entropy analysis,crash report,prediction model,mining software repositories|Bug triaging is the process that consists in screening and prioritising bugs to allow a software organisation to focus its limited resources on bugs with high impact on software quality. In a previous work, we proposed an entropy-based crash triaging approach that can help software organisations identify crash-types that affect a large user base with high frequency. We refer to bugs associated to these crash-types as highly-distributed bugs. The proposed triaging approach can identify highly-distributed bugs only after they have led to crashes in the field for a certain period of time. Therefore, to reduce the impact of highly-distributed bugs on user perceived quality, an early identification of these bugs is necessary. In this paper, we examine the characteristics of highly-distributed bugs in Mozilla Firefox and Fennec for Android, and propose statistical models to help software organisations predict them early before they impact a large population of users. Results show that our proposed prediction models can achieve a precision up to 69.8% as well as a recall up to 95.5%. We also evaluate the benefits of our proposed models and found that, on average, they could help reduce 23.1% of crashes for Firefox and 13.2% for Fennec for Android while reducing 29.5% of impacted machine profiles for Firefox and 49.8% for Fennec for Android. Software organisations could use our prediction models to catch highly-distributed bugs early during the triaging process, preventing them from impacting a larger user base.|Research Paper|reject|reject
MSR|2015|Towards Automatic Recommendation of Tests for Continuous Integration|linlin wang,eric knauss,agneta nilsson|Continuous integration,test case selection,test case prioritization|Continuous integration promises advantages by enabling software developing organizations to deliver new functions faster. However, implementing continuous integration, especially in large software development organisations, is challenging because of organizational, social, and technical reasons. One of the technical challenges is the ability to rapidly prioritise the test cases which can be executed quickly and trigger the most failures as early as possible. In this paper, we propose an automatic recommender based on mining correlations between test-case failures and source code changes. Based on experiences from this ongoing work, we discuss data needs, experimental setup, as well as challenges and opportunities that arise from this highly dynamic problem field.|Research Paper|reject|reject
MSR|2015|The Uniqueness of Changes: Characteristics and Applications|baishakhi ray,meiyappan nagappan,christian bird,nachiappan nagappan,thomas zimmermann|software evolution,repetitive changes,code clones,recommendation system|Changes in software development come in many forms.  Some changes are frequent, idiomatic, or repetitive (e.g. adding checks for nulls or logging important values) while others are unique.  We hypothesize that unique changes are different from the more common similar (or non-unique) changes in important ways; they may require more expertise or represent code that is more complex or prone to mistakes.  As such, these changes are worthy of study.  In this paper, we present a definition of unique changes and provide a method for identifying them in software project history. Based on the results of applying our technique on the Linux kernel and two large projects at Microsoft, we present an empirical study of unique changes. We explore how prevalent unique changes are and investigate where they occur along the architecture of the project. We further investigate developers' contribution towards uniqueness of changes. We also describe potential applications of leveraging the uniqueness of change and implement two such applications, evaluating the risk of changes based on uniqueness and providing change recommendations for non-unique changes.|Research Paper|reject|reject
MSR|2015|How do Open Source Programmers Choose Parent Classes?|yossi gil,sabih agbaria|Number of children,Object oriented programming,Preferential attachment,Power law,Long tail|Abstract Previous research repeatedly discovered a power-law distribution (or more generally, long-tail distributions) of object-oriented soft- ware metrics. The classical process of preferential attachment was proposed as a possible explanation. This empirical study focuses on the NOC (Number of Children) object-oriented software metric: By retrospection on the history of development of 16 moderately sized open-source JAVA software artifacts, we try to assess the hypothe- sis underlying preferential attachment, namely that the number of existing children a class already has, determines the probability it is selected as a parent again. We were able to establish partial empirical support for this hypothesis. We also show that other properties of the class, such as the number of methods it has, do not make a good predictor of the number of children. It is shown that the number of changes that a class suffered and the recency of these changes make good predictors of the probability that the class is selected as base class. This finding may suggest insights into the evolution of the extendability property of classes.|Research Paper|reject|reject
MSR|2015|Examining the Relationship between Security Ratings and User Ratings of Mobile Apps: A Case Study|daniel krutz,meiyappan nagappan,andrew meneely|Mobile Development,User Ratings,Software Engineering|The life or death of a mobile application (`app') is determined largely by user ratings. Users expect apps to continually provide new features while maintaining quality, or the ratings drop. However, apps must also be secure. A focus on features can lead to security tests slipping. But is this trade-off historically true? Or are app store ratings a more all-encompassing measure of product maturity? We collected and compared 798 random Android apps from the GooglePlay store with a user rating of less than 2.99 against 861 apps with a user rating of 3 or greater. Combining our evidence collected from the results of our experiments, we conclude that, historically, user ratings and potential security risks are not mutually exclusive and do not trade off.|Research Paper|reject|reject
MSR|2015|Differentiated Roles in Open Source Development: A Case Study of Interconnected Projects in GitHub|patrick wagstrom,corey jergensen,anita sarma|Roles,Open Source Development,Interconnected Projects,GitHub|Complex software development rarely occurs in isolation. In addi-tion to functional and technical dependencies, developers often follow and participate in different roles in a variety of different projects. The exchange of technical and social knowledge through these developers results in the creation of a loosely federated community of projects. Within these communities developers are able to leverage lessons learned across projects. Prior to the rise and centralization of software development around GitHub, it was difficulty to understand these relationships. Now, thanks to robust APIs and publicly available data sets, users and researchers can examine these  relationships across thousands of users and projects. In this paper, we examine a sub-community of projects from GitHub to understand how projects are linked together by developer interests and the overlap of roles among users associated with these projects. We utilize the rich traceability afforded by GitHub to expand the traditional open source development roles and provide a finer grained understanding of the relationships within the user community. We then investigate the characteristics of this community by analyzing the distribution of different development roles within and across projects. We find that developers often contribute to projects in a wide variety of roles and that although there is some preference for particular programing languages and environments, many developers actively contribute and write code for projects in multiple different languages|Research Paper|reject|reject
MSR|2015|Mining Trends and Patterns of Software Vulnerabilities|syed shariyar murtaza,wael khreich,abdelwahab hamou-lhadj,ayse bener|Software vulnerabilities,Patterns of vulnerabilities,National Vulnerability Database,Exploratory analysis|National Vulnerability Database (NVD) contains comprehensive information about vulnerabilities affecting software systems. Researchers and practitioners have mostly used this database to mine simple trends, specific characteristics of vulnerabilities, or classification of software vulnerabilities into new types. In this paper, we look at the last six years of vulnerabilities in software applications and identify the common sequential patterns of vulnerabilities. We also investigate that the use of historical pattern of vulnerabilities in predicting future vulnerabilities in software applications. In addition, identify if the trends of vulnerabilities in general and in individual applications have really any significant meaning or not. Our results show that sequences of similar vulnerabilities (e.g., buffer errors) can occur more than 50 times in a software product. Also, the results show that SQL injection vulnerabilities are decreasing in last six years and cryptographic vulnerabilities are increasing; however, statistically there is no significant change in trends of the vulnerabilities. Thus, attackers are still exploiting the applications in a similar manner as they were exploiting in the past despite the improved protection mechanisms.|Research Paper|reject|reject
MSR|2015|An Empirical Study of Architectural Change in Open-Source Software Systems|duc le,pooyan behnamghader,joshua garcia,daniel link,arman shahbazian,nenad medvidovic|software architecture,architectural change,software evolution,open-source systems,architecture recovery|From its very inception, the study of software architecture has recognized architectural decay as a regularly occurring phenomenon in long-lived systems. Architectural decay is caused by repeated changes to a system during its lifespan. Despite decay's prevalence, there is a relative dearth of empirical data regarding the nature of architectural changes that may lead to decay, and of developers' understanding of those changes. In this paper, we take a step toward addressing that scarcity by conducting an empirical study of changes found in software architectures spanning several hundred versions of 14 open-source systems. Our study reveals several new findings regarding the frequency of architectural changes in software systems, the common points of  departure in a system's architecture during  maintenance and evolution, the difference between system-level and component-level architectural change, and the suitability of a system's implementation-level structure as a proxy for its architecture.|Research Paper|accept|accept
MSR|2015|Investigating automated testing practices in OSS: Lessons Learned and Future Directions|soha makady,robert j. walker|Open source repositories,Automated testing,Problem analysis,Lessons learned|We are told of the significant benefits of automated approaches to testing over manual approaches, but it is unclear whether industry accepts that message. To investigate the conduct of automated testing in practice, we attempted a thorough, large-scale analysis of open source repositories. While this seems to be a straightforward task, we encountered multiple, practical challenges that resulted in a prolonged and painful journey. We present our lessons learned as a guide and a warning to other researchers as to the serious problems (and potential solutions) that future such analyses may encounter when mining large-scale open source repositories.|Research Paper|reject|reject
MSR|2015|Cross-Distribution Bug Duplicates Detection|vincent boisselle,bram adams|duplicate bug detection,open source distribution,empirical study,bug repository|Although open source distributions like Debian and Ubuntu are closely related, sometimes an issue reported in the Debian bug repository is reported independently for Ubuntu as well, without the Ubuntu users nor developers aware that it was previously fixed in the root distribution. To help users and developers to be more efficient and productive at identifying previously fixed bugs in related distributions, known techniques such as bug report duplicate detection could be used. Unfortunately, existing duplicate detection techniques are trained and optimized on one single project (distribution) instead of across projects and, on top of that, different projects typically use different bug repository technologies. We develop a cross-distribution bug duplicate detection technique based on a custom data mining approach, and evaluated it on the Ubuntu and Debian bug repositories. Our approach can detect duplicates across distributions with a recall of up to 79% while (in some cases) reducing the time lost by users waiting for a fix by 155 days. We also find that developers from the different distributions lose on average 149 days in which they could have collaborated together, had they been aware of duplicates.|Research Paper|reject|reject
MSR|2015|An Empirical Study of Core Developer Teams in GitHub Projects|kazuhiro yamashita,shane mcintosh,yasutaka kamei,ahmed e. hassan,naoyasu ubayashi|core development team,Pareto principle,developer activity|It is often observed that the majority of the development work of an Open Source Software (OSS) project is contributed by a core team, i.e., a small subset of the pool of active developers. In fact, recent work has found that core development teams follow the Pareto principle — roughly 80% of the code contributions are produced by 20% of the active developers. However, these studies have focused on a small sample size (fewer than 10 studied systems). With the large-scale adoption of social coding platforms, such as GitHub, a plethora of data is becoming available for researchers to better understand the nature of core development teams in OSS projects. In this paper, we mine the version control repositories of 2,923 projects hosted on GitHub. We find that even when we control for system size, team size, and project age: (1) 9%-44% of GitHub projects have core teams that include 10% (or less) of the active developers; and (2) 11%- 51% of GitHub projects have core teams that include at least 30% of the active developers. Indeed, for 46%-60% of GitHub projects, the Pareto Principle does not seem to apply. Moreover, we find that, when we control for the quantity of contributions, bug fixing accounts for similar proportion of the contributions of both core (16%) and non-core developers (20%). We also find that project characteristics like age, system size, and contribution rate share a link with core team size.|Research Paper|reject|reject
MSR|2015|Mining for Collaborative Groups in Apache Software Foundation Projects|mohammad gharehyazie,vladimir filkov|Teamwork,Collaborative Groups,Collaborative Development,OSS|Large software systems get the attention of multiple programmers; some files regularly get touched by several people at nearly the same time. To ensure the viability of their collaboration, when multiple people work on the same chunk of code at the same time, they communicate and employ safeguards in different ways then when working alone. Recent studies have considered group co-development in OSS projects, and found that it is an essential part of many projects, with benefits to coding efficiency. However, those studies were limited to groups of size two, i.e., to pairs of developers. In this paper we describe an approach for capturing group collaboration based on synchronized commit activities of multiple developers. Using this approach, we study the effects of group co-development on coding focus and productivity in 26 OSS projects from the Apache Software Foundation. We find that group co-development exists, and that the levels of teaming up are far lower than in our random models, implying that developers, in general, avoid it. We also find that high developer focus on specific packages is related with their lower team participation, while packages with higher ownership also get less attention from groups than from individuals. Finally, we show that productivity effort during co-development is more often lower for developers while they co-develop in groups.|Research Paper|reject|reject
MSR|2015|An Empirical Study on the Handling of  External and Internal Crash Reports: An Industrial Perspective|abdelwahab hamou-lhadj,abdou maiga,mathieu nayrolles,alf larsson|Software Maintenance,Crash Report Handling,Empirical Studies,Industrial Case Studies|In this paper, we report on an empirical study we have conducted at Ericsson to understand the handling of crash reports (CRs). The study was performed on a dataset of CRs spanning over two years of activities on one of Ericsson’s largest systems (+4 Million LOC). CRs at Ericsson are divided into two types: Internal and External. Internal CRs are reported within the organization after the integration and system testing phase. External CRs are submitted by customers and caused mainly by field failures. We examine the proportion and severity of internal CRs and that of external CRs. A large number of external (and severe) CRs could indicate flaws in the testing phase. Failing to react quickly to external CRs, on the other hand, may expose Ericsson to fines and penalties due to the Working Level Agreements (WLA) that Ericsson has with its customers. Moreover, we contrast the time it takes to handle each type of CRs with the dual aim to understand the similarities and differences as well as the factors that impact the handling of each type of CRs. Our results show that (a) it takes more time to fix external CRs compared to internal CRs, (b) the severity attribute is used inconsistently through organizational units, (c) assignment time of internal CRs is less than that of external CRs, (d) More than 50% of CRs are not answered within the organization’s fixing time requirements defined in WLA.|Practice Paper|reject|reject
MSR|2015|Analyzing Release Trajectory based on the Process of Issues Tracking|hani abdeen,houari a. sahraoui|Software Evolution,Software Release Development Process,Issue Tracking Systems,Sequence Analysis|Software release development process, that we refer to as "release trajectory", involves development activities that are usually sorted in different categories, such as incorporating new features, improving software, or fixing bugs, and associated to "issues". Release trajectory management is a difficult and crucial task. Managers must be aware of every aspect of the development process for managing the software-related  issues. Issue Tracking Systems (ITS) play a central role in supporting the management of release trajectory. These systems, which support reporting and tracking issues of different kinds (such as "bug", "feature", "improvement", etc.), record rich data about the software development process. Yet, recorded historical data in ITS are still not well-modeled for supporting practical needs of release trajectory management. In this paper, we describe a sequence analysis approach for modeling and analyzing releases' trajectories, using the tracking process of reported issues. Release trajectory analysis is based on the categories of tracked issues and their temporal changing, and aims to address important questions regarding the co-habitation of unresolved issues, the transitions between different statuses in release trajectory, the recurrent patterns of release trajectories, and the properties of a release trajectory.|Research Paper|reject|reject
MSR|2015|Lessons Learned from Building and Deploying a Code Review Analytics Platform|christian bird,trevor carnahan,micheala greiler|Practice Paper,Experience Report,Code Review,Software Analytics|Tool based code review is growing in popularity and has become a standard part of the development process at Microsoft.  Adoption of these tools make it possible to mine data from code reviews and provide access to it.  In this paper, we present an experience report for CodeFlow Analytics, a system that collects code review data, generates metrics from this data, and provides a number of ways for development teams to access the metrics and data.  We discuss the design decisions and challenges that we encountered when building CodeFlow Analytics along with who it is intended for.  We contacted teams that used CodeFlow Analytics over the past two years and report what prompted them to use CodeFlow Analytics, how they have used it, and what they have been able to accomplish with it. We survey research that has been enabled by using the CodeFlow Analytics platform.  We finish with a series of lessons learned from this experience to help others embarking on a task of building an analytics platform in an enterprise setting.|Practice Paper|accept|accept
MSR|2015|A Comprehensive Study of Mobile Bugs---Types, Reporting Practices, and Fixing Challenges|taiyue liu,michael pollind,lin tan,adrian nistor|Bugs in mobile applications,Software reliability,Empirical study|Mobile applications are becoming an essential category of software.  A recent study projects that there will be seven times more shipped mobile devices than PCs in 2015.  Due to the fierce competition in the mobile market, mobile applications must be reliable to attract and retain customers.  Designing effective techniques to address bugs in mobile applications---mobile bugs, for brevity---requires a deep understanding of these bugs. Toward this end, we study the bug types, bug reporting practices, and challenges encountered while fixing mobile bugs.  We compare the results for mobile bugs with corresponding results for the better-understood non-mobile bugs.  We study mobile and non-mobile bugs (400 bugs inspected manually, 4753 bugs processed automatically) from 7 mobile applications (Anki, CGeo, CSipSimple, K-9, Osmand, Wordpress, and XPrivacy) and 8 non-mobile applications (Chukwa, Clerezza, Giraph, JMeter, Nutch, PDFBox, Tika, and ZooKeeper), respectively.  Our first main finding is that, despite the unique characteristics of mobile platforms, there are fewer fixed performance bugs and similar numbers of fixed compatibility and concurrency bugs in mobile applications compared to non-mobile applications.  Perhaps surprisingly, energy bugs are very rarely reported and fixed.  The two main possibilities are that (1) mobile platforms do not increase the risk of having these types of bugs, or (2) these types of bugs easily escape detection and fixing in mobile applications.  Second, we find that mobile bug reports contain less detailed debugging information than non-mobile bug reports, suggesting that better information gathering and reporting techniques are needed for mobile bugs.  Third, initial fixes for both mobile and non-mobile bugs are similarly likely to require supplementary fixing, but for mobile bugs the initial fixes are more likely to contain omission bugs and the supplementary fixes are larger than for non-mobile bugs.  This finding implies we need better tools to fix and test the proposed fixes for mobile bugs.|Research Paper|reject|reject
MSR|2015|Open-Source Project Categorization with Deep Belief Network|hoan anh nguyen,tien n. nguyen|Open-source project categorization,Deep learning,Deep belief network,High-level concepts|Automated software categorization is important in supporting users to quickly find the relevant applications in a specific category in open-source software repositories. Researchers have investigated several automated approaches to categorize software projects using various techniques from Information Retrieval (IR) (e.g., Latent Semantic Indexing), to Machine Learning (ML) (e.g., Latent Dirichlet Allocation and other traditional ML models). In this work, we perform an empirical study to apply Deep Belief Network (DBN) in automatic software categorization. We implemented a DBN model that consists of four layers that takes the source code project and classifies it into categories. Our preliminary experimental result shows that DBN is able to recognize important features such as API calls and identifiers and use them in software categorization. It is able to form high-level concepts in a project from low-level code features. More importantly, DBN achieves higher categorization accuracy from 5.9-26.4% than traditional ML models.|Research Paper|reject|reject
MSR|2015|Sameness: An Experiment in Code Search|lee martie,andre van der hoek|Code,search,diversity,similarity,results,top ten,concise,sameness|To date, most dedicated code search engines use ranking algorithms that focus only on the relevancy between the query and the results.  In practice, this means that a developer may receive search results that are all drawn from the same project, all implement the same algorithm using the same external library, or all exhibit the same complexity or size, among other possibilities that are less than ideal. In this paper, we propose that code search engines should also locate both diverse and concise (brief but complete) sets of code results. We present four novel algorithms that use relevance, diversity, and conciseness in ranking code search results. To evaluate these algorithms and the value of diversity and conciseness in code search, twenty-one professional programmers were asked to compare pairs of top ten results produced by competing algorithms. We found that two of our new algorithms produce top ten results that are strongly preferred by the programmers.|Research Paper|accept|accept
MSR|2015|Recommending Insightful Comments for Source Code using Crowdsourced Knowledge|mohammad masudur rahman,chanchal kumar roy,iman keivanloo|Stack Overflow,code examples,program analysis,code insight,comment recommendation|Recently, automatic code comment generation is proposed to facilitate program comprehension. Existing code comment generation techniques focus on describing the functionality of the source code. However, there are other aspects such as insights about quality or issues of the code, which are overlooked by earlier approaches. In this paper, we describe a mining approach that recommends insightful comments about the quality, deficiencies or scopes for further improvement of the source code. First, we conduct an exploratory study that analyzes crowdsourced knowledge from Stack Overflow discussions as a potential resource for source code comment recommendation. Second, based on the findings from the exploratory study, we propose a heuristic-based technique for mining insightful comments from Stack Overflow Q & A site for source code comment recommendation. Experiments with 292 Stack Overflow code segments and 733 discussion comments show that our approach has a promising recall of 85.42%. We also conducted a complementary user study which confirms the accuracy and usefulness of the recommended comments.|Research Paper|reject|reject
MSR|2015|Can Review Discussions Reveal Risk?|parastou tourani,bram adams|Software Code Review,Sentiment Analysis,Defect Prediction Models|Reviewing is an essential part of any organization’s quality assurance program, yet, even though reviewing methodologies and tool support have seen a huge amount of development in recent years, bugs still slip under the radar. To reduce the percentage of bug-prone changes being accepted, different defect prediction models have been designed, which focus especially on attributes of a software change like the number of added or deleted lines or developer experience, but ignore the actual review discussions. This is unfortunate, since similar to how body language can reveal implicit hints about a person’s real feelings, characteristics like the length, intensity or positivity of a review discussion could qualify the review outcome more clearly. In this paper, we investigate statistical models to study the impact of characteristics of the review discussion of a commit on the defect-proneness  of that commit. Comparison of these models to traditional models shows that review discussion metrics significantly improve established metrics like churn and developer experience.|Research Paper|reject|reject
MSR|2015|Beyond the Surface: An Analysis of Fine-grained Attack Surface Metrics in FFmpeg|andrew meneely,nuthan munaiah,kevin campusano gonzalez|attack surface,entry point,exit point,vulnerability,risk,proximity,reachability,surface coupling|As our software systems become more interconnected, attackers can find more ways to break in. Engineering secure software requires continual assessment of the risk of an attack as the development (or maintenance) of the software progresses. Measuring the "attack surface" of a software system has provided means for such assessment. In the attack surface metaphor, the call graph of a system is analyzed to determine the ways in which functions can be reached from various entry and exit points. The assumption of the attack surface metaphor is that if more functions can be reached (e.g. more entry or exit points, or a dense call graph), then the system is at a higher risk of an attack. In this study, we analyzed the call graph of 15 releases of an open source media transcoding library, FFmpeg. We also analyzed 171 vulnerabilities and 565 commits that fixed those vulnerabilities across releases of FFmpeg. We formulated, collected, and analyzed various metrics, broadly grouped into reachability, proximity, and surface coupling metrics, to investigate the relationship between the attack surface and vulnerable functions. We found that entry (exit) points with higher reachability were more likely to be the source (sink) of a vulnerable function. Our analysis also revealed a counter-intuitive result: that a function connected to fewer exit points per SLOC (i.e. lower surface coupling with exit) was more likely to have a vulnerability. The result runs counter to conventional wisdom and prior research, and will shape the kinds of empirical questions that should be asked of the attack surface metaphor.|Research Paper|reject|reject
MSR|2015|A Study of Design Degradation: Why Software Projects Get Worse Over Time|iftekhar ahmed,rahul gopinath,carlos jensen|Software Decay,Design Problems,Project History|Software decay is a key concern for large, long-lived software projects. Systems degrade over time as design and implementation compromises and exceptions pile up. However, there has been little research into quantifying this decay, or understand how different software projects deal with these issues. While the best approach to improve the quality of a project is to spend time on reducing both software defects (bugs) and addressing design issues (refactoring), we find that design issues are frequently ignored in favor of fixing defects. We find that design issues have a higher chance of being fixed in the early stages of a project, and that efforts to correct these stall as projects mature and code bases grow, leading to a build-up of design problems. From studying a large set of open source projects, our research suggests that, while core contributors tend to fix design issues more often than non-contributors, there is no difference once the relative quantity of commits is accounted for.|Research Paper|reject|reject
MSR|2015|Discipline versus Diversity: In Search for Release Cycle Time Patterns by Mining Mobile App Repositories|maleknaz nayebi,guenther ruhe,bram adams|Mining app stores,Release cycle time,Consistency across repositories,Empirical study|Reducing time between consecutive releases (i.e., the cycle time) has become the goal of many software organizations in order to bring high value features faster to the end users. For mobile apps, however, cycle time reduction poses additional challenges, since organizations no longer have control over the mechanism (i.e., app store) by which an app is distributed. A closer look into release cycle durations of mobile apps showed that there can also be a strong variation in the release cycle times of an app. In order to understand the impact of diversity (varying release cycle times) and discipline (having a fixed cycle time across releases) of mobile apps as well as the way in which the apps have been perceived by the end users, we performed two case studies on (i) 6,003 closed-source mobile apps in Google Play and (ii) 122 open source mobile apps from F-Droid. After data cleaning and filtering process, we extracted seven release patterns and find three of them being in common to both data sets. For detecting patterns, we used a custom pattern recognition approach. For the apps following different patterns, we further analyzed the amount of work that goes into the mobile app releases and the way in which apps with different cycle times are received by users.|Research Paper|reject|reject
MSR|2015|A Look at Work-Item Population Dynamics in Commercial Software Projects|ripon saha,stanley sutton,peri l. tarr|Work Dynamics,Work Item Population Dynamics,Project Understanding,Project Management|Good project outcomes in software development projects depend on accurate assessments of in-flight projects and realistic predictions of expected time to completion.  Assessments and predictions depend on an understanding of the rates at which work is added to and completed in a project, which may evolve over time.  Work in software projects is commonly represented in some form of work item, so assessments and predictions may depend on an understanding of work-item population dynamics (which we call “work dynamics” for short).  This paper describes an empirical study of work-item population dynamics in versions of IBM commercial products. The purpose of this study is to gather data on how developers create, schedule, and complete different kinds of work items (specifically tasks and defects), whether pre-planning leads to completion of more work items, whether there is a relationship between task and defect dynamics, and so on. We observed that software development projects do seem to exhibit coherent patterns of work dynamics.  However, both similarities and differences exist between the patterns of different projects depending on the length of release cycle, process, and software types. These similarities and differences must be anticipated if patterns are to be effectively leveraged in software project management.|Research Paper|reject|reject
MSR|2015|A Comparative Analysis of the Usability of StackOverflow Code Snippets Across Languages|cristina videira lopes,di yang,aftab hussain|StackOverflow,Programming Languages,Usability|Enriched by the surrounding natural language texts, StackOverflow code snippets make up for an invaluable code-centric knowledge base of small units of programming that could, conceivably, serve as the basis for recombinant automatic program generation. This paper makes a first step towards that vision by investigating the following questions: how usable are the StackOverflow code snippets, and how does usability vary across popular programming languages? A total of 3M code snippets were analyzed across four languages: C\#, Java, JavaScript, and Python. Python and JavaScript proved to be the languages for which the most code snippets are usable: 537,767 (66\%) JavaScript snippets are parsable and 163,247 (20\%) of them are runnable; for Python, 402,249 (75\%) are parsable and 135,147 (27\%) are runnable. Conversely, Java and C\# proved to be the languages with the lowest usability rate: 129,691 (25\%) C\# snippets are parsable but only 986 (0.2\%) of them are compilable; for Java, only 35,619 (6\%) are parsable and 9,177 (1.6\%) compile. Based on the most common error messages, we applied simple heuristic repairs to C\# and Java snippets, but the end result is still very low. Although a difference between dynamically-typed and statically-typed languages was expected, the very low pass rates of Java and C\# cast doubts about whether these languages could serve as assembly languages for automatic program generation. Our results point to JavaScript and Python being the best candidates for those kinds of investigations, simply because, among these four popular languages, there are much more usable snippets in JavaScript and Python than in the other two languages.|Research Paper|reject|reject
MSR|2015|Effects of Collaborative Development on  Popularity, Efficiency, and Defect Detection|weixuan mao,yuriy brun,donald f. towsley,zhongmin cai|Mining software repositories,Software metric,Defect prediction|Software development is an inherently collaborative activity and the nature of the collaboration likely affects projects success. Collaboration can lead to faster development and increased creativity, but it may also adversely affect software quality. In this paper, we explore the effects of collaboration on software development and use a model of collaboration to detect software defects. We conduct an at-scale study of more than five million GitHub projects to reveal ollaboration's effects on software popularity and speed of development. We find that having more collaborators results in higher popularity but slower development. Projects that use Objective-C are at an even more significant risk for lower development speeds than other projects. We define collaboration networks and co-commit networks, which capture the projects' collaboration patterns use structural features of these networks to identify files within commits with defects. Examining 18 large popular projects, we find that involving these features statistically significantly improves defect detection by 3.9%.|Research Paper|reject|reject
MSR|2015|Correlating the Human, Social and Technical Dimensions of Collaborative Software Development|michele galli,marios fokaefs,eleni stroulia|collaborative software development,sentiment analysis,project management|Collaborative software development is complex socio-technical process. The goal is the development of a software system, but at the center of the process are the developers. The challenge that presents itself to the development team is to handle and manage all the individual personalities and emotional states in order to achieve its ultimate goal. In this paper, we conduct a study with real software teams and projects to demonstrate the complexity of the development process and reveal the interdependencies among the human, social and technical aspects of the process. We study five teams within the context of an undergraduate software engineering course. Our study shows that there is significant correlation between the developers' personalities and emotional states and their productivity, the frequency of their communication and their relationships with the rest of the team. Developers with positive attitude are generally more productive, more communicative and better appreciated by their peers, compared to those with a more negative attitude.|Research Paper|reject|reject
MSR|2015|Unveiling and Reasoning about Hidden Dependencies Induced by Co-Evolution|marcos oliveira,rodrigo bonifacio,guilherme ramos|Modularity,Co-Change Clusters,Software Clustering,Design Structure Matrices|Flexibility is one of the expected benefits of a modular design, and thus "It should be possible to make drastic changes to one module without a need to change others". Accordingly, based on the evolutionary data available on version control systems, it should be possible to analyze the quality of a software architecture- and thus decide whether it is worth or not to restructure the design of a software. In this paper we investigate this issue using a novel approach based on a general theory of modularity that uses design structure matrices for reasoning about the quality attributes of a design. We carried out a comprehensive study using our approach and we found that unveiling and reasoning about the hidden dependencies of a software design, computed through the co-evolution of its modules, might impact the quality attributes of a software design, but to reorganize the source code to match the co-change clusters does not worth the effort. This contrasts with previous works that suggest that the analysis of co-change clusters might help developers in the challenging task of redesigning a software.|Research Paper|reject|reject
MSR|2015|Mining Android App Usages for Generating Actionable GUI-based Execution Scenarios|mario linares vasquez,martin white,carlos eduardo bernal cardenas,kevin moran,denys poshyvanyk|GUI models,mobile apps,actionable scenarios,language models|GUI-based models extracted from Android app execution traces, events or source code can be extremely useful for challenging tasks such as generation of scenarios or test cases. However, extracting effective models can be an expensive process. Moreover, existing approaches for automatic derivation of GUI-based models are not able to generate scenarios that include events that were not observed in execution (or event) traces. In this paper we address these and other major challenges in our novel hybrid approach, coined as MONKEYLAB. Our approach is based on the Record→Mine→Generate→Validate frame- work, which relies on recording app usages that yield execution (event) traces, mining those event traces and generating execution scenarios using statistical language modeling, static and dynamic analyses, and validating resulting scenarios using interactive execution of the app on a real device. The framework aims at mining models capable of generating feasible and fully replayable (i.e., actionable) scenarios reflecting either natural user behavior or uncommon usages (e.g., corner cases) for a given app. We evaluated MONKEYLAB in a case study involving several medium- to-large open-source Android apps. Our results demonstrate that MONKEYLAB is able to mine GUI-based models that can be used to generate actionable execution scenarios for both natural and unnatural sequences of events on Google Nexus 7 tablets|Research Paper|accept|accept
MSR|2015|Toward Deep Learning Software Repositories|martin white,christopher vendome,mario linares vasquez,denys poshyvanyk|Software repositories,machine learning,deep learning,software language models,n-grams,neural networks|Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered tremendous advances in many fields such as natural language processing (NLP). Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files, since they only require lexically analyzed source code written in any programming language, and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data (e.g., streaming software tokens), and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cache-based n-grams on a corpus of Java projects. We experiment with two of the models' hyperparameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.|Research Paper|accept|accept
MSR|2015|Automatic Change Suggestions by Detecting Code Clones: An Exploratory Study|manishankar mondal,chanchal kumar roy,kevin a. schneider|Change Suggestion,Clone Detection,Change Pattern|In this research we automatically mine the evolution history of a software system in order to identify software change patterns and infer these patterns to suggest future changes through detection of code clones. According to our investigation on thousands of revisions of five diverse subject systems, Our proposed technique can provide change suggestions for overall 24.5% of the future changes by mining and inferring change patterns from the past evolution history, and for around 29% of the cases the suggestions provided by our technique contain the actual change to be implemented. We believe that our proposed mechanism for providing automatic change suggestions can help programmers reduce their implementation time considerably.|Research Paper|reject|reject
MSR|2015|Mining Energy-Aware Commits|irineu moura,gustavo pinto,felipe ebert,fernando castor filho|Energy aware mining,Github mining,Practitioners,Theme analysis|Over the last years, energy consumption has become a first class citizen in software development practice. While energy efficient solutions on lower-level layers of the software stack are well-established, there is convincing evidence that even better results can be achieved by encouraging practitioners to participate in the process. For instance, previous work has shown that using a newer version of a concurrent data structure can yield a 2.19x energy savings when compared to the old associative implementation [35]. However, in spite of that, little is known about how much software engineers are employing energy efficient solution into their applications and what solutions they employ for improving energy-efficiency. In this paper we describe a large-scale qualitative study of “energyaware commits”. Using Github as our primary data source, we perform a thorough analysis on an initial sample of over 2000 commits and carefully curate set of 280 energy-aware commits spread over 200 real-world non-trivial applications. Based on our study, we discover several interesting observations. For example, we find that software developers heavily rely on low-level energy management approaches, such as frequency scaling and multiple levels of idleness. Also, we observed that a ill-chosen energy saving technique can impact on the correctness of the application.|Research Paper|accept|accept
MSR|2015|Modifications, Tweaks, and Bug Fixes in Architectural Tactics|mehdi mirakhorli,jane cleland-huang|Architectural tactics,software quality,Software Evolution|Architectural qualities such as reliability, perfor-mance, and security, are often realized in a software system through the adoption of tactical design decisions such as the decision to use redundant processes, a heartbeat monitor, or a specific authentication mechanism. Such decisions are critical for delivering a system that meets its quality requirements. Despite the stability of high-level decisions, our analysis has shown that tactic-related classes tend to be modified more frequently than other classes and are therefore stronger predic-tors of change than traditional Object-Oriented coupling and cohesion metrics. In this paper we present the results from this initial study, including an analysis of why tactic-related classes are changed, and a discussion of the implications of these findings for maintaining architectural quality over the lifetime of a software system.|Research Paper|accept|accept
MSR|2015|A Characterization of Task Hierarchies in Issue Repositories for Software Development|casey albert thompson,gail c. murphy|characterization,mining,issue repositories|Software developers use issue repositories for many purposes, from recording features and how the features might be implemented to defects that have been found and how those defects might be fixed. The ways in which developers go about their work and the processes that are used are recorded in issue repositories in part by the organization of tasks into hierarchies by the developers. To better understand the task hierarchies used in issue repositories, we undertook a study of the hierarchies in the repositories for two open source systems. We applied a variety of methods to automatically group the task and sub-task pairs and then qualitatively analyzed a subset of these pairs. We discovered developers often use descriptive sub-tasks to fix bugs rather than the action oriented sub-tasks used for regular development of the project. This characterization of the structure of task hierarchies in software development may aid in the development of tools to recommend sub-tasks that a developer might need to undertake, in determining where processes are not being completely followed, or in enhancing existing techniques for automatically determining who should undertake particular tasks or which issues are duplicates of each other.|Research Paper|reject|reject
MSR|2015|Can Developers' Interaction Data Improve Change Recommendation?|akihiro yamamori,anders mikael hagward,takashi kobayashi|change guide,interaction history,commit history,software maintenance|Overlooking changes are one of the most common causes of bugs.  Many studies on change guide based on logical couplings extracted from change histories have been performed. While valuable change rules based on the logical coupling can be found from a change history, these rules often fail to find appropriate candidates and can't include many predictable ones because a change history in repositories only preserves the summary of changes between commits. To improve the performance, we've focused on the interaction data between a developer and an IDE. Such interaction data contain not only the detailed change history, but also reference activities between commits. In this paper, we investigate whether logical coupling extracted from interaction data can improve the recommendation performance. We use the interaction data of the Mylyn project recorded by the Mylyn plug-in itself, which is the largest available one in actual developments. Since the Mylyn plug-in doesn't discriminate change and reference in the interaction history, we generated virtual interaction data by combining Git revision histories and the interaction histories uploaded to Bugzilla. Experimental results show that interaction data has the capability to extend the chance of recommendation and to improve the accuracy of the recommendation while keeping the chance of the recommendation.|Research Paper|reject|reject
MSR|2015|Understanding the Rationale for Log Changes|heng li,weiyi shang,ying zou,ahmed e. hassan|software logs,log improvement,mining software repositories|Software developers typically insert logging statements in their source code to record runtime information of applications. Prior research finds that developers often change logs without considering the needs of other stakeholders. Such changes to logs can lead to failures and errors for log processing applications that depend on the changed logs. In order to help developers improve their logging practices, we first need to better understand why developers change logs. This work performs a case study on four open source projects (Hadoop, Directory Server, Commons HttpClient and Qpid) to investigate the rationale behind log changes. As the first step, we randomly sample a subset of log changes from these projects and manually infer the reasons for those log changes. Based on the results of the manual analysis, we derive a set of metrics and use random forest classifiers to study the rationale for log changes. We find that: (i) The rationale for log changes can be grouped along four categories: changing context code, fixing log issue, adding needed information, and removing redundant information; (ii) our random forest classifiers can effectively suggest whether to change logs, with a precision of 0.90 to 0.94 and a recall of 0.84 to 0.90; (iv) code changes in a revision and the static snapshot of the source code are important factors for log changes in the revision.|Research Paper|reject|reject
MSR|2015|Towards More Accurate Deleted Question Prediction on Stack Overflow|xin xia,david lo,denzil correa,ashish sureka,emad shihab|Deleted Question,Stack Overflow,Text Processing,Classification|Stack Overflow is a popular community-based Q&A website that caters to technical needs of software developers. As of February 2015 -- Stack Overflow has more than 3.9M registered users, 8.8M questions, and 41M comments. Stack Overflow provides explicit and detailed guidelines on how to post questions but, some questions are very poor in quality. Such questions are deleted by the experienced community members and moderators.  Deleted questions increase maintenance cost and have an adverse impact on the user experience. Therefore, predicting deleted questions is an important task. In this study, we propose a two stage hybrid approach -- DelPredictor -- which combines text processing and classification techniques to predict deleted questions. In the first stage, DelPredictor converts text in the title, body, and tag fields of questions into numerical textual features via text processing and classification techniques. In the second stage, it extracts meta features that can be categorized into: profile, community, content, and syntactic features. Next, it learns and combines two independent classifiers built on the textual and meta features. We evaluate  DelPredictor on 5 years (2008--2013) of deleted questions from Stack Overflow. Our experimental results show that DelPredictor  improves the F1-scores over baseline prediction, a prior approach  and a text-based approach by by 29.50%, 9.48%,  and 28.11%, respectively.|Research Paper|reject|reject
MSR|2015|Bayesian Approach to Grade Software Developers based on Historical Evidences|satya sai prakash kanakadandi,dhanyamraju s u m prasad,ashutosh shukla|Bayesian approach,Software developer,Project Complexity,Developer rating,Customer reported defects,internal defects|Grading/rating software developers is an involved, complex and highly subjective task. In our maiden effort to make this an objective, simplified and data driven, applied Bayesian approach on the relevant historical data that has been gathered across multiple repositories namely User profile system, SVN, defect repositories such as Bugzilla and project tracking systems such as Jira. Proposed Bayesian Grading System (BGS) mines the diverse software repositories and correlates the data to score each developer based on multitude of parameters such as years of experience, years of relevant experience, code check-in frequency, code completion duration, number of defects fixed, number defects reported (from the developed code), complexity of the task and resource location with reference to the team. This paper espouses the effective usage of informative and non-informative prior belief functions and ability to derive the conditional dependencies among the several parameters by using Bayes network related to each developer in a distributed identical software system development environment and use it further to arrive at a score. This BGS system has been experimented and validated with a couple of  projects whose life cycles spanned across 3+ years with 35+ releases based on bug fixes and CRs (Change Requests) having 20+ resources worked on each project.|Practice Paper|reject|reject
MSR|2015|Transforming Software Metrics for Better Cross-project Defect Prediction|feng zhang,iman keivanloo,ying zou|software metrics,defect prediction,cross-project,log transformation,box-cox transformation,rank transformation,ensemble|Software metrics are commonly used to build defect prediction models. Earlier studies have reported that many software metrics are not normally distributed (e.g., following power-law distribution). The non-normality in the distribution of software metrics can affect the performance of prediction models. A set of transformation methods are available to improve normality of software metrics. For instance, log and rank transformations are commonly used in earlier studies for defect prediction models. The Box-Cox transformation subsumes log and other power transformations (e.g., square root), but have not been investigated for defect prediction models. In this study, we first compare the ability of improving normality of software metrics among three transformation methods (i.e., log, Box-Cox, and rank). We then investigate the impact of the transformation methods on cross-project defect prediction. Finally, we propose an approach to integrate predictions by models built using the three transformations. We perform an experiment using three publicly available data sets (i.e., AEEEM, ReLink, and PROMISE) for cross-project defect prediction. The results show that our approach can significantly improve the performance of cross- project prediction models using logistic regression. Comparing to the baseline (i.e., models built using log transformation), our approach can improve F-measure by 48%, 50%, and 23% for the three data sets, respectively. The impact of a transformation method can vary using different classifiers. Hence, we further evaluate our approach using nine classifiers (e.g., Naive Bayes, decision tree, and random forest), and find that our approach outperforms the baseline in general for the nine classifiers.|Research Paper|reject|reject
MSR|2015|Studying Developers Copy and Paste Behavior|tarek ahmed,weiyi shang,ahmed e. hassan|Mining Software Repositories,Code Cloning,Copy and Paste,Eclipse UDC|Developers use Copy and Paste frequently. Despite the fact that Copy and Paste produces less maintainable code and increases the risk of code reuse issues, developers tend to paste code on a regular basis. To study the practice of copy and paste, automated approaches are proposed to identify cloned code. However, such automated approach can only identify where is the code that has been copied and pasted but little is known about the context of copy and paste. On the other hand, prior research studying actual copy and paste actions is based on a small number of users in an experimental setup. In this paper, we study the behavior of developers copying and pasting code in Eclipse IDE. We mine the usage data of Eclipse IDE from over 20,000 users. We aim to explore the different patterns of Copy and Paste (C&P) used by Eclipse developers during development. We compare such usage pattern to the regular users’ usage of copy and paste during non-programming tasks reported in earlier studies. Our findings would motivate developers and researchers to improve IDEs in several ways. We show that developers’ behavior using C&P is considerably different from the behavior of regular users. For example, developers tend to perform C&P in the same file contrary to regular users, accordingly, developers need different C&P support tools than regular users. Moreover, we find that C&P behavior across different programming languages is a frequent behavior as we extracted more 75,000 C&P incidents across different programming languages, therefore, Eclipse code cloning tools shall consider this type of behavior as major factor to efficiently detect code clones.|Research Paper|accept|accept
MSR|2015|Influence of Product Metrics on Source Code Changes and Defects by Future Organizations|seiji sato,hironori washizaki,yoshiaki fukazawa,sakae inoue,yoshiiku hanai,hiroyuki ono,mikihiko yamamoto,masanobu kanazawa,kazutaka sone,katsushi namba|Software transfer,change-proneness,product metrics,defects|Software is sometimes developed by multiple organizations. Because the new organization has a different understanding of the software, it may make undesired changes or create defects. Thus, it is important for developers to know which parts of the software are likely to be modified by a future organization. Although some approaches have been proposed to predict the change-proneness of software modules, they do not consider an organizational change. In this study, we utilize origin, our previously proposed metric, to investigate how files are modified when the development organization changes. Specifically, two open source projects, OpenOffice and VirtualBox, and one industrial embedded system project, which were all developed by three organizations, are analyzed. The relationships between product metrics, origin, modification scale, number of modifications, and defects reveal that files lacking modularity tend to be modified by future organizations, and such files are frequently and heavily modified when the organization changes, inducing defects.|Research Paper|reject|reject
MSR|2015|Exploring the Essential Content of Defect Prediction|divya ganesan,tim menzies|defect prediction,clustering,feature selection,data reduction|How shall we understand the intricacies of software analytics? One approach is to reduce defect data to its essential content, then reasoning about that reduced set. This paper explores methods (a) removing spurious and redundant columns then (b) clustering the dataset and replacing each cluster by one exemplar per cluster then (c)  making conclusions by extrapolating between the exemplars (via k=2 nearest neighbor between cluster centroids). We find that numerous defect data sets can be reduced to around 25 exemplars containing around 6 attributes. These tables of 25*6 values can then we used for (a) effective  and simple defect prediction as well as (b) simple presentation of that data.  Also, in an investigation of numerous common clustering methods, we find that the details of the clustering method are less important that ensuring that those methods produce enough clusters (which, for defect data sets, seems to be around 25 clusters).|Research Paper|reject|reject
MSR|2015|On the Influence of Mobile Device Features on the User-perceived Quality of Android Apps|ehsan noei,iman keivanloo,ying zou,ahmed e. hassan|mobile applications,software features,hardware features,user-perceived quality|The number of mobile applications (apps) and mobile devices has considerably increased in the past few years. Online app markets, such as Google play store, use the star-rating mechanism to quantify the user-perceived quality of mobile apps. Recent studies explore that the influence of app features, such as lines of code, on the user-perceived quality. However, the user-perceived quality reflects the end-users' experience using mobile apps on a variety of mobile devices. Hence, we conjecture that the user-perceived quality is not solely determined by app features. In this paper, we study the influence of both device features and app features on the user-perceived quality of Android apps from the Google play. We study the influence of 26 device features and 11 app features based on the data gathered from 83 Android mobile devices and 280 Android apps. We use logistic linear regression models to identify the device features with the most influence on the user-perceived quality.  We find that device features, such as CPU and camera resolution, have a significant influence on the user-perceived quality. However, we observe that improving the characteristics of a device features does not lead to a higher user-perceived quality in all cases. Furthermore, we find that the complexity of (UI) has the highest influence on the user-perceived quality relative to other app features. We observe that both app and device features play an important role in the user-perceived quality of the apps. Although, some device features, such as CPU, are more influential than app features.|Research Paper|reject|reject
MSR|2015|Wait For It: Determinants of Pull Request Evaluation Latency on GitHub|yue yu,huaimin wang,vladimir filkov,premkumar t. devanbu,bogdan vasilescu|Pull-request,Continuous Integeration,Pull-based Development,Empirical Software Engineering|The pull-based development model, enabled by git and popularised by collaborative coding platforms like BitBucket, Gitorius, and GitHub, is widely used in distributed software teams. While this model lowers the barrier to entry for potential contributors (since anyone can submit pull requests to any repository), it also increases the burden on integrators (i.e., members of a project’s core team, responsible for evaluating the proposed changes and integrating them into the main development line), who struggle to keep up with the volume of incoming pull requests. In this paper we report on a quantitative study that tries to resolve which factors affect pull request evaluation latency in GitHub. Using regression modeling on data extracted from a sample of GitHub projects using the Travis-CI continuous integration service, we find that latency is a complex issue, requiring many independent variables to explain adequately.|Research Paper|accept|accept
MSR|2015|On the Analysis of Co-occurrence of Anti-patterns and Clones|fehmi jaafar|Anti-patterns,Clones,Faults proneness,Software quality|In software engineering, a smell is any symptom in the source code of a software system that possibly indicates a deeper problem. Although many kinds of smells have been studied to analyze their causes, their behaviours and their impact on software quality, those smells typically are studied independently from each other. However, if two or smells coincide inside a class, they could interfere negatively (e.g., spaghetti code that is being cloned across the system). Since code smell interference has been studied in depth, this paper presents results from an empirical study aimed at understanding the relationship between two specific kind of smells: code clones and anti-patterns. We conducted our study on five open-source software systems: Azureus, Eclipse, JHotDraw, Lucene, and XalanJ. Results show that (1) between 8% and 63% of classes in the analysed systems present co-occurrence of smells, (2) such classes could be five times more risky in term of fault-proneness, (3) and those classes have a negative impact on software reliability and maintainability.|Research Paper|reject|reject
MSR|2015|Detecting and Mitigating Secret-Key Leaks in Source Code Repositories|vibha sinha,diptikalyan saha,pankaj dhoolia,rohan padhye,senthil mani|version management systems,security,api keys,empirical evaluation,amazon,github,program slicing|Several news articles in the past year highlighted incidents in which malicious users stole API keys embedded in files hosted on public source code repositories such as GitHub and BitBucket in order to drive their own work-loads for free. While some service providers such as Amazon have started taking steps to actively discover such developer carelessness by scouting public repositories and suspending leaked API keys, there is little support for tackling the problem from the code sharing platforms themselves. In this paper, we discuss practical solutions to detecting, preventing and fixing API key leaks. We first outline a handful of methods for detecting API keys embedded within source code, and evaluate their effectiveness using a sample set of projects from GitHub. Second, we enumerate the mechanisms which could be used by developers to prevent or fix key leaks in code repositories manually. Finally, we outline a possible solution that combines these techniques to provide tool support for protecting against key leaks in version control systems.|Research Paper|accept|accept
MSR|2015|What is the Gist? Understanding the Use of Public Gists on GitHub|weiliang wang,german poo-caamano,evan wilde,daniel m. german|Mining Software Repositories,GitHub,Gists,Empirical Software Engineering|GitHub is a popular source code hosting site which serves as a collaborative coding platform. The many features of GitHub have greatly facilitated developers’ collaboration, communication, and coordination. Gists are one feature of GitHub, which defines them as “a simple way to share snippets and pastes with others.” This three-part study explores how users are using Gists. The first part is a quantitative analysis of Gist metadata and contents. The second part investigates the information contained in a Gist: We sampled 750k users and their Gists (totalling 762k Gists), then manually categorized the contents of 398. The third part of the study investigates what users are saying Gists are for by reading the contents of web pages and twitter feeds. The results indicate that Gists are used by a small portion of GitHub users, and those that use them typically only have a few. We found that Gists are usually small and composed of a single file. However, Gists serve a wide variety of uses, from saving snippets of code, to creating reusable components for web pages.|Research Paper|accept|accept
MSR|2015|Identifying Software Process Management Challenges: Survey of Practitioners in a Large Global IT Company|monika gupta,ashish sureka,padmanabhuni srinivas,allahbaksh asadullah|Process Mining,Qualitative Study,Software Management,Software repositories,Analytics|Process mining consists of mining event logs generated from business process execution supported by Information Systems (IS). Process mining of software repositories has diverse applications because vast data is generated during Software Development Lifecycle (SDLC) and archived in IS like Version Control System (VCS), Peer Code Review (PCR) System, Issue Tracking System (ITS), and mail archives. However, there is need to explore its diverse applications on different repositories to directly benefit the managers. We conduct two phase surveys and interviews with managers in a large, global, IT company to identify the process challenges encountered by them while managing projects which can be addressed by novel applications of process mining. During the first phase of the study, we filter, group and formulate 30 generic problem statements by abstracting 130 responses collected from 46 participants. On the basis of process mining type, we classify identified problems to eight categories such as control analysis, organizational analysis, conformance analysis, and preventive analysis. Thereafter, in second phase of the study, we perform importance analysis survey with distinct participants to determine importance of solving the identified problems. We propose net importance metric and evaluate it based on 1262 ratings from 43 participants. We believe solving these validated problems will aid managers to improve the project quality and productivity. We also elaborate on possible solutions for most frequent and important problems.|Research Paper|accept|accept
MSR|2015|Mining Software Patents|german poo-caamano,daniel m. german|Intellectual Property,Replication Study,Software Patents|Previous research has documented the legal and economic perspective of software patents; proponents claim that patenting software foster innovation whereas critics claim that low quality of some of those patents granted discourage innovation and have a negative effect on small software companies and Free and Open Source Software (FOSS) projects. In this paper we propose that the use of MSR techniques to study software patents can help to better understand the evolution of software patents, consequences for the industry and software projects, and mainly to determine areas for future research. To explain this in more detail, we reproduced part of the empirical study on software patents conducted by Bessen and Hunt. The original study established a criteria to determine software patents, and provided a look at the evolution of patents granted until 2002. We present a simple approach to retrieve patents from the full text database provided by the US Patent and Trademark Office (USPTO), which is freely accessible. We compared our results against the original study, and then we expanded to study the evolution of software patents until 2012. In the new period studied, we observed an increase of software patents granted higher than the periods previously studied, and an increase of the proportion of software patents with respect to utility patents (the category they belong to). These findings open new research problems, such as, improving the algorithm to identify software patents, assessing the quality of the patents, identifying possible prior–art.|Research Paper|reject|reject
MSR|2015|A Study on the Role of Software Architecture in the Evolution and Quality of Software|ehsan kouroshfar,mehdi mirakhorli,hamid bagheri,lu xiao,sam malek,yuanfang cai|Software Repositories,Software Architecture,Defects|Conventional wisdom suggests that a software system's architecture has a significant impact on its evolution. Prior research has studied the evolution of software using the information of how its files have changed together in their revision history. No prior study, however, has investigated the impact of architecture on the evolution of software from its change history. This is mainly because most open-source software systems do not document their architectures. We have overcome this challenge using several architecture recovery techniques. We used the recovered models to examine if co-changes spanning multiple architecture modules are more likely to introduce bugs than co-changes that are within modules. The results show that the co-changes that cross architectural module boundaries are more correlated with defects than co-changes within modules, implying that, to improve accuracy, bug predictors should also take the software architecture of the system into consideration.|Research Paper|accept|accept
MSR|2015|Mining Questions about Code Clone from Stack Overflow|eunjong choi,norihiro yoshida,katsuro inoue|Stackoverflow,Code clone,Refactoring|Although much research has been done on the detection and the analysis of code clones in source code, only a little is known about the impact of the research into industrial and OSS development. Recently, several studies investigated practitioner's interest by mining Q&As in Stack Overflow (SO). In this paper, we present an investigation of SO in order to reveal practitioner's interests about code clones and then discuss future directions of research on code clone.|Research Paper|reject|reject
MSR|2015|How Do Bug Characteristics Differ Across Severity Classes: A Multi-platform Study|bo zhou,iulian neamtiu,rajiv gupta|Bug severity,Cross-platform analysis,Bug report,Topic model|Bugs of different severities have so far been lumped into the same category, but their characteristics differ significantly.  For example, the nature of, and process for, handling low-severity bugs differ significantly from those associated with medium-severity and high-severity bugs. Moreover, the nature of issues with the same nominal severity, e.g., high, differ markedly between desktop and smartphone platforms.  To understand these differences, we perform an empirical study on a bug corpus consisting of 72 projects on desktop and Android platforms.  First, we define and cluster bugs into three severity classes: high, medium, and low. Next, we study how severity might change and quantify differences between classes, from fix time to comment activity to developer expertise, and find that there are significant differences between classes.  Then, we focus on qualitative differences between severity classes: using Latent Dirichlet Allocation, we extract topics associated with each severity class on each platform, and study how these topics differ across classes, platforms, and over time. Our findings include: both severity and priority affect bug fixing time and there are marked differences in bug-fixing process features among the three severity classes on desktop; medium-severity contributors are more experienced than high-severity than low-severity contributors; and there have been more concurrency and cloud-related high-severity bugs on desktop since 2009, while on Android concurrency bugs have been and continue to be prevalent.|Research Paper|reject|reject
FSE|2015|A Taxonomy of Crowdsourcing Campaigns|majid alshehry,bruce ferguson|Crowdsourcing,Collaboration platform,Crowdfunding|Crowdsourcing serves different needs of different sets of users.   Most existing definitions and taxonomies of crowdsourcing address platform purpose while paying little attention to other parameters of this novel social phenomenon.  In this paper, we analyze 41 crowdsourcing campaigns on 21 crowdsourcing platforms to derive 9 key parameters of successful crowdsourcing campaigns and introduce a comprehensive taxonomy of crowdsourcing. Using this taxonomy, we identify crowdsourcing trends in two parameters, platform purpose and contributor motivation.  The paper highlights important advantages of using this conceptual model in planning crowdsourcing campaigns and concludes with a discussion of emerging challenges to such campaigns.|None|reject|reject
FSE|2015|Automatic Program Repairing for Specific Bugs|hao zhong,hong mei|Automatic program repair,Exception-related bugs,Bug fix|The battle against bugs starts ever since the beginning of software, with ups and downs in both sides. Until now, most programmers agree that it is still a time-consuming and error-prone task to fix software bugs. It has long been a hot research topic to fix bugs. In the traditional research direction, researchers analyze the characteristics of target bugs, and propose corresponding treating techniques. The limitation of this line is that the proposed approaches typically cannot effectively deal with the variety in bugs. Recently, researchers start to explore general approaches that fix all bugs. With several predefined repair operators and test cases, these approaches generate patches for previously unknown bugs, under the guide of search algorithms. Their results show promising results in fixing bugs, but skeptics doubt the potential of this research direction, since they believe bugs are too complicated and diverse to be fixed in this way. Indeed, it needs different knowledge to fix different types of bugs, and the search space of fixing general bugs becomes extremely large, due to the complexity of the task. As a result, even the most recent approaches fix only simple bugs. We believe that the sweet spot lies between the two research lines, and the divide-and-conquer strategy is essential to further improve the state-of-the-art. It is much easier to design repair operators and guide algorithms for specific bugs than for general bugs, and it does not hurt the generality of a bug-fix approach, since even the same type of bugs have much variety. In this paper, we select exception-related bugs to practice the divide-and-conquer strategy, since these bugs are many and important. In total, more than 20% of bugs are related to exceptions, and priorities of exception-related bugs are higher than the other bugs. We propose an approach, called MIMO, that mines repair models for exception-related bugs. For each exception, our mined repair model describes the history of known repair shapes and the probability distribution of repair actions. We have implemented a tool for MIMO, and evaluated its effectiveness on six popular projects. Our results show that our mined models are all significantly different from the general model. The results confirm that different types of bugs require significantly different repair knowledge, and a general approach may not work well on all the bugs. We selected 24 real bugs from another popular project and used both our models and the general model to generate correct repair shapes for these bugs. The results show that our models are more effective. Our practice on the key component of automatic program repair, i.e., searching repair shapes, highlights the effectiveness of our strategy.|None|reject|reject
FSE|2015|Bridging the gap between evolving compliance requirements and software architecture|syeda uzma gardazi,arshad ali shahid|Common Audit Framework,International Organization for Standardization (ISO) 9001,Health Insurance Portability and Accountability Act (HIPAA),Health Information Technology for Economic and Clinical Health (HITECH),Quality Management System (QMS) and,Office of Inspector General (OIG)|The United States legislation known as the Health Insurance Portability and Accountability Act (HIPAA) of 1996 aimed at strengthening patient rights, increasing efficiency and decreasing administrative costs. Under HIPAA all covered entities in the field of healthcare are required to ensure compliance with certain privacy and security rules concerned with protecting private patient health information. Building upon the objectives of HIPAA, the American Recovery and Reinvestment Act (ARRA) of 2009, in Section 13411 of the Health Information Technology for Economic and Clinical Health (HITECH) Act, require the Department of Health and Human Services (HHS) are mandated to conduct periodic audits of covered entities against HIPAA Privacy and Security Rules. This paper presents and evaluates a new approach to achieve compliance with HIPAA regulation using ISO 9001 guideline. The authors have used the United States based Healthcare Billing Transcription Company (UHBTC) with a backup office located in Pakistan as a case study. UHBTC develops software for mobile devices used by professionals in the US healthcare industry along with providing third party medical billing and transcription services. Recently the trend to access electronic health records (EHR) using mobile devices e.g. IPad, IPhone or Android phone has been tremendously increased among healthcare providers [13]. Understanding the need of hour, UHBTC has also developed mobile oriented practice and revenue cycle management software, applications and websites for private physician offices and hospitals throughout the US. Due to size and portability features of mobile devices, probability of possible data breaches has been increased. UHBTC as a Covered Entity under HIPAA/HITECH regulation is required to ensure HIPAA compliance while developing software. We propose an approach named Framework and Architecture to ensure Information Assurance and Regulatory Compliance (FAIR) that intends to help such types of software companies to track compliance with international security regulations and standards at software architecture level.|None|reject|desk reject
FSE|2015|Efficient Identification of Core and Dead Features in Variability Models|hector perez-morago,ruben heradio gil,david fernandez-amoros,roberto bean,jose antonio cerrada somolinos|Variability Model,Variability Management,Binary Decision Diagram,Core Feature,Dead Feature|Mass customization supports the creation of personalized products that fulfill the features desired by specific customers. In this context, variability models are used to specify which configurable features are supported and what constraints among the features must be satisfied to guarantee the validity of the derived products. As the market demand grows and evolves, variability models become increasingly complex. In such entangled models, it is hard to identify which features are absolutely essential or dispensable because they are required to be included or excluded from all the possible products, respectively. This paper exposes the limitations of existing approaches to automatically detect those features and provides an algorithm that efficiently performs such detection.|None|reject|reject
FSE|2015|Gamification for Enforcing Coding Conventions|christian prause,matthias jarke|coding conventions,gamification,software quality,experiment,prototype,code style|Software is a knowledge intensive product, which can only evolve if there is effective and efficient information exchange between developers. Complying to coding conventions improves information exchange by improving the readability of source code. However, without some form of enforcement, compliance to coding conventions is limited. We look at the problem of information exchange in code and propose gamification as a way to motivate developers to invest in compliance. Our concept consists of a technical prototype and its integration into a Scrum environment. By means of an experiment with two software teams and subsequent interviews, we show that gamification can effectively improve adherence to coding conventions. In the future, further evolved gamification concepts may help quality management to improve the internal quality of source code.|None|accept|accept
FSE|2015|Empirical Evaluation of UML Action Languages|omar badreddin|UML,Model Driven Development,Alf,Object Orientation,Model Oriented Programming Languages,Comprehension Experiment|Action Languages represent an is an emerging paradigm where modeling abstractions are embedded to bridge the gap with visual models, such as those of UML. The paradigm is gaining momentum, evident by the growing number of tools and standards that provide support for this paradigm. Materials and Methods. This paper is the first empirical work to execute a con-trolled experiment of model oriented paradigm. Participants are assigned to different artifacts to measure comprehension levels. Three notations are deployed; object oriented, Action Language, and UML visual models. Results. The experiment results suggest that Action Language code remains significantly better than traditional object oriented code. The experiment did not reveal any comprehension value for UML models when combined with object oriented or Action Language code. Conclusion. The data analysis suggests that Action Language code is more comprehensible than modern object oriented code, and seems to be as comprehensible as visual models.|None|reject|reject
FSE|2015|ARDev: Agile Research Development on Emerging Software Technologies|daniella bezerra,arilo dias-neto,raimundo barreto|Agile Research,Agile Methodology,Software Engineering,Emerging Technologies|Agile research development refers to the management of the research lifecycle, responding to change rather than simply following a set plan. By understanding the iteractive cycle and how to best manage factors that affect research productivity, researchers can often better understand, trust, reproduce, validate and optimize the research and development (R&D) process. The agile technique has the functionality to answer questions regarding the fragmentation of the complexity. It encourages self-organized research teams and more even time distribution. It focus on the delivery of results, observes the feedback, cooperation, open discussions and gives foresight of possible risks and errors. ARDev is a methodology based on Scrum Principles, and has the aim to supporting research designs, making them workable and agile. In this paper, we outline the steps involved in applying ARDev, analyze its effectiveness, and illustrate its use with two case studies, where we specifically evaluate the productivity, agility, adoptability and impact of applying ARDev.|None|reject|reject
FSE|2015|Uncertainty quantification of software reliability model for various debugging processes|jiajun xu,peng wang,shuzhen yao|reliability,web environment,NHPP,PDF method,risk assessment,uncertainty quantification|Internet provides great flexibility to web-based software development but also introduces great uncertainty for product maintenance. We propose a novel approach, the PDF method, to quantify the uncertainties associated with software reliability in web environment. Based on the stochastic model for various debugging process, we consider the environmental uncertainties collectively as a noise of arbitrary correlation. Under the new stochastic framework, one could compute the full statistical information of the debugging process, e.g. its probabilistic density function (PDF). Through a number of comparisons with historical data and existing methods, such as the classic non-homogeneous Poisson process (NHPP) model, the PDF method exhibits a closer fitting to observation. In addition to conventional focus on the mean value of fault detection, the newly-derived full statistical information could further help software developers make decisions on system maintenance and risk assessment.|None|reject|reject
FSE|2015|Exploration of the scalability of LocFaults approach for error localization with While-loops programs|mohammed bekkouche|Error localization,LocFaults,BugAssist,Off-by-one bug,Minimal Correction Deviations,Minimal Correction Subsets|A model checker can produce a trace of counterexample, for an erroneous program, which is often long and difficult to understand. In general, the part about the loops is the largest among the instructions in this trace. This makes the location of errors in loops critical, to analyze errors in the overall program. In this paper, we explore the scalability capabilities of LocFaults, our error localization approach  exploiting paths of CFG(Control Flow Graph) from a counterexample to calculate the MCDs (Minimal Correction Deviations), and MCSs (Minimal Correction Subsets) from each found MCD. We present the times of our approach on programs with While-loops unfolded b times, and a number of deviated conditions ranging from 0 to n. Our preliminary results show that the times of our approach, constraint-based and flow-driven, are better compared to BugAssist which is based on SAT and transforms the entire program to a Boolean formula, and further the information provided by LocFaults is more expressive for the user.|None|reject|reject
FSE|2015|SeFo-REL: A Semi-Formal Approach to Requirements Elicitation in Natural Language|christoph knauf,bodo igler,johannes schmitz-lenders|Semi-Formal Requirements Elicitation,Natural Language Processing,Domain Specific Languages|Several tools and approaches exist for the elicitation of formal requirements. However, stakeholders usually document business requirements informally, i.e. in natural language. This leads either to higher efforts for validation or to reduced quality and can have a negative impact on subsequent development phases. This article proposes a semi-formal approach that allows stakeholders to elicit requirements seemingly in natural language, while most, possibly all, of the aspects of each requirement are recorded formally. The approach employs a mixture of context-free-grammar parsing and natural language processing in an efficient way. A corresponding requirements elicitation tool is designed, implemented and evaluated to demonstrate the feasibility and practicality of the approach.|None|reject|reject
FSE|2015|Ohua: Implicit Dataflow Programming for Concurrent Systems|sebastian ertel,christof fetzer,pascal felber|dataflow,concurrency,parallel programming,functional programming,imperative programming,multi-core|Concurrent programming has always been a challenging task best left to expert developers. Yet, with the advent of multi-core systems, programs have to explicitly deal with multi- threading to fully exploit the parallel processing capabilities of the underlying hardware. There has been much research over the last decade on abstractions to develop concurrent code that is both safe and efficient, e.g., using message pass- ing, transactional memory, or event-based programming. In this paper, we focus on the dataflow approach as a way to design scalable concurrent applications. We propose a new dataflow engine and programming framework, named Ohua, that supports implicit parallelism. Ohua marries object- oriented and functional languages: functionality developed in Java can be composed with algorithms in Clojure. This al- lows us to use different programming styles for the tasks each language is best adapted for. The actual dataflow graphs are automatically derived from the Clojure code. We show that Ohua is not only powerful and easy to use for the programmer, but also produces applications that scale remark- ably well: comparative evaluation indicates that a simple web server developed with Ohua outperforms the highly- optimized Jetty server in terms of throughput while being competitive in terms of latency. We also evaluate the impact on energy consumption to validate previous studies indicating that dataflow and message passing can be more energy-efficient than concurrency control based on shared-memory synchronization.|None|reject|reject
FSE|2015|The Emerging Role of Data Scientists on Software Development Teams|miryung kim,thomas zimmermann,robert deline,andrew begel|Data Science,Software Analytics,Software Engineering,Empirical Study|Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their raison d’être in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists and describe a set of strategies that they employ to increase the impact and actionability of their work.|None|reject|reject
FSE|2015|FlexJava: Language Support for Safe and Modular Approximate Programming|jongse park,hadi esmaeilzadeh,xin zhang,mayur naik,william harris|Programming Language,Approximate Computing,Program Analysis,Modularity|Energy efficiency is a primary constraint in modern systems. Approximate computing is a promising approach that trades quality of result for gains in efficiency and performance. State-of-the-art approximate programming models require extensive manual annotations on program data and operations to guarantee safe execution of approximate programs. The need for extensive manual annotations hinders the practical use of approximation techniques. This paper describes FlexJava, a small set of language extensions, that significantly reduces the annotation effort, paving the way for practical approximate programming. These extensions enable programmers to annotate approximation- tolerant method outputs. The FlexJava compiler, which is equipped with an approximation safety analysis, automatically infers the operations and data that affect these outputs and selectively marks them approximable while giving safety guarantees. The automation and the language–compiler codesign relieve programmers from manually and explicitly annotating data declarations or operations as safe to approximate. FlexJava is designed to support safety, modularity, generality, and scalability in software development. We have implemented FlexJava annotations as a Java library and we demonstrate its practicality using a wide range of Java applications and by conducting a user study. Compared to EnerJ, a recent approximate programming system, FlexJava provides the same energy savings with significant reduction (from 2× to 17×) in the number of annotations. In our user study, programmers spend 6× to 12× less time annotating programs using FlexJava than when using EnerJ.|None|accept|accept
FSE|2015|Application Caching Approach to Store, Retrieve, and  Facet-Search Database Objects with Redis|adam pahlevi baihaqi|key-value store,Redis,website optimization,performance,cache,faceted search|The advancements in cloud computing allow many services which were once expensive to become much more affordable. Redis, a key/value store database, is one of that service that could cost $0 to operate. This is the service that is used by many household names including, but not limited to, GitHub, Twitter, Snapchat, Digg, and Flickr. It is known theoretically that in-memory key/value store such as Redis is faster to use than database. Therefore, data can be retrieved several times faster from the KV store rather than from the database. We attempt to proof this practically in this paper. The major contribution of this paper is by showing how data can be persisted in KV, and how it can be retrieved back. Several retrieval techniques are addressed in this paper including single retrieval, categorized retrieval, and faceted search retrieval.|None|reject|reject
FSE|2015|Query by Example in Large-Scale Code Repositories|vipin balachandran|code search,query-by-example,abstract syntax trees,locality-sensitive hashing|Searching code samples in a code repository is an important part of program comprehension. Even though, most of the existing tools for code search support syntactic element search and regex pattern search, they are text-based and cannot handle queries which are syntactic patterns. Different solutions have been proposed for querying syntactic patterns using specialized query languages, which implies a learning curve for the users. The querying would be more user-friendly if the syntactic pattern can be formulated in the underlying programming language (as a sample code snippet) instead of a specialized query language. In this paper, we propose a solution for 'query by example' problem using Abstract Syntax Tree (AST) structural similarity match. The query snippet is converted to an AST, then its subtrees are matched against AST subtrees of source files in the repository and similarity values of matching subtrees are aggregated to arrive at a relevance score for each of the source files. To scale this approach to large code repositories, we use locality-sensitive hash functions and numerical vector approximation of trees. Our experimental evaluation involves running control queries against a real project. The results show that our algorithm can achieve high precision (0.73) and recall (0.81) and scale to large code repositories without compromising quality.|None|reject|reject
FSE|2015|High Productivity Programmers Use Effective Task Processes|damodaram kamma,pankaj jalote|High Productivity Programmers,Low Productivity programmers,Task Processes,Field Study,Modeling|Software productivity is influenced by how efficiently programmers execute tasks assigned to them. For executing a task, programmers execute several steps. How the execution of these steps is organized by a programmer is referred to as task process. While overall software process has been well studied, the impact of task processes has not been studied much. In this work, we study the impact of task processes on productivity of programmers. We first model a task process as a Markov chain with each state representing a step, and then study the difference in task processes of high and low productivity programmers using the Euclidean distance between Markov chains. We applied this method in a field study conducted at Robert Bosch Engineering & Business Solutions Limited, a CMMi Level 5 software company. We analyzed the task processes of eighteen programmers from three live model-based unit-testing projects, and modeled them as Markov chains. We compared the task processes used by a programmer for executing similar tasks, and then compared the task processes of programmers within the same group (high/low productivity) and across groups. The results of the study indicate: a) each programmer uses similar task processes for executing similar tasks, though the task processes of a high productivity programmer are more similar than the task processes of a low productivity programmer, b) task processes of high productivity programmers are similar to each other, and c) task processes of a low productivity programmer differ widely from all other programmers.|None|reject|reject
FSE|2015|Strengthening the Requirements of Safety Critical System by means of UML Extensive Mechanism:  Road Traffic Management System (RTMS)|monika singh,dr. a.k. sharma,ruhi saxena|Unified modeling Language (UML),Extensible mechanism,safety critical system,Road Traffic Management System|The aim of this paper is to describe the extension mechanisms of the UML in real time system to strengthening the requirements and shows its usefulness by implementing in road traffic management system as a case study.  This paper also focus on the process of specifying, translating and verifying UML specifications for road traffic management system. Different kinds of existing UML Meta-models used in road traffic management system are analyzed UML traffic system by implementing the Stereotypes will be created based on the real time system using the analysis report.|None|reject|reject
FSE|2015|Responsive Designs in a Snap|nishant sinha,rezwana karim|Constraint-based design repair,HTML CSS Mobile,Design tree graphs,Multi-device designs|With the massive adoption of mobile devices with different form-factors, UI designers face the challenge of designing responsive UIs which are visually appealing across a wide range of devices. Designing responsive UIs requires a deep knowledge of HTML/CSS as well as responsive patterns - juggling through various design configurations and re-designing for multiple devices is laborious and time-consuming. We present DECOR, the first recommendation tool for creating multi-device responsive UIs. Given an initial UI design, user-specified design constraints and a list of devices, DECOR provides ranked, device-specific recommendations to the designer for approval. Design space exploration involves a combinatorial explosion: we formulate it as a design repair problem and devise several design space pruning techniques to enable efficient repair. An evaluation over real-life designs shows that DECOR is able to compute the desired recommendations, involving a variety of responsive design patterns, in less than a minute.|None|accept|accept
FSE|2015|A New Algorithm for Construction of a P2P Multicast Overlay Tree Based on Topological Distances|sergej alekseev,jorg schaffer|peer-to-peer,overly multicast tree,live streaming|Peer to Peer technology became popular recently, because it overcomes many limitations compared to the traditional client server paradigm. In this paper we present algorithms for the construction and the maintenance of a P2P overlay multicast tree based on topological distances. The essential idea of these algorithms is to build a multicast tree by choosing neighbours close to each other. The topological distances can be easily obtained by the browser using the geolocation API. Thus the implementation of algorithms can be done web-based in a distributed manner. We present proofs of our algorithms as well as practical results and evaluations.|None|reject|reject
FSE|2015|Hybrid Approach to Test Case Prioritization for Regression Testing|taiki homma,shingo takada,haruto tanno,morihide oinuma|regression testing,test case prioritization,interaction coverage,code coverage|Regression testing is important as each time software is modified, it must be retested to check that previously working functions still work correctly. Ideally, all test cases would be executed again, but this can be too time consuming due to the number of test cases being too large. Much work has thus been done on reducing the cost of regression testing. This paper focuses on test case prioritization. One commonly researched approach is to use code coverage. However, software may use libraries where code is not available. There also may be cases where faults reside in already-covered code, thus delaying their detection. Another well-researched approach uses interaction coverage which is based on combinatorial testing, where parameter-value combinations are the focus. This also has the issue of already-covered combination delaying the detection of faults. This paper proposes a hybrid approach to test case prioritization, which incorporates both code coverage and interaction coverage. We conducted an evaluation comparing our hybrid approach against test prioritization using a singular coverage scheme of either code coverage or interaction coverage. We found that code coverage and interaction coverage in our hybrid approach complements each other well resulting in a more effective prioritization.|None|reject|reject
FSE|2015|Trace Generation and Deterministic Execution for Concurrent Programs|raphael negrisoli batista,paulo sergio lopes de souza,simone do rocio senger de souza,rafael regis do prado,george gabriel mendes dourado,julio cesar estrella|testing,coverage,concurrent program,trace,replay,deterministic execution|This paper addresses non-determinism, one of the main problems of the structural testing of concurrent programs. It proposes new algorithms for trace file generation and deterministic execution of such programs. This paper presents a literature review of the most relevant related studies in this context, explaining the current lack of research in this field. Afterwards, new algorithms for programs with multiple paradigms of communication and synchronization (blocking and non-blocking point-to-point message passing, collective and shared memory) are described in detail. Validating and evaluating the performance of algorithms is carried out by doing experiments based on nine representative benchmarks, which exercise non-trivial aspects of synchronization found in real concurrent applications. The results obtained show that the algorithms present a robust behavior and they meet the proposed objectives, highlighting the overhead generated while executing them. The proposed algorithms are essential to automate the structural testing of concurrent programs and form a basis, so that new synchronizations can be executed automatically, increasing the source code coverage focusing on communication and synchronization edges.|None|reject|reject
FSE|2015|Bottom-up Context-Sensitive Pointer Analysis for Java|yu feng,xinyu wang,isil dillig,thomas dillig|Java,Pointer analysis,Bottom-up context sensitivity,Summary-based|This paper describes a new bottom-up subset-based and context-sensitive pointer analysis algorithm for Java. The main novelty of our technique is the constraint-based handling of virtual method calls and instantiation of method summaries. Since our approach generates fully polymorphic method summaries, it can achieve context-sensitivity without reanalyzing the same method multiple times. We have implemented the proposed algorithm in a tool called Scuba, and we compare its precision and performance  with k-CFA and k-object sensitive algorithms on large Java applications from the DaCapo and Ashes benchmark suites. Our experimental results show that our new algorithm achieves better or comparable precision to top-down k-CFA and k-object-sensitive analyses at only a fraction of the cost.|None|reject|reject
FSE|2015|Component Based Reliability Assessment from UML Models|vaishali chourey,meena sharma|UML,Activity Diagram,Reliability,Functional Flow,BlockSim Tool|This is an era of component based system development approaches for robust and reusable software. Model Based Development techniques have ventured diverse research directions to assure quality of the software product. Models developed during architecture and design phases are efficient tools to assess quality at an early stage. Functionality or behavior is only aspect that is predominantly tested. However, testing the extra-functional or non-functional properties of software systems is not frequently practiced. Reliability is one such important metric for system analysis. It requires that the components of complex software systems interact in a consistent manner. The paper focuses on developing a phenomenon to test system for such non-functional attributes. Our technique is composition of annotated design models to represent system and realization of its behavior. Assigning reliability specific attributes to components of the system will help the designers to understand the critical components and nature of their interaction. This approach translates the system components to Reliability Block Diagram (RBD) which is a reliability wise analytic representation of system. The estimations are performed using ReliaSoft's BlockSim tool. It enables several distinct features for reliability assessment and conveniently resembles association with UML models. The estimations and design improvements are possible to make the overall system further reliable. The analytic includes computations that are useful to understand the failure behavior of the components and the overall system. Assigning test importance based on criticality of the components enables better test visualization and prioritization.|None|reject|reject
FSE|2015|Intent, Tests, and Release Dependencies: Pragmatic Recipes for Source Code Integration|martin brandtner,philipp leitner,harald c. gall|source code integration,best practices,interview study,release management,repository mining,open source projects|Continuous integration of source code changes, for example, via pull-request driven contribution channels, has become standard in many software projects. However, the decision to integrate source code changes into a release is complex and has to be taken by a software manager. In this work, we identify a set of pragmatic recipes to support the decision making of integrating code contributions into a release. We interviewed senior developers, lead-developers, and software managers from industry to identify best-practices for source code integration. These interviews led to three pragmatic recipes plus variations, covering the isolation of source code changes, contribution of test code, and the linking of commits to issues. We analyze the development history of 21 open-source software projects, to evaluate whether, and to what extent, those recipes are followed in open-source, projects. The results of our analysis showed that open-source projects largely follow recipes on a compliance level of > 75%. Hence, we conclude that the identified recipes plus variations can be seen as wide-spread relevant best-practices for source code integration.|None|reject|pre-reject
FSE|2015|Industry Application of Continuous Integration Modeling: A Multiple-Case Study|daniel stahl,jan bosch|Agile software development,Software integration,Continuous integration,Automation,Modeling,Visualization|The popular agile practice of continuous integration has become an essential part of the software development process in many companies, sometimes to the extent that delivery to customer is impossible without it. Due to this pivotal role it is an important field of research to better understand the practice: how automated software integration flows behave, how they might be made to behave differently and what the impact of changing them would be. This paper investigates existing techniques for modeling and visualizing continuous integration through a systematic literature review and industry multiple-case study. The ability of these techniques to help professionals gain a better understanding of their continuous integration systems, as well as to identify improvements and determine their implications, is examined. In addition, guidelines for conducting similar modeling work are presented and feedback on the studied models provided. This work presents software professionals with demonstrated effective methods for communicating and understanding their system and to plan and analyze potential improvements to those systems and thereby enhancing their ability to improve the efficacy of their software development efforts.|None|reject|pre-reject
FSE|2015|Don't Panic: Reverse Debugging of USB Drivers|pavel dovgalyuk,denis dmitriev,vladimir makarov|Debugging,Deterministic replay,Reverse debugging,USB|Device drivers cause 85% of operating systems failures. Debugging of these failures is a very tough task because of kernel panics, blue screens of death, hardware volatility, long periods of time required to expose the bug, perturbation of the drivers by the debugger, and non-determinism of multi-threaded environment. This paper shows how the deterministic replay of the program execution can be used to eliminate these factors from the process of debugging. We created the reverse debugger based on multi-target whole-system simulator. Using this debugger one can investigate the failures even in the kernel code not affecting its behavior. Although reverse debugging was a subject of many prior researches, there is no widely available practical tool for debugging software on different platforms. We present reverse debugger as a practical tool, which was tested for i386, x86-64, and ARM platforms, for Windows and Linux guest operating systems. We show that this tool incurs very low overhead (about 10%). Such a low overhead allows using our tool for debugging real-time applications. The paper also presents an empirical study that demonstrates reverse debugging of the USB kernel drivers for Linux.|None|reject|reject
FSE|2015|On the Value of Parametric Software Effort Estimation|tim menzies,yang ye,george mathew,barry w. boehm,jairus hihn|effort estimation,nearest neighbor,COCOMO,CART,clustering,feature selection,prototype generation,bootstrap sampling,effect size,A12|Despite decades of research into software effort estimation, industry still makes most use of parametric methods developed in the 1970s. Why is there so little adoption of innovative estimation methods? One explanation is an absence of results showing that (1) parametric estimation is no longer useful and that (2) supposedly more innovation methods are comparatively better. Accordingly, this paper tries to demonstrate these two points. We were unsuccessful. From this study we conclude that (a) it is still valid and recommended practice to try parametric estimation rather than other methods that are supposedly more innovative; (b) it is useful to augment parametric estimation with some local calibration and column pruning (which are techniques discussed in this paper). Also, given the small size of effort estimation data sets, (c) the details of data collection must be given due consideration.|None|reject|reject
FSE|2015|How Relevant is Software Engineering Research?|david lo,nachiappan nagappan,thomas zimmermann|Software Engineering Research,Survey,Industry|The number of software engineering research papers over the last few years has grown significantly. An important question here is: how relevant is software engineering research to practitioners in the field? To address this question, we conducted a survey at Microsoft where we invited 3,000 industry practitioners to rate the relevance of research ideas contained in 571 papers ICSE and FSE papers that were published over a five year period. We received 17,913 ratings by 512 practitioners who labelled ideas as essential, worthwhile, unimportant, or unwise. The results from the survey suggest that practitioners are positive towards studies done by the software engineering research community: 71% of all ratings were essential or worthwhile. We found no correlation between the citation counts and the relevance scores of the papers. Through a qualitative analysis of free text responses, we identify several reasons why practitioners considered certain research ideas to be unwise. The survey approach is this paper is very lightweight, participants spent only 22.5 minutes to respond to the survey. At the same, the results can provide useful insights to conference organizers, authors, and the participating practitioners.|None|reject|reject
FSE|2015|Automatically Deriving Pointer Reference Expressions From Executions For Memory Dump Analysis|yangchun fu,zhiqiang lin,david brumley|dynamic data dependence analysis,function pointer integrity check,memory analysis,kernel rootkit defense|Given a crash dump or a kernel memory snapshot, it is often desirable to have a capability that can traverse the pointers to locate the root cause of the crash, or check their integrity to detect the control flow hijacks. To achieve this, one key challenge lies in how to locate where the pointers are. While locating a pointer usually requires the data structure knowledge of the corresponding program, an important advance made by this work is that we show a technique of extracting  {data reference expressions} for pointers through dynamic binary analysis. This novel {pointer reference expression} encodes how a pointer is accessed through the combination of a base address (usually a global variable) with certain offset and further pointer dereferences. We have applied our techniques to OS kernels, and our experimental results with a number of real world kernel malware show that we can correctly identify the hijacked kernel function pointers by locating them using the extracted pointer reference expressions when only given a memory snapshot.|None|accept|accept
FSE|2015|NRG-Loops: Conditionally Adjusting Applications to Conserve Power and Energy|melanie kambadur,martha a. kim|Energy,Power,Program adaptation|This paper introduces NRG-Loops, a source-level abstraction that allows an application to manage its own power and energy consumption through dynamic adjustments to functionality, performance, and accuracy. The adjustments, which come in the form of truncated, adapted, or perforated loops, are conditionally enabled as runtime power and energy constraints dictate.  NRG-Loops are portable across different hardware platforms and operating systems and are complementary to existing system-level efficiency techniques, such as DVFS and idle states. We also present a prototype C library that implements the NRG-Loops interface using hardware energy meters.  Using the library, we demonstrate four applications: (1) providing an estimated solution to a mathematical clustering problem when power is constrained, (2) adapting third-party advertisement playback to consume only a programmer-selected portion of a video game's energy, (3) reducing the parallelism of a substring search algorithm to cap power at a programmer-selected level below the system TDP, and (4) modifying the framerate of a video tracking benchmark to maximize quality within the confines of a given energy allotment. These changes require only 2-6 added or modified lines of code per program, yet they reduce power by up to 55%, reduce energy by up to 90%, and result in up to 12X better energy efficiency than system-level techniques.|None|reject|pre-reject
FSE|2015|Staged Program Repair in SPR|fan long,martin rinard|Program Repair,Transformation Schema,Condition Synthesis|We present SPR, a new program repair system that uses condition synthesis to instantiate transformation schemas to repair program defects. SPR’s staged repair strategy combines a rich space of potential repairs with a targeted search algorithm that makes this space viably searchable in practice. This strategy enables SPR to successfully find correct program repairs within a space that contains many meaningful and useful patches. The majority of these correct repairs are not within the search spaces of previous automatic program repair systems.|None|accept|accept
FSE|2015|Iterative Strategy Selection for Self-Adaptive Systems with Interval-Estimate Probabilities|guoxin su,taolue chen,yuan feng,david s. rosenblum,p.s. thiagarajan|Confidence interval,Markov Decision Process,Iterative decision making,Self-adaptive system,Value-iteration algorithm|Engineering self-adaptive systems involves decision making about adaptation strategies. Due to the dynamic environments in which such systems operate, a key characteristic is the uncertainty surrounding observations of certain metrics relevant to the adaptation. In view of this, the Markov Decision Process (MDP) model may be a suitable formal decision model. However, the selection of a runtime adaptation strategy  for self-adaptive systems usually is based on sample data, and is subject to practical constraints including the computational efficiency and the usage of sample data. Traditional MDP-based methods are less applicable to self-adaptive systems, because those methods usually rely on point estimates of probabilistic parameters and may lead be cumulative errors that downgrade the system performance, and because those methods do not support the tradeoffs between the correctness rate, computational efficiency and data usage. In this paper, we present a novel iterative formal approach to adaptation strategy selection, which employs interval estimates of probabilities and provides flexibility to adjust the aforementioned three aspects of constraints. We validate our approach with the Rainbow framework and present empirical results to demonstrate the advantage of our approach.|None|reject|pre-reject
FSE|2015|Extending the H-index to Classify Faulty Modules|sandro morasca|Logistic Regression,Faultiness,Fault-proneness,Estimation accuracy,Thresholds,H-index|Background. Fault-proneness estimation models provide an estimate of the probability that a software module is faulty. These models can be used to classify modules as faulty or non-faulty, by using a fault-proneness threshold: modules whose fault-proneness exceeds the threshold are classified as faulty and the others as non-faulty. However, the selection of the threshold value is to some extent subjective, and different threshold values may lead to very different results in terms of classification accuracy. Objective. We introduce and empirically validate a new approach to setting thresholds, based on an extension of the H-index defined in Bibliometrics, called the Fault-proneness H-Index. We define and use this extension to identify the most fault-prone modules, which are candidates for intensive Verification \& Validation activities. Method. We carried out the empirical validation on two data sets with different faultiness characteristics hosted on the PROMISE repository, by using T-times K-fold cross validation. We computed  $Precision$, $Recall$, and the $F-measure$ for the results obtained with our approach and compared them with the values obtained with other approaches. Results. In the empirical validation, our approach provides better $Precision$ results than most other thresholds, in a statistically significant way. Conclusions. Our approach seems to be able to contribute to accurately classifying modules as faulty or non-faulty.|None|reject|reject
FSE|2015|Developer See, Developer Do? Homophily and Influence in OSS Projects|david kavaler,vladimir filkov|social networks,networks and behavior,network evolution,behavior evolution,knowledge flow,homophily,influence,apache,open source software,stochastic actor-oriented model,siena|Learning to work with others is part and parcel of software development, especially in Open Source Software projects. The social and technical activities of all developers in those projects change and evolve together, organized in a network of socio-technical links. And as they do, traces are typically visible to all, in project repos and through communication between them. Just like open source code makes cloning a viable learning and coding mechanism, here we ask if the openly observable socio-technical behavior of developers in these projects affects others, and is perhaps imitated by them? We are interested in three behaviors in particular, which might be visible to all in a socio-technical network of developers: productivity, social linking, and file ownership. Since OSS projects change dramatically in both social and technical activity over time, static approaches, that either ignore time or simplify it to few slices, are frequently inadequate to study these networks. On the other hand, ad-hoc dynamic approaches are often only loosely supported by theory and can yield misleading findings. Borrowing from social network analysis, here we adapt for our purposes the stochastic actor-oriented models for studying the evolution of network properties. This modeling technique provides methods to study the interplay between behavior influence and network architecture as they change over time, in a statistically sound way. When we apply these modeling formalism to data from three Apache Software Foundation projects, we find evidence of preferential social linking in all projects. On the other hand, we find no evidence for the spread of either ownership or productivity behaviors through the networks. This work illustrates the potential of dynamical network models for studying influences affecting the software process. Data and scripts used in this work can be found at http://web.cs.ucdavis.edu/~filkov/software/ASF-Siena/.|None|reject|reject
FSE|2015|Syntactic and Semantic Differencing for Combinatorial Models|rachel tzoref-brill,shahar maoz|Combinatorial Models,Semantic Differencing,Software Evolution,Combinatorial Test Design|Combinatorial test design (CTD) is an effective test design technique, considered to be a testing best practice. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. As the system under test evolves, e.g., due to iterative development processes and bug fixing, so does the test space, and thus, in the context of CTD, evolution translates into frequent manual model definition updates. Manually reasoning about the differences between versions of real-world models following such updates is infeasible due to their complexity and size. Moreover, representing the differences is challenging. In this work, we propose a first syntactic and semantic differencing technique for combinatorial models. We define a canonical representation for differences between two models, and suggest a scalable algorithm for automatically computing it. We further use our differencing technique to analyze the evolution of 42 real-world industrial models. As part of the analysis we identify 6 change patterns that occur in combinatorial models evolution. The analysis provides evidence for the potential usefulness of our differencing approach. Our work advances the state-of-the-art in CTD with better capabilities for change comprehension and management.|None|reject|reject
FSE|2015|Inferring Software Behavioral Models with MapReduce|chen luo,fei he,carlo ghezzi|Model Inference,Parametric Trace,Log Analysis,MapReduce|Software systems are often built without developing any explicit model and therefore research has been focusing on automatic inference of models by applying machine learning to  execution logs. However, the logs generated by a real software system may be very large and the inference algorithm can exceed the processing capacity of a single computer. This paper focuses on inference of behavioral models and explore to use of MapReduce to deal with large logs. The approach consists of two distributed algorithms that perform trace slicing and model synthesis. For each job, a distributed algorithm using MapReduce is developed. With the parallel data processing capacity of MapReduce, the problem of inferring behavioral models from large logs can be efficiently solved. The technique is implemented on top of Hadoop. Experiments on Amazon clusters show efficiency and scalability of our approach.|None|reject|reject
FSE|2015|Compositional Regression Symbolic Execution through Incremental Summary Update and Solution Reuse|xiangyang jia,shi ying|regression analysis and verification,compositional symbolic execution,incremental verification,constraint solution reuse|Compositional symbolic execution is an approach that generates summaries for invoked methods (or code fragments) in a program, and reuses them when methods are called again. Summaries can also be reused in new runs of program to support regression symbolic execution. Current approaches treat summaries as monolithic entities in regression symbolic execution: a summary is either valid and can be reused, or it is invalid and cannot be reused. Often, however, an invalid summary may include some valid paths which can be reused, and the solutions of constraints that refer to invalid summaries might be reused in new runs. In this paper, we present a framework named IGUS, to support compositional regression symbolic execution by reusing summaries at a finer granularity level. IGUS regenerates the summaries for the changed code, and updates the affected paths of higher-level summaries in response to lower-level changes. Also, we enhance our existing constraint solution reuse framework GreenTrie, to reuse previous solutions of summary-enabled constraints, even if the constraint is partially changed in the new version program. The experiments show that IGUS and GreenTrie achieve significant performance improvement in compositional regression symbolic execution.|None|reject|reject
FSE|2015|An Empirical Study of Practitioners’ Perspectives on Green Software Engineering|irene manotas,christian bird,lori l. pollock,james a. clause|Green Software Engineering,Empirical Study,Survey|The energy consumption of software is an increasing concern as the use of mobile and data center-based, in-the-cloud services expands. While research in green software engineering is correspondingly increasing, the current lack of communication between researchers and practitioners could potentially lead to underused tools and techniques. This paper describes the first empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report findings from a quantitative, targeted survey of 247 practitioners that was motivated by and supported with qualitative data from 18 in-depth interviews. Major observations provide directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications.|None|reject|reject
FSE|2015|Questions Developers Ask While Diagnosing Potential Security Vulnerabilities With Static Analysis|justin smith,brittany johnson,emerson r. murphy-hill,bill chu,heather richter lipford|program comprehension,developer questions,software security engineering|Security tools can help developers answer questions about potential vulnerabilities in their code. A better understanding of the types of questions asked by developers may help toolsmiths design more effective tools. In this paper, we describe how we collected and categorized these questions by conducting an exploratory laboratory experiment with novice and experienced software developers. We equipped them with Find Security Bugs, a security-oriented static analysis tool, and observed their interactions with security vulnerabilities in an open-source system that they had previously contributed to. We found that they asked questions not only about security vulnerabilities, associated attacks, and fixes, but also questions about the software itself, the social ecosystem that built the software, and related resources and tools. For example, when participants asked questions about the source of tainted data, their tools forced them to make imperfect tradeoffs between systematic and ad hoc program navigation strategies.|None|reject|reject
FSE|2015|Evaluating the Flexibility of the Java Sandbox|zack coker,michael maass,tianyuan ding,claire le goues,joshua sunshine|empirical study,security,sandbox,frameworks|The ubiquitously-installed Java Runtime Environment (JRE) provides a complex, flexible set of mechanisms that support the execution of untrusted code inside a secure sandbox.  However, many recent exploits have successfully escaped the sandbox, allowing attackers to infect numerous Java hosts.  We hypothesize that the Java security model affords developers more flexibility than they need or use in practice, and thus its complexity compromises security without improving practical functionality.  We describe an empirical study of the ways benign open-source Java applications use and interact with the Java security manager. We found that developers regularly misunderstand or misuse Java security mechanisms, that benign programs do not use all of the vast flexibility afforded by the Java security model, and that there are clear differences between the ways benign and exploit programs interact with the security manager. We validate these results by deriving two restrictions on application behavior that restrict (1) security manager modifications and (2) privilege escalation.  We demonstrate that enforcing these rules at runtime stop a representative proportion of modern Java 7 exploits without breaking backwards compatibility with benign applications. These practical rules should be enforced in the JRE to fortify the Java sandbox.|None|reject|reject
FSE|2015|Extending RUP to Distributed Systems Development|carlos eduardo de barros paes,celso massaki hirata|Software Engineering,Fault Tolerance,Security,Performance,Software Development Process|The increasing number of distributed systems results from the progress of computing and communication technology.  Nowadays the development of mature software for those systems is accomplished using good practices of software engineering. The software engineering discipline provides processes, methods, techniques and tools that allow a suitable organization of the development process, as well as a quality result. Security, performance and fault tolerance are considered essential requirements and these quality requirements should be considered from the initial to the end phases of software development lifecycle. Nowadays, some software development processes were proposed considering this understanding and they provide some support to the development of systems for some of the aforementioned concerns. They generally extend RUP (Rational Unified Process). RUP is a well-known software engineering process that provides a disciplined approach to assigning tasks and responsibilities.  However, the proposals address them individually. There is not a holistic approach that considers these three relevant requirements together. In this paper, we propose an extension to RUP for the development of distributed systems with requirements of security, performance, and fault tolerance. The choice of these requirements is based on the availability of previous published work. All these aspects are embodied in RUP as a knowledge area (discipline), named distributed systems, with activities and roles defined according to the architecture of process engineering UMA (Unified Method Architecture). Examples were elaborated to clarify and show the feasibility of the proposal.|None|reject|reject
FSE|2015|An Incremental and Product Centric Approach to Product Line Evolution|lukas linsbauer,stefan fischer,roberto erick lopez-herrejon,alexander egyed|feature,product line,evolution|A Software Product Line (SPL) represents a longterm investment for companies. To keep SPLs up to date with changing requirements - different infrastructures, ecosystems, machinery or customer groups - they have to be maintained and evolved. The evolution of an SPL is more complex than the evolution of a single system because of the variability and the interdependencies between the SPL products. In this paper we introduce an approach that allows software engineers to evolve SPLs by evolving single products and incrementally merging them into the SPL portfolio. In our work, to exert evolution changes only individual products that contain the desired changes need to be modified. Changes are automatically propagated through the other products that are affected and valuable hints are provided along the way to help engineers preserve consistency in the SPLs. We evaluate our approach to show its feasibility on three diverse, publicly available SPLs.|None|reject|reject
FSE|2015|Externalizing and Improving the Process of Collaborative Systematic Reviews with Computational Support|fabio octaviano,cleiton silva,tony gorschek,sandra fabbri|collaborative systematic literature reviews,multiple reviewers,guidelines,StArt tool,Software engineering|Background: The Kitchenham systematic review (SR) is a methodology used to find and aggregate relevant existing evidence about a specific research question of interest. Traditionally an SR is performed by two reviewers, but multiple reviewers working collaboratively can perform it. Objective: The goal of this paper is to externalize the guidelines to conduct a collaborative SR and to propose some improvements in the split activity, using the StArt tool to support the process. Method: A collaborative SR process is detailed, including the proposed improvements in the split activity. In addition, the support provided by StArt for the process is presented. Aiming to evaluate the collaborative process, we conducted an experiment to compare the efficiency and effectiveness of a collaborative SR (performed by multiple reviewers divided in groups using and not using shared primary studies) and a traditional SR (performed by two reviewers). Results: The experiment showed that the collaborative SR can be conducted in a shorter time than the traditional SR with a low percentage of errors. The use of studies in common shared among reviewers helped to minimize the number of not relevant papers included in the initial selection activity. Conclusions: The proposed collaborative SR process help in minimizing efforts of reviewers to perform a SR with a large set of primary studies. The computational support provided by StArt helped reviewers in the activities of the whole collaborative SR process.|None|reject|reject
FSE|2015|Automatically Categorizing Software Technologies|mathieu nassif,christoph treude,martin p. robillard|Traceability,Information Extraction,Software Documentation,Application Programming Interfaces,Software Frameworks|Informal language and the absence of a standard taxonomy for software technology makes it difficult to reliably analyze technology trends on discussion forums and other on-line venues. We propose an automated approach called Witt for the categorization of software technologies (an expanded version of the hypernym discovery problem). Witt takes as input a phrase describing a software technology or concept and returns a general category that describes it (e.g., integrated development environment), along with attributes that further qualify it (commercial, php, etc.). Our approach relies on Stack Overflow and Wikipedia, and involves numerous original domain adaptations and a new solution to the problem of normalizing automatically-detected hypernyms. We compared Witt with five independent taxonomy tools and found that, when applied to software terms, Witt demonstrated better coverage than all evaluated alternate solutions, without a corresponding degradation in the relative number of false positives.|None|reject|reject
FSE|2015|The Influences of Edge Instability on Change Propagation and Connectivity in Call Graphs|lei wang,han li,ping wang|Change Propagation,Software Evolution,Complex Networks,Network Model|During the lifetime of any software there are numerous changes, which lead to a large number of versions over time. A change in the software can ripple through a software system and affect components that need to be updated accordingly. The amount of effort in programming and debugging for these updates and therefore the reliability of the software depends substantially on how far the change propagates. We introduced the concept of Propagation Scope (PS) to quantify change propagation and investigated several open-source software systems. We found that the propagation property varies even with systems of similar scales. According to the asymmetry between the in-degree and out-degree distributions, we defined Edge Instability (EI) to measure the propagation of a call graph. Analyzing newly added nodes in six software, we found that the new nodes exhibited preferential attachment behaviors and were more likely to call new nodes. We proposed a model based on these observations to adjust EI and Clustering Coefficient (CC). CC has been believed to be the major factor determining the propagation scope in a network. Our experiments showed, however, that EI had a larger impact on the propagation of Direct Acyclic Graphs (DAGs). In both real software and our model, we measured the connectivity of call graphs with EI and evaluated connectivity under three edge-removal strategies. Our experiments showed that removing edges with high EI s hurt network connectivity the most.|None|reject|reject
FSE|2015|Heterogeneous Cross-company Defect Prediction by Unified Metric Representation and CCA-based Transfer Learning|xiaoyuan jing,fei wu,xiwei dong,fumin qi,baowen xu|Heterogeneous cross-company defect prediction (HCCDP),Common metrics,Company-specific metrics,Unified metric representation,Canonical correlation analysis (CCA)|Background. Cross-company defect prediction (CCDP) learns a prediction model by using training data from one or multiple projects of a source company and then applies the model to the target company data. Existing CCDP methods are based on the assumption that the data of source and target companies should have the same software metrics. However, for CCDP, the source and target company data is usually heterogeneous, namely the metrics used and the size of metric set are different in the data of two companies. We call CCDP in this scenario as heterogeneous CCDP (HCCDP) task. Objective. In this paper, we aim to provide an effective solution for HCCDP. Method. We propose a unified metric representation (UMR) for the data of source and target companies. The UMR consists of three types of metrics, i.e., the common metrics of the source and target companies, source-company specific metrics and target-company specific metrics. To construct UMR for source company data, the target-company specific metrics are set as zeros, while for UMR of the target company data, the source-company specific metrics are set as zeros. Based on the unified metric representation, we for the first time introduce canonical correlation analysis (CCA), an effective transfer learning method, into CCDP to make the data distributions of source and target companies similar. Results. Experiments on 14 public heterogeneous datasets from four companies indicate that: 1) for HCCDP with partially different metrics, our approach significantly outperforms state-of-the-art CCDP methods; 2) for HCCDP with totally different metrics, our approach obtains comparable prediction performances in contrast with within-project prediction results. Conclusion. The proposed approach is effective for HCCDP.|None|accept|accept
FSE|2015|Secure Bisimulation for Interactive Systems|guanjun liu,changjun jiang|Bisimulation,behavior equivalence,labeled Petri nets,security-oriented interactive systems|In the internet environment, security is one of the most important topics for interactive systems such as electronic trading system. Different security policies can result in different securities for these security-oriented interactive systems. We find that a security policy can affect the behavior of a system, which is exactly the reason why a good policy can strengthen the security of a system. In other words, for two interactive systems that accomplish the same function but have different security policies, their behaviors are not equivalent, but by the (weak) bisimulation theory we draw an opposite conclusion that they have equivalent behaviors. The (weak) bisimulation theory is not suitable for these security-oriented interactive systems. This paper extends the notion of (weak) bisimulation and thus put forward \emph{secure bisimulation}. This extended notion can solve the above problem. Based on secure bisimulation, we furthermore define a binary relation to compare the securities of two systems. This comparison operator is shown to be a partial order but not a total one. That is to say, the non-equivalence of securities of two systems does not mean that one must be securer than another one. This is also coincident with the reality. To the best of our knowledge, it is the first time to utilize the formal method to consider the behavioral equivalence for security-oriented interactive systems and to compare their securities. Therefore, it is helpful for the design/development of such systems.|None|reject|reject
FSE|2015|Heterogeneous Defect Prediction|jaechang nam,sunghun kim|defect prediction,quality assurance,heterogeneous metrics|Software defect prediction is one of the active research areas in software engineering. We can build a prediction model with defect data collected from a software project and predict defects in the same project, i.e. within-project defect prediction (WPDP). Researchers also proposed cross-project defect prediction (CPDP) to predict defects for a new project lacking in defect data by using prediction models built by other projects. In recent studies, CPDP has been proved to be feasible. However, CPDP requires projects that have the same metric set, meaning metric sets should be identical between projects. As a result, current techniques for CPDP are difficult to apply across projects with heterogeneous metric sets. To address the limitation, we propose heterogeneous defect prediction (HDP) to predict defects across projects with heterogeneous metric sets. Our HDP approach conducts metric selection and metric matching to build a prediction model between projects with heterogeneous metric sets. Our empirical study on 28 subjects shows that about 68% of predictions using our approach outperform or comparable to WPDP with statistical significance.|None|accept|accept
FSE|2015|Software Process Improvement in Agile Software Development: A Systematic Literature Review|celio santana,fabiola queiroz,alexandre vasconcelos|Software Process Improvement,Agile Software Development,Systematic Literature Review|In this paper, we describe the formatting guidelines for ACM SIG It is recognized the relevance and importance that software process improvement (SPI) and agile development have gained in the field of software engineering. Both are approaches that increase the efficiency and effectiveness of a software development organization and to enhance software products. This paper aims to identify and characterize SPI in agile environments through a systematic literature review, including 423 papers published between 2001 and March/2013. The selected papers were classified according to SPI aspects. Distinct approaches to Agile SPI were identified, the most common is to adapt traditional approaches of SPI in agile environments, but there are novel approaches such as improving people behavior or even improving practices being innovative in SPI consideration. Conducting SPI initiatives in agile environments is quite different from the traditional one, becoming necessary to adapt existing SPI approaches or creating new methods. The main discussion about agile SPI is: what the role of SPI in a context with people and interactions are more valued than process and tools.|None|reject|reject
FSE|2015|Assured and Correct Dynamic Controller Update|leandro nahabedian,victor a. braberman,nicolas d'ippolito,shinichi honiden,jeffrey kramer,kenji tei,sebastian uchitel|Dynamic Controller Update,Adaptive Systems,Controller Synthesis|Continuous operation is a common software-intensive system quality attribute in many application domains. Thus, there is a need for sound engineering techniques that can change a system without stopping or disturbing its operation in the face of environment and requirements changes. In this paper we address the problem dynamic controller update when the specification (both environment assumptions and requirements) of the current system change. We present a general solution that computes a controller for the new specification, handles the transition from the old to the new, assures that the current system will reach a state in which such a transition can correctly occur. Indeed, using controller synthesis we show how to automatically build a controller that guarantees both progress towards update and also a safe update.|None|reject|reject
FSE|2015|What change history tells us about thread synchronization|rui gu,guoliang jin,linjie zhu,shan lu|multi-threaded software,empirical study,change history,concurrency bugs,software reliability|Multi-threaded programs are pervasive and difficult to write. Too little synchronization leads to correctness bugs and too much synchronization leads to performance problems. To improve the correctness and efficiency of multi-threaded software, we need a better understanding of synchronization challenges faced by real-world developers. This paper studies the code repositories of open-source multi-threaded software projects to obtain a broad and in-depth view of how developers handle synchronizations. We first examine how critical sections are changed when software evolves by examining over 250,000 revisions of four representative open-source software projects. The findings help us answer questions like how often is synchronization an afterthought for developers; whether it is difficult for developers to decide critical section boundaries and lock variables; and what are real-world over-synchronization problems. We then conduct case studies to better understand (1) how critical sections are changed to solve performance problems (i.e. over-synchronization issues) and (2) how software changes lead to synchronization-related correctness problems (i.e. concurrency bugs). This in-depth study shows that tool support is needed to help developers tackle over-synchronization problems; it also shows that concurrency bug avoidance, detection, and testing can be improved through better awareness of code revision history.|None|accept|accept
FSE|2015|Finding and Analyzing Compiler Warning Defects|chengnian sun,vu le,zhendong su|compiler bug,compiler warning,differential testing|Good compiler diagnostic warnings facilitate software development as they indicate likely programming mistakes or code smells. However, due to compiler bugs, the warnings may be erroneous, superfluous or missing, even for mature production compilers like GCC and Clang. In this paper, we (1) propose the first randomized differential testing technique to detect compiler warning defects and (2) describe our extensive evaluation in finding warning defects in widely-used C compilers. At the high level, our technique leverages random program generators to emit a variety of compiler warnings, aligns the warnings from different compilers, and identifies inconsistencies as potential bugs. We develop effective techniques to overcome two specific challenges: (1) How to align warnings, and (2) how to reduce test programs for bug reporting? Our technique is very effective — within five months of extensive testing, we have found and reported 58 bugs for GCC (36 confirmed, assigned or fixed) and 31 for Clang (7 confirmed or fixed). This case study not only demonstrates our technique’s effectiveness, but also highlights the need to continue improving compilers’ warning support, an essential, but rather neglected aspect of compilers.|None|reject|reject
FSE|2015|The impact of human factors on agile projects: A systematic literature review.|aline chagas,celio santana,alexandre vasconcelos|Human Factors,Agile Software Development,Systematic Literature Review|an important aspect of software development is the influence of people and their interactions in the quality of final product. Human factors have great relevance in software development in both approaches, traditional or agile. However, researches about the role of human factors in software development are still scarce. In this light, this research aims to present a secondary study about the impact of human factors in agile software projects. We have conducted a systematic literature review (SLR) to investigate how human factors impact these projects. During the synthesis of 48 resulting studies, can be seen that the most cited human factors were: Communication (23 papers), Collaboration (6 papers) and Trust (8 papers). Some exceptions were found for Communication, Customer Involvement and Autonomy factors. So we conclude that Communication, Trust and Collaboration are important factors in projects using agile methods. It is worth mentioning that other factors have given importance as knowledge, leadership and motivation as results of SLR.|None|reject|reject
FSE|2015|Quantifying Developers’ Adoption of Security Tools|jim witschey,olga zielinska,allaire welk,emerson r. murphy-hill,chris mayhorn,thomas zimmermann|security tools,adoption,diffusion of innovation|Security tools could help developers find critical vulnera- bilities, yet such tools remain underused. We surveyed de- velopers from 14 companies and 5 mailing lists about their reasons for using and not using security tools. The result- ing thirty-nine predictors of security tool use provide both expected and unexpected insights. As we expected, devel- opers who perceive security to be important are more likely to use security tools than those who do not. However, that was not the strongest predictor of security tool use, it was instead developers’ ability to observe their peers using secu- rity tools|None|reject|reject
FSE|2015|Trace Links – A Novel Data Source for Ontology Generation in Software-Intensive Projects|jin guo,natawut monaikul,jane cleland-huang|Traceability,Safety Critical,Ontology Building|Software-intensive projects are specified and modeled using domain terminology. These concepts, and their interrelationships, can be documented in an ontology and used to enhance communication, clarify domain assumptions, enable reuse, and facilitate meaningful analysis of text-based artifacts. Ontology is particularly useful for supporting automated tasks such as traceability, ambiguity analysis, and project level Q&A that rely upon text-based processing techniques. However, building ontology for the highly technical software and system engineering domains is a complex task which requires significant domain expertise and human effort. We present and evaluate a hierarchy of ontology-building solutions -- beginning with a cold-start approach based purely on analyzing available domain documents, transitioning through a highly novel approach in which initial artifacts of the engineering process are used to supervise and constrain the ontology building process, and culminating with a human-in-the-loop approach.  We show how progressively useful ontology can be created as we transition through the hierarchy. Our approach is illustrated and evaluated across the three domains of Driverless Vehicle Control, Electronic Health Records, and Medical Infusion Pumps.|None|reject|reject
FSE|2015|TLV: Abstraction through Testing, Learning and Validation|jun sun,hao xiao,yang liu,shang-wei lin,shengchao qin|Specification Mining,Test Case Generation,Predicate Abstraction,Symbolic Execution|A (Java) class provides a service to its clients (i.e., programs which use the class). The service must satisfy certain specifications. Different specifications might be expected at different levels of abstraction depending on the client's objective. In order to effectively contrast the class against its specifications, whether manually or automatically, one essential step is to automatically construct an abstraction of the given class at a proper level of abstraction. The abstraction should be correct (i.e., over-approximating) and accurate (i.e., with few spurious traces). We present an automatic approach, which combines testing, learning, and validation, to constructing an abstraction. Our approach is designed such that a large part of the abstraction is generated based on testing and learning so as to minimize the use of heavy-weight techniques like symbolic execution. The abstraction is generated through a process of abstraction/refinement, with no user input, and converges to a specific level of abstraction depending on the usage context. The generated abstraction is guaranteed to be correct and accurate. We have implemented the proposed approach in a toolkit named TLV and evaluated TLV with a number of benchmark programs as well as three real-world ones. The results show that TLV generates abstraction for program analysis and verification more efficiently.|None|accept|accept
FSE|2015|Effective Test Suites for Mixed Discrete-Continuous Stateflow Controllers|reza matinnejad,shiva nejati,lionel c. briand,thomas bruckmann|test suite effectiveness,mixed discrete-continuous Stateflow,meta-heuristic search algorithms,adaptive random search,embedded software controllers|Modeling mixed discrete-continuous controllers using Stateflow is common practice and has a long tradition in the embedded software system industry.  Testing Stateflow models is  complicated by expensive and manual test oracles that are not amenable to full automation due to the complex continuous behaviors of such models. In this paper, we reduce the cost of manual test oracles by providing test case selection algorithms that help engineers develop small test suites with high fault revealing power for Stateflow models.  We present six test selection algorithms for  discrete-continuous Stateflows:  An adaptive random test selection algorithm that diversifies test inputs, two white-box coverage-based algorithms, a black-box algorithm that diversifies test outputs, and two search-based black-box algorithms that aim to maximize the likelihood  of presence of continuous output failure patterns. We evaluate and compare our test selection algorithms, and find that our three output-based algorithms consistently outperform  the coverage- and input-based algorithms in revealing faults in  discrete-continuous Stateflow models. Further, we show that our output-based algorithms are complementary as the two search-based algorithms perform best in revealing specific failures with small test suites, while the output diversity algorithm is able to identify different failure types better than other algorithms when test suites are above a certain size.|None|accept|pre-accept
FSE|2015|Augmenting API Documentation with Insights from Stack Overflow|christoph treude,martin p. robillard|software documentation,machine learning,Stack Overflow,API documentation,summarization|Software developers need access to different kinds of knowledge which is often dispersed among different documentation sources, such as API documentation or Stack Overflow. We present an approach to automatically augment API documentation with "insight sentences" from Stack Overflow---sentences that are related to a particular API type and that provide insight not contained in the API documentation of that type. Based on a development set of 1,574 sentences, we compare the performance of two state-of-the-art summarization techniques as well as a pattern-based approach for insight sentence extraction. We then present SISE, a novel machine learning based approach which uses as features the sentences themselves, their formatting, their question, their answer, and their users as well as part-of-speech tags and the similarity of a sentence to the corresponding API documentation. With SISE, we were able to achieve a precision of 0.64 and a coverage of 0.7 on the development set. In a comparative study with eight software developers, we found sentences extracted by SISE to be significantly more meaningful than those extracted by the baseline approaches. These results indicate that taking into account the meta data available on Stack Overflow as well as part-of-speech tags can significantly improve unsupervised extraction approaches when applied to Stack Overflow data.|None|reject|reject
FSE|2015|Fitness Landscape Characterisation for Constrained Software Architecture Optimisation Problems|aldeida aleti,i. moser|Software architecture optimisation,fitness landscape characterisation,reliability,constraints|The automation of software architecture design is an important goal in software engineering. A plethora of automated design exploration techniques have been devised in the last decades to handle the complexity of making design decision in large scale, complex software systems. The common aim of these methods is the optimisation of quality attributes, such as reliability and safety. The majority of approaches use heuristic methods, such as local search or genetic algorithms. Such methods are designed to use gradients in the fitness space to guide the search to the local optimum. When problems are constrained, search gradients are disrupted by infeasible regions, which may have a great impact on the difficulty of solving optimisation problems. Discovering the conditions under which a search heuristic will succeed or fail is critical for understanding the strengths and weaknesses of different software architecture optimisation methods. This paper investigates how to adequately characterize the features of constrained problem instances that have impact on difficulty in terms of algorithmic performance, and how such features can be defined and measured for the component deployment optimisation problem. We employ fitness landscape characterisation metrics that measure uniformity of the gradients in the search space, and investigate how two different constraints shape the search space, and as a result affect the performance of software architecture optimisation approaches.|None|reject|pre-reject
FSE|2015|Using Static Analysis in Regression Testing for Deadlocks|tuba yavuz|deadlock detection,static analysis,regression testing|This paper presents an approach that integrates static analysis to regression testing of multithreaded Java applications to help detect changes that may introduce potential deadlocks. Our approach leverages bug reports of manifested deadlocks and proposes to keep a  watch list of lock type pairs that had been involved in a deadlock. We have implemented a simplified version of the deadlock analysis algorithm presented in [13] using the WALA static analysis framework. To guide the users in navigating though the set of potential deadlock scenarios, our algorithm associates a ranking score with each unique scenario based on the possible set of threads that may realize it and the level of accessibility of the participating methods. Our experiments on several case studies show that our approach is effective in 1) guiding developers to devise a complete fix that eliminates not only the reported deadlock scenario but all similar cases as well and 2) highlighting code changes that may potentially introduce deadlocks.|None|reject|reject
FSE|2015|Crowd Debugging|fuxiang chen,sunghun kim|Crowd Debugging,Crowd Sourcing,Debugging|Research shows that, in general, many people turn to multiple QA sites to solicit answers to their problems. We observe in Stack Overflow a huge number of recurring questions, 1,632,590, despite mechanisms having been put into place to prevent these recurring questions. Recurring questions imply developers are facing similar issues in their source code. However, limitations exist in QA sites. Developers need to visit them frequently and/or should be familiar with all the content to take advantage of the crowd's knowledge. Due to the large and rapid growth of QA data, it is difficult, if not impossible for developers to catch up. To address these limitations, we propose mining QA site to leverage the huge mass of crowd knowledge to help developers debug their code. Our approach reveals 172 warnings and 157 (91.3%) of them are confirmed by developers from eight high-quality and well-maintained projects. Developers appreciate these findings because the crowd provides solutions and comprehensive explanations to the issues. We compared the confirmed bugs with three popular static analysis tools (FindBugs, JLint and PMD). Of the 157 bugs identified by our approach, only FindBugs detected six of them whereas JLint and PMD detected none.|None|accept|accept
FSE|2015|Economic Analysis for Problem Resolution in Structural Test Generation|sihan li,xusheng xiao,tao xie,nikolai tillmann|Structural test generation,Economic analysis,Symbolic execution,Problem resolution|Automatic tools that assist various software engineering activities often encounter problems that compromise their effectiveness when dealing with complex programs. Resolving these problems often costs substantial human or machine efforts. In practice, the tool users such as developers tend to plan their eorts in an ad-hoc way (e.g., using an arbitrary or default order), causing low return-on-investment. To help developers better invest their eorts, particularly in structural test generation, we propose an economic-analysis framework, EcoCov, for economically resolving problems encountered by a test-generation tool based on developers' testing goals (e.g., maximizing the structural coverage). EcoCov prioritizes problems to be resolved via computing two key estimates: the gain from a certain investment (e.g., coverage improvement) and the cost of investment for a certain gain (e.g., the number of problems to be resolved). We evaluate EcoCov on four popularly used real-world open source projects, and the results show that EcoCov achieves average 94.6% precision and 90.7% recall in estimating the coverage improvement of resolving a problem, and successfully estimates the cost of covering a branch in 31 out of 40 cases.|None|reject|reject
FSE|2015|Detecting JavaScript Races that Matter|erdal mutlu,serdar tasiran,v. benjamin livshits|race detection,symbolic exploration,JavaScript,asynchrony,non-determinism|As JavaScript has become virtually omnipresent as the language for programming large and complex web applications in the last several years, we have seen an increase in interest in finding data races in client-side JavaScript. While JavaScript execution is single-threaded, there is still enough potential for data races, created largely by the non-determinism of the scheduler. Recently, several academic efforts have explored both static and run-time analysis approaches in an effort to find data races. However, despite this, we have not seen these analysis techniques deployed in practice and we have only seen scarce evidence that developers find and fix bugs related to data races in JavaScript. In this paper we argue for a different formulation of what it means to have a data race in a JavaScript application and distinguish between benign and harmful races, affecting persistent browser or server state. We further argue that while benign races — the subject of the majority of prior work — do exist, harmful races are exceedingly rare in practice (19 harmful vs. 621 benign). Our results shed a new light on the issues of data race prevalence and importance. To find races, we also propose a novel lightweight run-time symbolic exploration algorithm for finding races in traces of run-time execution. Our algorithm eschews schedule exploration in favor of smaller run-time overheads and thus can be used by beta testers or in crowd-sourced testing. In our experiments on a range of sites, we demonstrate that benign races are considerably more common than harmful ones.|None|accept|pre-accept
FSE|2015|Auto-Patching DOM-based XSS At Scale|inian parameshwaran,enrico budianto,shweta shinde,hung dang,atul sadhu,prateek saxena|Auto-patching,DOM-based XSS,Browser-agnostic tainting,DOM construction technique,Web security|DOM-based cross-site scripting (XSS) is a client-side code injection vulnerability that results from unsafe dynamic code generation in JavaScript applications, and has few known practical defenses. We study dynamic code evaluation practices on nearly a quarter million URLs crawled starting from the the Alexa Top 1000 websites. Of 777,082 cases of dynamic HTML/JS code generation we observe, roughly 14% use unsafe string interpolation for dynamic code generation — a well-known dangerous coding practice. To remedy this, we propose a technique to generate secure patches that replace unsafe string interpolation with safer code that utilizes programmatic DOM construction techniques. Our system transparently auto-patches the vulnerable site while incurring only 5.2-8.07% overhead. The patching mechanism requires no access to server-side code or modification to browsers, and thus is practical as a turnkey defense.|None|accept|accept
FSE|2015|Efficient and Reasonable Object-Oriented Concurrency|scott west,sebastian nanz,bertrand meyer|Concurrent programming,Parallel programming,Data race freedom,Optimization|Making threaded programs safe and easy to reason about is one of the chief difficulties in modern programming.  This work provides an efficient execution model for SCOOP, a concurrency approach that provides not only data-race freedom but also pre/postcondition reasoning guarantees between threads.  The extensions we propose influence both the underlying semantics to increase the amount of concurrent execution that is possible, exclude certain classes of deadlocks, and enable greater performance.  These extensions are used as the basis of an efficient runtime and optimization pass that improve performance 15x over a baseline implementation.  This new implementation of SCOOP is, on average, also 2x faster than other well-known safe concurrent languages.  The measurements are based on both coordination-intensive and data-manipulation-intensive benchmarks designed to offer a mixture of workloads.|None|accept|accept
FSE|2015|Efficient Dependency Detection for Safe Java Test Acceleration|jonathan bell,gail e. kaiser,eric melski,mohan dattatreya|testing,test dependencies,test acceleration|Slow builds remain a plague for software developers. The frequency with which code can be built and tested directly impacts the productivity of developers: longer build times mean a longer wait before determining if a change to the application being built was successful. We have discovered that in the case of some languages, such as Java, the majority of build time is spent running tests, where dependencies between individual tests are complicated to discover, making many existing test acceleration techniques unsound to deploy in practice. Without knowledge of which tests are dependent on others, we can not safely parallelize the execution of the tests, nor can we perform incremental testing (i.e, execute only a subset of an application's tests for each build). The previous techniques for detecting these dependencies did not scale to large test suites: given a test suite that normally ran in two hours, the best-case running scenario for the previous tool would have taken over 422 days to find dependencies between test methods (and would not soundly find all dependencies) — on the same project the exhaustive technique (to find all dependencies) would have taken over 10^300 years. We present a novel approach to detecting all dependencies between test cases in large projects that can enable safe exploitation of parallelism and test selection with a modest analysis cost.|None|accept|pre-accept
FSE|2015|GR(1) Synthesis for LTL Specification Patterns|shahar maoz,jan oliver ringert|reactive synthesis,linear temporal logic,specification patterns|Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification.  Two of the main challenges in bringing reactive synthesis to software engineering practice are its very high worst-case complexity -- for linear temporal logics (LTL) it is double exponential in the length of the formula, and the difficulty of writing declarative specifications using basic LTL operators. To address the first challenge, Piterman et al. have suggested the General Reactivity of Rank 1 (GR(1)) fragment of LTL, which has an efficient polynomial time symbolic synthesis algorithm. To address the second challenge, Dwyer et al. have identified 55 LTL specification patterns, which are common in industrial specifications and make writing specifications easier. In this work we show that almost all of the 55 LTL specification patterns identified by Dwyer et al. can be expressed in the GR(1) fragment of LTL. Specifically, we present an automated, sound and complete translation of the patterns to the GR(1) form, which effectively results in an efficient reactive synthesis procedure for any specification that is written using the patterns. We have validated the correctness of the catalog of GR(1) templates we have created.  The work is implemented in our reactive synthesis environment.  It provides positive, promising evidence, for the potential feasibility of using reactive synthesis in practice.|None|accept|accept
FSE|2015|Expressive and Fast Class Diagrams Analysis Using a New Translation to Alloy|shahar maoz,jan oliver ringert|model-driven software engineering,class diagrams,Alloy|Class diagrams (CDs) specify constraints on classes and their relations and much work has been published about their analysis. One approach to checking CD satisfiability and to computing CD instances is by translation to Alloy, a popular first-order relational language with automated SAT-based analysis. However, existing translations either support a limited set of CD language features and satisfiability checks (e.g. do not support multiple inheritance, only allow the analysis of a single CD, only check for weak satisfiability), or incur severe performance penalties. These two issues make existing translations impractical. We present a new translation from CDs to Alloy which addresses both issues. It supports an extended list of language features and analyses, while significantly outperforming all previously published translations in analysis times. The key ideas behind the new translation include flattening the inheritance hierarchy, representing bidirectional associations using single relations, and computing least upper bounds for signatures, field types, and multiplicities. An extensive evaluation provides evidence for the significant advantages and practical relevance of the new translation, which is implemented and available in a new version of the CD2Alloy tool.|None|reject|pre-reject
FSE|2015|Can We Use Combinatorial Testing for Any Software?|huayao wu,changhai nie,hareton leung|Software Testing,Combinatorial Testing,Application,Systematic Survey|Over the past decade, there has been a great development in the theories and applications of combinatorial testing. Studies have demonstrated the effectiveness of combinatorial testing, but the general applicability of this method is still in doubt. To investigate whether combinatorial testing can be used in any software, in this paper we describe a survey of 319 research papers and another 200 real world software systems. This study aims to provide practical evidence to support combinatorial testing, including classifying favorable combinatorial testing scenarios and offering practical guidelines. The results obtained will not only give new insights about combinatorial testing, but also help improve and apply this popular testing method.|None|reject|reject
FSE|2015|Why Do Programmers Have Difficulty Understanding Program Analysis Tool Notification? A Cross Tool Study|brittany johnson,emerson r. murphy-hill,sarah heckman,caitlin sadowski|program analysis tools,notifications,software development,usability|Programmers can create high quality software by using program analysis tools to automate software development tasks, which often use notifications to communicate with programmers. Previous research suggests that programmers may have difficulty understanding these notifications. This paper presents an in-depth, qualitative analysis of the difficulties programmers encounter when attempting to understand and address program analysis tool notifications. Our results suggest that programmers face a variety of difficulties; for example, participants had difficulty interpreting notifications that use concepts misaligned with their programming experience. We also found that programmers may find notifications more effective if they present information pertaining to best practices. Our results have implications for the design of program analysis tools; for example, notifications that adapt to programmer experience could increase programmers' effectiveness in resolving them.|None|reject|reject
FSE|2015|Synthesis of Protocol Mediators as Composition of Transducers|marco autili,paola inverardi,francesca marzi,filippo mignosi,massimo tivoli|Mediator Synthesis,Transducers,Protocol Interoperability,Software Engineering,Formal Methods|Ubiquitous and pervasive computing promotes the creation of a computing environment where heterogeneous Networked Systems (NSs) seamlessly interact. In this context, achieving interoperability among heterogeneous NSs represents an important issue. In order to mediate the NSs interaction protocol and solve possible mismatches, mediators are often built. In the literature, many approaches propose the automated synthesis of mediators. Unfortunately, state-of-the-art approaches do not provide a rigorous characterization of the concept of interoperability, hence making hard to assess their general applicability and soundness. Furthermore, they rely on assumptions that, in the practice of mediator development, turn out to be often unfounded. In this paper, relying on the use of transducers and Mazurkiewicz trace theory, we formalize a method for the automated synthesis of mediators that allows us to: (i) relax the assumptions state-of-the-art approaches rely on; (ii) characterize the conditions that ensure the mediator existence and correctness in our setting; and (iii) establish the applicability boundaries of the synthesis method.|None|reject|reject
FSE|2015|Evolving Requirements-to-Code Trace Links across Versions of a Software System|mona rahimi,william goss,jane cleland-huang|Traceability,Link Evolution,Trace Links|Trace links provide critical support for numerous software engineering activities including safety analysis, compliance verification, test-case selection, and impact prediction. However, as the system evolves over time, there is a tendency for the quality of trace links to degrade into a tangle of inaccurate and untrusted links. This is especially true with the links between source-code and upstream artifacts such as requirements – because developers frequently refactor and change code without updating the links. We present TLE (Trace Link Evolver), a solution for automating the evolution of trace links as changes are introduced to source code. We use a set of heuristics, open source tools and information retrieval methods to detect common change scenarios in different versions of code. Each change scenario is then associated with a set of link evolution heuristics which are used to evolve trace links. We evaluate our approach through a controlled experiment and also through applying it to a selection of classes taken from the Cassandra Database System. Results show that the trace links produced by our approach are significantly more accurate than those produced using information retrieval alone.|None|reject|reject
FSE|2015|Is the Cure Worse than the Disease? A Large-Scale Analysis of Overfitting in Automated Program Repair|edward k. smith,earl t. barr,claire le goues,yuriy brun|automatic program repair,GenProg,TSPRepair,reproducibility,overfitting,empirical study,data-driven evaluation,large-scale evaluation|Recent research in automated program repair has shown promise for reducing the significant manual effort debugging requires. This paper addresses a deficit of earlier evaluations of automated repair techniques caused by repairing programs and evaluating generated patches' correctness using the same set of tests. Since tests are an imperfect metric of program correctness, evaluations of this type do not discriminate between correct patches, and patches that overfit the available tests and break untested but desired functionality. This paper evaluates two well-studied repair tools, GenProg and TSPRepair, on a 956-bug dataset, each with a human-written patch. By evaluating patches on tests independent from those used during repair, we find that the tools are unlikely to improve the proportion of independent tests passed, and that the quality of the patches is proportional to the coverage of the test suite used during repair. For programs with fewer bugs, the tools are as likely to break tests as to fix them. However, novice developers also overfit, and automated repair can, under the right conditions, outperform these developers. In addition to overfitting, we measure the effects of test suite coverage, test suite provenance, starting program quality, and the difference in quality between novice-developer-written and tool-generated patches when quality is assessed with an independent test suite from patch generation. We have released the 956-bug dataset to allow future evaluations of new repair tools.|None|accept|accept
FSE|2015|Reducing Combinatorics in GUI Testing of Android Applications|nariman mirzaei,joshua garcia,hamid bagheri,alireza sadeghi,sam malek|Android,Software Testing,Input Generation|The rising popularity of Android and the GUI-driven nature of its apps have motivated the need for applicable automated GUI testing techniques. Although exhaustive testing of all possible GUI combinations is the gold standard for maximizing the code coverage, it is often infeasible, due to the combinatorial explosion of test cases. This paper presents TrimDroid, a framework for GUI testing of Android apps that uses a novel strategy to generate the tests in a combinatorial, yet scalable, fashion. It is backed with automated program analysis and formally rigorous test generation engines. TrimDroid relies on program analysis to extract formal specifications. These models express the app’s behavior (i.e., control flow between the various app screens) as well as the GUI elements and their dependencies. The dependencies among the GUI elements comprising the app are used to reduce the number of combinations with the help of a solver. Our experiments have corroborated TrimDroid’s ability to achieve a comparable coverage as that possible under exhaustive GUI testing using significantly fewer test cases.|None|reject|reject
FSE|2015|The Making of Cloud Applications – An Empirical Study on Software Development for the Cloud|jurgen cito,philipp leitner,thomas fritz,harald c. gall|cloud software development,devops,empirical study|Cloud computing is gaining more and more traction as a deployment and provisioning model for software. While a large body of research already covers how to optimally operate a cloud system, we still lack insights into how professional software engineers actually use clouds, and how the cloud impacts development practices. This paper reports on the first systematic study on how software developers build applications in the cloud. We conducted a mixed-method study, consisting of qualitative interviews of 25 professional developers and a quantitative survey with 294 responses. Our results show that adopting the cloud has a profound impact throughout the software development process, as well as on how developers utilize tools and data in their daily work. Among other things, we found that (1) developers need better means to anticipate runtime problems and rigorously define metrics for improved fault localization and (2) the cloud offers an abundance of operational data, however, developers still often rely on their experience and intuition rather than utilizing metrics. From our findings, we extracted a set of guidelines for cloud development and identified challenges for researchers and tool vendors.|None|accept|accept
FSE|2015|Finding and Evaluating the Performance Impact of Data Mismatches Between the Needed Data in the Application Code and the Requested DBMS Data|tse-hsun chen,weiyi shang,zhen ming jiang,ahmed e. hassan,mohamed nasser,parminder flora|Performance,Database,ORM,Object-relational mapping,Static analysis,Dynamic analysis|Accessing database management systems (DBMSs) can be very complex for large-scale systems. Hence, developers usually leverage Object-Relational Mapping (ORM) to abstract database accesses. However, since ORM frameworks do not know how developers will use the data that is returned from the DBMS, ORM frameworks cannot provide an optimal data retrieval approach for all applications. Thus, using ORM may result in data mismatches between the needed and the requested data. Such data mismatches can have significant performance impacts. Even though ORM frameworks provide various approaches to resolve data mismatches, due to the complexity of software systems, developers may not be able to locate such problems in the code, and thus may not proactively resolve the problems. In this paper, we propose an automated approach to locate the data mismatches in the code, and we have implemented our approach as a Java framework. We conduct a case study on two open source systems and one enterprise system. We find that data mismatch problems exist in 87% of the exercised transactions. Due to the large number of detected data mismatches, we further propose an automated approach to assess the impact and prioritize the resolution efforts on the data mismatches. Our performance assessment result shows that by resolving the data mismatches, system response time can be improved by an average of 17%.|None|reject|pre-reject
FSE|2015|Towards the integration of decision in SOA  Using the new standard Decision Model and Notation (DMN)|boumahdi fatima|DMN (Decisional Model Notation),SOA (Service Oriented Architecture),Decision-making process,BPMN (Business Process Modeling Notation),Pharmacy Inventory Management|Various models and methods are used to support the process of SOA (service oriented architecture) analysis and design, but still after many years of practice, there are a lot of questions and unsolved problems that cause SOA development projects to fail. One of the reasons is that rapid changes in the business environment make it necessary to introduce the decision design, which should be eectively supported by SOA. Moreover, this challenge wasn't addressed in the literature. In this paper, we present the ideas and work behind the consolidation of decision aspect and SOA. In this paper, we proposes a new design of SOA integrates a solution for decision problems and focalize on three views: business, information system and decision view, this last represent our contribution. Indeed, our aim is to use standard models in each of the steps dened, this is one of our main reasons to use BPMN, UML and DMN (Decisional Model Notation) for design the SOA, as we will show later. Thus, we will show as illustration an application of pharmacy inventory management to il-lustrate how to use our approach.|None|reject|reject
FSE|2015|Summarizing and Measuring Development Activity|christoph treude,fernando figueira filho,uira kulesza|empirical study,grounded theory,development activity,metrics,summarization|Software developers pursue a wide range of activities as part of their work, and making sense of what they did in a given time frame is far from trivial as evidenced by the large number of awareness and coordination tools that have been developed in recent years. To inform tool design for making sense of the information available about a developer's activity, we conducted an empirical study with 156 GitHub users to investigate what information they would expect in a summary of development activity, how they would measure development activity, and what factors influence how such activity can be condensed into textual summaries or numbers. We found that unexpected events are as important as expected events in summaries of what a developer did, and that many developers do not believe in measuring development activity. Among the factors that influence summarization and measurement of development activity, we identified development experience, team size, and programming languages. These findings lay the empirical foundation for the construction of tools that condense development activity into textual summaries or numbers.|None|accept|accept
FSE|2015|A Programmable Interface for Customizing and Extending  Android ROM|nouha ghribi|Android,Customize,framework,Automated,ROM|Nowadays, Android devices are increasingly in high demand. These devices could be multi-functional in order to make the life easier and more productive. Therefore, researches on improving the Android Operating System (OS) capability to better support and provide extension features become interesting. This paper presents a framework which is specically designed to customize and extend with new features the firmware flashes in Android based devices. Preliminary experimental results on real world Android devices show the feasibility of the approach and encourage further research activities.|None|reject|reject
FSE|2015|Timeline Based Issue Management Data Visualization for Observing Software Projects|anna-liisa mattila,anna etelaaho,outi sievi-korte,kati kuusinen,marko leppanen,kari systa,mikael niemela,akseli kelloniemi|Information Visualization,Software Analytics,Software Process Improvement|Access to valid data and understanding of the data is crucial for management of any process. Alike, various business information systems are very important for corporate management. Sometimes businesses declare some metrics to be Key Performance Indicators and use some framework like Balanced Scorecards to set, communicate and monitor important measurements. However, business analysis can not be limited to tracking a few key numbers. Therefore, visualization methods are additionally used in getting a better overall picture of the organization and its business. In this paper we present results of our study, where issue management data from two industrial software projects were visualized to identify possible problems in the software process. We also address the development of the visualization method. The conclusion of the study is that our visualization method of project management data can help to identify problems in following the development process and thus help to improve the process.|None|reject|pre-reject
FSE|2015|Generating TCP/UDP Network Data for Automated Unit Test Generation|andrea arcuri,gordon fraser,juan p. galeotti|Unit testing,automated test generation,JUnit|Although automated unit test generation techniques can in principle generate test suites that achieve high code coverage, in practice this is often inhibited by the dependence of the code under test on external resources. In particular, a common problem in modern programming languages is posed by code that involves networking (e.g., opening a TCP listening port). In order to generate tests for such code, we describe an approach where we mock (simulate) the networking interfaces of the Java standard library, such that a search-based test generator can treat the network as part of the test input space. This not only has the benefit that it overcomes many limitations of testing networking code (e.g., different tests binding to the same local ports, and deterministic resolution of hostnames and ephemeral ports), it also substantially increases code coverage. An evaluation on 23,886 classes from 110 open source projects, totalling more than 6.6 million lines of Java code, reveals that network access happens in 2,642 classes (11%).  Our implementation of the proposed technique as part of the EvoSuite testing tool addresses the networking code contained in 1,672 (63%) of these classes, and leads to an increase of the average line coverage from 29.1% to 50.8%.  On a manual selection of 42 Java classes heavily depending on networking, line coverage with EvoSuite more than doubled with the use of network mocking, increasing from 31.8% to 76.6%.|None|accept|accept
FSE|2015|Getting to know you... Towards a Capability Model for Java|ben hermann,michael reif,michael eichberg,mira mezini|library,reuse,static analysis,security,capability,permission,authority|Developing software from reusable libraries lets developers face a security dilemma: Either be efficient and reuse libraries as they are or inspect them, know about their resource usage, but possibly miss deadlines as reviews are a time consuming process. In this paper, we propose a novel capability inference mechanism for libraries written in Java. It uses a coarse-grained capability model for system resources that can be presented to developers. We found that the capability inference agrees on most expectations towards capabilities that can be derived from project documentation. Moreover, our approach can find capabilities that cannot be discovered using project documentation. It is thus a helpful tool for developers mitigating the aforementioned dilemma.|None|accept|accept
FSE|2015|MultiSE: Multi-Path Symbolic Execution using Value Summaries|koushik sen,george necula,liang gong,wontae choi|symbolic execution,value summary,dynamic analysis,JavaScript|Dynamic symbolic execution (DSE) has been proposed to effectively generate test inputs for real-world programs.  Unfortunately, DSE techniques do not scale well for large realistic programs, because often the number of feasible execution paths of a program increases exponentially with the increase in the length of an execution path. In this paper, we propose MultiSE, a new technique for merging states incrementally during symbolic execution, without using auxiliary variables. The key idea of MultiSE is based on an alternative representation of the state, where we map each variable, including the program counter, to a set of guarded symbolic expressions called a value summary.  MultiSE has several advantages over conventional DSE and conventional state merging techniques: value summaries enable sharing of symbolic expressions and path constraints along multiple paths and thus avoid redundant execution.  MultiSE does not introduce auxiliary symbolic variables, which enables it to 1) make progress even when merging values not supported by the constraint solver, 2) avoid expensive constraint solver calls when resolving function calls and jumps, and 3) carry out most operations concretely.  Moreover, MultiSE updates value summaries incrementally at every assignment instruction, which makes it unnecessary to identify the join points and to keep track of variables to merge at join points. We have implemented MultiSE for JavaScript programs in a publicly available open-source tool.  Our evaluation of MultiSE on several programs shows that 1) value summaries are an effective technique to  take advantage of the sharing of value along multiple execution path, that 2) MultiSE can run significantly faster than traditional dynamic symbolic execution and,  3) MultiSE saves a substantial number of state merges compared to conventional state-merging techniques.|None|accept|pre-accept
FSE|2015|JITProf: Pinpointing JIT-unfriendly JavaScript Code|liang gong,michael pradel,koushik sen|Just-in-time compilation,JIT-unfriendly code,JIT-compiler,JIT,JavaScript,dynamic analysis,JITProf,hidden class,inline caching,polymorphic operations|Most modern JavaScript engines use just-in-time (JIT) compilation to translate parts of JavaScript code into efficient machine code at runtime. Despite the overall success of JIT compilers, programmers may still write code that uses the dynamic features of JavaScript in a way that prohibits profitable optimizations. Unfortunately, there currently is no way to measure how prevalent such JIT-unfriendly code is and to help developers detect such code locations. This paper presents JITProf, a profiling framework to dynamically identify code locations that prohibit profitable JIT optimizations. The key idea is to associate meta-information with JavaScript objects and code locations, to update this information whenever particular runtime events occur, and to use the meta-information to identify JIT-unfriendly operations. We use JITProf to analyze widely used JavaScript web applications and show that JIT-unfriendly code is prevalent in practice. Furthermore, we show how to use the approach as a profiling technique that finds optimization opportunities in a program. Apply the profiler to popular benchmark programs shows that refactoring these programs to avoid performance problems identified by JITProf leads to statistically significant performance improvements of up to 26.3% in 15 benchmarks.|None|accept|pre-accept
FSE|2015|Iterative Distribution-Aware Sampling for Probabilistic Symbolic Execution|mateus borges,antonio filieri,marcelo d'amorim,corina s. pasareanu|probabilistic analysis,symbolic execution,distribution-aware sampling,Monte Carlo sampling|Probabilistic symbolic execution aims at quantifying the probability of reaching program events of interest assuming that program inputs follow given probabilistic distributions. The technique collects constraints on the inputs that lead to the target events and analyzes them to quantify how likely it is for an input to satisfy the constraints. Current techniques either handle only linear constraints or only support continuous distributions using a "discretization" of the input domain, leading to imprecise and costly results. We propose an iterative distribution-aware sampling approach to support probabilistic symbolic execution for arbitrarily complex mathematical constraints and continuous input distributions. We follow a compositional approach, where the symbolic constraints are decomposed into sub-problems whose solution can be solved independently. At each iteration the convergence rate of the computation is increased by automatically refocusing the analysis on estimating the sub-problems that mostly affect the accuracy of the results, as guided by three different ranking strategies. Experiments on publicly available benchmarks show that the proposed technique improves on previous approaches in terms of scalability and accuracy of the results.|None|accept|accept
FSE|2015|A Large-scale Survey about the Essential Attributes of Software Engineering Expertise|paul li,andrew begel,andrew ko|Software engineers,Expertise,Productivity,Teamwork|Software engineers are fundamental to software engineering. Their importance belies a deficient understanding about the essential attributes of software engineering expertise. We present the results of a large-scale world-wide survey of 1,926 expert Microsoft engineers across 67 countries to learn about the importance of 54 previously identified attributes of great software engineers. To help interpret our quantitative findings, we followed up with brief email interviews with 77 of the survey respondents. We found that the two key drivers of the importance ratings were having the mental capacity to handle complexity and embracing life-long learning. We also found unexpected relationships between the respondents’ ratings and their level of experience, education history, and cultural background, for which we provide possible explanations. Relative to rankings in prior research, we found the essential attributes of software engineering expertise to encompass internal attributes of the engineers’ personality and decision-making abilities, in addition to external attributes of the software they produce and their interactions with teammates. We discuss the implications of our results for researchers, educators, and practitioners.|None|reject|reject
FSE|2015|Automated Multi-Objective Control for Self-Adaptive Software Design|antonio filieri,henry hoffmann,martina maggio|adaptive software,control theory,multi-objective control,dynamic systems,non-functional requirements|While software is becoming more complex everyday, the requirements on its behavior are not getting any easier to satisfy. An application should offer a certain quality of service, adapt to the current environmental conditions and withstand runtime variations that were simply unpredictable during the design phase.  To tackle this complexity, control theory has been proposed as a technique for managing software's dynamic behavior, obviating the need for human intervention. Control-theoretical solutions, however, are either tailored for the specific application or do not handle the complexity of multiple interacting components and multiple goals. In this paper, we develop an automated control synthesis methodology that takes, as input, the configurable software components (or knobs) and the goals to be achieved. Our approach automatically constructs a control system that manages the specified knobs and guarantees the goals are met.  These claims are backed up by experimental studies on three different software applications, where we show how the proposed automated approach handles the complexity of multiple knobs and objectives.|None|accept|accept
FSE|2015|On the "Naturalness" of Buggy Code|baishakhi ray,vincent hellendoorn,zhaopeng tu,connie nguyen,saheel godhane,alberto bacchelli,premkumar t. devanbu|bug detection,unnatural code,language models,static bug finders|Real software, the kind programmers produce by the kLOC to solve real-world problems, tends to  be ``natural,'' like speech or natural language; it tends to be highly repetitive, predictable. Researchers captured this naturalness of software through statistical models and used them to good effect in suggestion engines, porting tools, coding standards checkers, and idiom miners. This suggests that code that appears improbable, or surprising to a good statistical language model is ``unnatural" in some sense, and thus possibly suspicious. In this paper, we investigate this hypothesis. We consider a large corpus of bug fix commits (ca. 38,000),  from 10 different  projects, and we focus on its language statistics, evaluating the naturalness of buggy code and whether the fixes increase the naturalness. We find that code with bugs tends to be more entropic (i.e. unnatural), becoming less so as bugs are repaired. Moreover, focusing on highly entropic lines is similar in cost-effectiveness to some well-known static bug finders (PMD, FindBugs) and ordering warnings from these bug finders using an entropy measure improves the cost-effectiveness of inspecting code implicated in warnings. This suggests that entropy may be a valid language-independent and simple way to complement the effectiveness of PMD or FindBugs, and that search-based bug-fixing methods may benefit from using entropy both for fault-localization and searching for fixes.|None|reject|reject
FSE|2015|Application Management in Dynamic Environments using Continuous Deployment|ozan gunalp,clement escoffier,philippe lalanda|Software Deployment,Dynamism,Pervasive Computing|Modern applications raise new challenges for the developers and other stakeholders who participate in the development process. It is now impossible to manage applications running in dynamic, heterogeneous environments using human-centric processes. Instead, there is an increasing need for automation tools that continuously deploy applications into execution, push updates or adapt existing software regarding contextual and business changes. Existing solutions fall short on providing fault-tolerant, reproducible deployments that can be used in dynamic environments. In this paper we present a deployment process that enables application management using continuous deployment principles at runtime. We implement this process in a deployment manager and validate our approach in a pervasive environment on the platform provisioning as well as on the application installations and continuous reconfigurations.|None|reject|reject
FSE|2015|Architectural Tactics for the Design of Scalable and Available Cloud Applications|david gesvindr,barbora zimmerova|cloud computing,software architecture design,architectural tactics,software quality,scalability,elasticity,availability,performance|Cloud computing is becoming a popular approach to software application operation, utilizing on-demand network access to a pool of shared computing resources, and associated with many benefits including low-effort provisioning, rapid elasticity, maintenance cost reduction and pay-as-you-go billing model. However, application deployment in cloud is not itself a guarantee of high performance, availability, and other quality attributes, which may come as a surprise to many software engineers who detract from the importance of proper architecture design of a cloud application. In this paper we analyze the issues and challenges associated with architecture design of a cloud application that has to be in compliance with given quality criteria, including scalability, availability and throughput, and structure our observations into architectural tactics that can guide architecture design of efficient and scalable cloud applications. To support our findings, we illustrate the impact of identified tactics on multiple case studies and examples.|None|reject|reject
FSE|2015|A study on the contribution of groups of developers on the complexity of frameworks|leandro alves,ricardo choren noya|structural complexity,cohesion,coupling,framework,data mining,k-means,exploratory study,software development and maintenance,development teams|The use of frameworks in software development has as its main goal the reuse classes and components for treatment of a problem and have characteristics that are directly related to the internal complexity of these frameworks. This work presents a method that, from the developers classification in groups considering common characteristics related to changes made in the source code, enables the analysis of positive and negative contribution of each group of developers on the variation of the complexity of the software. We analyzed data on free software projects frameworks to determine what characteristics related to changes in the source code made by developers has influence on the variation of complexity. Once certain these characteristics were performed experiments using data mining algorithms for grouping and developers determine the type and degree of contribution of each developer group. Among the results obtained through this work, it is possible to affirm that there are groups of developers with specific features that contribute negatively to the variation of complexity. Furthermore the study demonstrates that there are groups that bring benefits in relation to the reduction of complexity|None|reject|reject
FSE|2015|Lexical Similarity between Arguments and Parameters: Analysis and Application|hui liu,qiurong liu,michael pradel,yue luo|Empirical study,Name-based program analysis,Identifier names,Static analysis,Method arguments|Programmer-provided identifier names convey information about the semantics of a program. This information can complement traditional program analyses in various software engineering tasks, such as bug finding, code completion, and generating documentation. Even though identifier names appear to be a rich source of information, little is known about their properties and their potential usefulness in real-world software. This paper studies and exploits the lexical similarity between arguments and parameters of methods, which is one prominent situation where identifier names can provide otherwise missing information. We empirically study the similarity of arguments and parameters in a corpus of 60 Java programs. Our results show that the distribution of the lexical similarity is a U-shape: the similarity is either extremely high or extremely low. Our results also show that one can automatically identify parameters that are dissimilar to their corresponding arguments. As an application of our findings, we present an automated bug detection approach that warns developers about potentially incorrect arguments and that suggests the correct argument to use instead. The approach detects incorrect arguments with a precision of 93% and a recall of 63%.|None|reject|reject
FSE|2015|An Empirical Study of goto in C Code from GitHub Repositories|meiyappan nagappan,romain robbes,yasutaka kamei,eric tanter,shane mcintosh,audris mockus,ahmed e. hassan|Empirical Study,GitHub,Large Scale,Quantitative and Qualitative|It is nearly 50 years since Dijkstra argued that goto obscures the flow of control in program execution and urged programmers to abandon the goto statement. While past research has shown that goto is still in use, little is known about whether goto is used in the unrestricted manner that Dijkstra feared, and if it is ‘harmful’ enough to be a part of a post-release bug. We, therefore, conduct a two part empirical study - (1) qualitatively analyze a statistically representative sample of 384 files from a population of almost 2 million C programming language files collected from over 11K GitHub repositories and find that developers use goto in C files for error handling (80.21 ± 5%) and cleaning up resources at the end of a procedure (40.36 ± 5%); and (2) quantitatively analyze the commit history from the release branches of six OSS projects and find that no goto statement was removed/modified in the post-release phase of four of the six projects. We conclude that developers limit them- selves to using goto appropriately in most cases, and not in an unrestricted manner like Dijkstra feared, thus suggesting that goto does not appear to be harmful in practice.|None|accept|accept
FSE|2015|RefScheduler：A Task Scheduling Approach to Reducing Waste Incurred by Postponing Needed Refactoring|jie chen,qing wang,leon osterweil,junchao xiao,mingshu li|Refactoring,Project Management,Planning,Multi-Objective Optimization|Code refactoring is often used to enhance project agility and improve such nonfunctional code attributes as readability, maintainability, and extensibility. But, many projects are reticent to refactor, fearing that it comes at the cost of creating new features and fixing bugs. Indeed, in rapidly evolving markets, immediate economic benefit is derived from the latter two kinds of tasks, while the benefits from refactoring are largely invisible, intangible and long term. Unfortunately, deferring refactoring tends to increase the difficulty of cleaning up code.  It can result in lingering bad code smells and increased costs for both ongoing development and the eventual refactoring itself. Our case study shows that cost increases due to postponing needed refactoring is readily identifiable in at least one open source project. This suggests that decisions about when to refactor are important.  They should be based upon careful consideration of increased cost incurred by postponing needed refactoring versus business value lost because refactoring prevented creating new features and fixing bugs. This paper shows how the problem of deciding when to refactor can be addressed as a task scheduling problem integrated into the overall development process. We introduce an approach and a system, RefScheduler, for planning and scheduling refactoring. RefScheduler aims to find feasible plans representing a selection of tasks that are optimal with respect to minimizing the waste cost due to deferred refactoring and ensuring no loss of business value. We demonstrate the efficacy of our approach by simulating its application to the management of a representative open source project.|None|reject|reject
FSE|2015|Survival Analysis of the Occurrence of First Fork Programming Language Source Files of Small and Medium Migrated Projects|bee chua|Open Source,Survival Analysis,Forking,Programming Languages|Background: The aim of this study was to estimate survival time (ST) of program language source files on small and medium enterprise migrated projects, including median time of survival, and to assess the association and impact of covariates (fork risk factors) on event status and ST. Methods: Statistical analysis of 301 programming language source files from seven small and seven medium enterprise projects that survived on the Github hosting server for a five year period (1999–2014). The Kaplan Meier (KM) survival analysis technique was used to investigate the relationship between project ST and several variables, and Cox regression modelling was used to determine fork risk factors on small and medium project ST and hazard time (HT). Results: The mean age for small projects to receive the first fork was 14.7 months (median 10 months) and 37.8 months for medium projects to receive the first fork (median 36 months). Multivariate Cox regression showed a missing programming language build script was 3.5 times more hazard for small rather than medium projects. In addition, no release file was 2.8 and no license file was 1.8 hazard times for small compared with medium projects. Conclusion: There was a significant difference in survival plots of small and medium projects and fork period. Small projects had a shorter ST compared with medium projects but medium projects had a higher HT compared with small projects. The visibility of the ReadMe, license, and release files were associated with longer ST while variables associated with shorter ST included programming language trend, small build script and copyright, and single directory.|None|reject|reject
FSE|2015|Improving Static Analysis Precision by Machine Learning--guided Precision-Effective Unsoundness|kihong heo,hakjoo oh|Static analysis,Unsound static analysis,Parameterized static analysis|We present a new technique to improve static analysis precision by compromising the soundness as harmless as possible.  A sound static analysis is able to detect all target bugs but usually suffers from a lot of false alarms. By compromising the soundness, one can reduce the false alarms but careless unsoundness decisions may miss a large amount of real bugs as well.  Our goal is to employ unsoundness only when it is likely to reduce false alarms while still being able to detect real bugs. Given a static analysis and a set of training programs, our technique automatically learns such precision-effective unsoundness and applies it to new, unseen programs. We implemented this technique in a realistic C static analyzer and experimented on existing 23 benchmark programs previously used for static analysis evaluation. The results show that our method reduces the number of false alarms by on average 38 while missing real bugs only by 9, compared to the sound version of the analysis.|None|reject|reject
FSE|2015|Guided Differential Testing of Certificate Validation in SSL/TLS Implementations|yuting chen,zhendong su|guided differential testing,mutation,MCMC,certificate validation|Certificate validation in SSL/TLS implementations is critical for Internet security. There is recent strong effort, namely frankencert, in automatically synthesizing certificates for stress-testing certificate validation. Despite its early promise, it remains a significant challenge to generate effective test certificates as they are structurally complex with intricate syntactic and semantic constraints. This paper tackles this challenge by introducing mucert, a novel, guided technique to much more effectively test real-world certificate validation code. Our core insight is to (1) leverage easily accessible Internet certificates as seed certificates, and (2) diversify them by adapting Markov Chain Monte Carlo (MCMC) sampling. The diversified certificates are then used to reveal discrepancies, thus potential flaws, among different certificate validation implementations. We have implemented mucert and extensively evaluated it against frankencert. Our experimental results show that mucert is significantly more cost-effective than frankencert. Indeed, 1K mucerts (i.e., mucert-mutated certificates) yield three times as many distinct discrepancies as 8M frankencerts (i.e., frankencert-synthesized certificates), and 200 mucerts can achieve higher code coverage than 100,000 frankencerts. This improvement is significant as it incurs much cost to test each generated certificate. We have analyzed and reported 20+ latent discrepancies (presumably missed by frankencert), and reported an additional 357 discrepancy-triggering certificates to SSL/TLS developers, who have already confirmed some of our reported issues and are investigating causes of all the reported discrepancies. We believe that mucert is practical and effective for helping improve the robustness of SSL/TLS implementations.|None|accept|pre-accept
FSE|2015|Are Software Bugs Contagious?|xiaodong gu,sunghun kim,yo-sub han|bug contagion,defect prediction,software graph,empirical software engineering|Contagion in diseases such as an influenza epidemic is a well-known phenomenon. Since disease contagion often results in serious consequences, the prevention of epidemic outbreaks is a very important and well-studied area. Surprisingly, several social phenomena such as obesity, suicide, and shopping habits are also considered to be contagious among some social ties (graphs). These ndings are used to encourage better lifestyles, publish appropriate suicide news, and promote products. In this paper, we study contagion of software bugs. Well-known defect factors in the software engineering literature indirectly imply the existence of bug contagion. We first evaluate bug contagion on graphs of six known defect factors such as function calls, LOC, and Cyclomatic. Our evaluation on four open-source projects, HTTPClient, Jackrabbit, Lucene, and Rhino reveals that bugs are indeed contagious as seen on some artifact tie graphs. Based on evaluation and observation, we propose a hybrid graph that exhibits a higher degree of bug contagion. As an application of our ndings, we apply our hybrid graph to defect prediction. The bug contagion on the proposed hybrid graph improves defect prediction performance by 12%. The properties of bug contagion enable researchers to develop better defect prediction models and help developers to perform maintenance actions to prevent bug epidemics.|None|reject|reject
FSE|2015|SAT-based Model Checking of Software Product Lines|fei he,yuan gao,liangze yin,ming gu|Software product line,Model checking,Satisfiability|Software-produce-line (SPL) engineering is increasingly adopted in safety-critical systems. It is highly desirable to rigorously show that these systems are correctly designed. However, formal analysis for SPLs is more difficult than for single systems because an SPL may contain a large number of individual systems. In this paper, we propose a SAT-based model checking technique for SPLs. This technique avoids explicit enumeration of states by employing SAT procedures. A new formalism for SPLs is proposed, which is fully symbolic and thus suitable for SAT-based model checking. The model checking algorithm is carefully designed to exploit the distinguishing characteristics of SPLs. Two optimization techniques (i.e. feature cube enlargement and incremental SAT solving) are proposed to speed up the verification process. With our algorithm, the set of products satisfying (or dissatisfying) the given property can be identified. Experimental results show promising performance of our approach.|None|reject|reject
FSE|2015|Taking the Guesswork out of Project Selection: Qualitative Repository Analysis with RepoGrams|daniel rozenberg,ivan beschastnikh,fabian kosmale,valerie poser,heiko becker,marc palyart,gail c. murphy|software engineering research tool support,software repository analysis,software visualization|The availability of open source software projects has created an enormous opportunity for software engineering research. However, this availability requires that researchers judiciously select an appropriate set of evaluation targets and properly document this rationale. Often, this selection is critical, as it can be used to argue for the generality of the evaluated tool or process. To understand the selection criteria that researchers use in their work we systematically read 29 research papers appearing in six major software engineering conferences. Using grounded theory we iteratively developed a codebook and coded these papers along five different dimensions, all of which relate to how the authors select evaluation targets in their work. Our results indicate that most authors relied on qualitative features to select their evaluation targets. Building on these results we developed a tool called RepoGrams, which supports researchers in comparing and contrasting source code repositories of multiple software projects and helps them in selecting appropriate evaluation targets for their studies. We describe RepoGrams’ design and evaluate it with two user studies and one case study. We find that RepoGrams helps SE researchers understand and compare characteristics of a project’s source repository and that RepoGrams can be used by non-SE experts to investigate project histories. The tool is open source and is available online: http://repograms.net/|None|reject|reject
FSE|2015|On Moving Duplicate Java Application Code to Libraries|yoshiki higo,tomoya ishihara,yoshihiro nagase,jiachen yang,keisuke hotta,hiroshi igaki,shinji kusumoto|Code clone detection,Moving application code to libraries,Empirical study on Large-scale source code|Libraries offer commonly used functions that benefit software developers.  Existing research efforts have revealed that many code clones exist across different software projects.  Therefore, finding across-project clones would be useful for making libraries.  However, existing clone detection techniques do not necessarily fit this purpose because of their detection scalability and granularity.  In this paper, we propose a method-level clone detection technique with code degeneration for identifying opportunities to move duplicate Java application code to libraries.  The method-level granularity is appropriate for building libraries because each method generally composes a functionally coherent unit that can be easily moved to libraries.  Besides, the method-level granularity realizes a highly scalable detection on a huge amount of source code.  Furthermore, our code degeneration makes it possible that the functionally same methods are detected as clones even if they include different code for error handling.  The proposed technique was able to finish clone detection within two hours on a huge dataset (360 million lines of code in 13,000 Java projects).  We also evaluated the detected clones with thirteen research participants. The participants judged whether each of given 50 clones was an appropriate candidate for libraries or not.  More than half of the participants accepted 43 out of them.|None|reject|reject
FSE|2015|Hardware/Software Co-monitoring|li lei,kai cong,zhenkun yang,fei xie|Runtime Verification,Formal Device Model,Hardware/Software Interfaces|Hardware/Software (HW/SW) interfaces are pervasive and typically implemented by devices and device drivers. Unfortunately, HW/SW interfaces are unreliable and most system failures are caused by errors in device/driver interactions. Moreover, device/driver interfaces are vulnerable to malicious attacks, which can be launched from either the device or driver by exploiting interface vulnerabilities. In this paper, we present HW/SW co-monitoring, an approach to simultaneously monitoring a hardware device and its driver at runtime. The foundation of our approach is a formal device model (FDM), a transaction-level, executable model derived from the device specication and capturing the device behaviors. Our co-monitoring framework carries out two-tier runtime checking: (1) device checking which checks if the device behaviors conform to the FDM behaviors; (2) property checking veries the system-level properties against the device/driver composition indirectly through co-verication of the FDM/driver. HW/SW comonitoring helps detect bugs as well as malicious exploits in the device, the driver, and their interface. We have applied our approach to four real devices and their Linux drivers. We simulated three types of malicious attacks on device/driver interfaces. Our HW/SW co-monitoring framework was able to detect them successfully. It also discovered several bugs and vulnerabilities in the four devices and their drivers while introducing modest runtime overhead. The results demonstrate that HW/SW co-monitoring has a major potential in improving system reliability and security.|None|reject|reject
FSE|2015|Qualitative Analysis for Multiple Adaptation Loops|kenji tei,ryuichi takahashi,nicolas d'ippolito,hiroyuki nakagawa,shinichi honiden|Self-adaptive system,adaptation loops,qualitative analysis,reachability|Using multiple adaptation loops enlarges the adaptation space of self-adaptive systems, but poses an adaptation chain problem. Adaptation loops are triggered not only by changes in the environment but also by changes caused by other adaptation loops. The chain of adaptations should be analyzed to validate specifications of these loops early in the development cycle of self-adaptive systems. In this paper, we focus on qualitative analysis of multiple adaptation loops to validate their specifications. Our analysis explores all possible adaptation chains and checks whether their loops are possible to lead the qualities of the managed system to the desired levels. To this end, we devise a process for constructing behavior models of adaptation loops and the managed system, and a tool for automatically transforming the behavior models into a formal model used for model checking with a reachability property. We present case studies based on extended Znn.com to validate the results of our qualitative analysis. Furthermore, we also evaluate the scalability of the analysis and show that it can be completed within a reasonable amount of time.|None|reject|reject
FSE|2015|Focus on Major Devices Heavily Using Your Apps! –A Tale of Three Android App Categories|xuan lu,huoran li,xuanzhe liu,tao xie,blake bassett,gang huang 0001,feng feng|Mobile apps,testing prioritization,user behavior|The Android platform has been adopted by billions of smartphones and tablet computers. However, the fragmentation of Android devices causes substantial diculties for app developers when selecting major device models from thousands of device models to test their apps and nd device-specic problems. Due to the lack of knowledge of which device models are the major ones for their apps, app developers usually rely on the reported/predicted market share of device models to select device models to test against. To address the limitations faced by such coarse-grained selection, in this paper, we propose a new metric, called the time share, as an indicator to measure how long users interact with an app on a specic device model. We leverage the real-world in-eld user data collected from a leading Android management app, called Wandoujia, with about 4 million users, over 16 thousand device models, and about 0.2 million apps. We also propose a time-share-based approach to aiding app developers to reduce the space of device selection and prioritize the major devices in three popular networked-app categories, Communication, Media, and Game. Given 10 device models to test, our approach can more eectively and accurately select major device models that account for more user interaction time of an app, compared to device models recommended based on market share. We find that the number of major device models vary widely among dierent categories. Hence, we further explore how to apply our approach.|None|reject|reject
FSE|2015|Automated Debugging Assisted by Testing|ethar elsaka,atif m. memon|Automated Debugging,Automated Testing,Fault Localization,code coverage|Automated model-based test generation has seen an undeniable trend towards obtaining large numbers of test cases. However, the full benefits of this trend have not yet percolated to downstream activities, such as debugging.  We present Disqover for automated software debugging based on code sequence covers that leverages execution traces, or alternatively, sequence covers of large numbers of test cases to quickly identify causes of test failures, thereby aiding debugging. We develop a new algorithm that efficiently extracts commonalities between sequence covers in the form of ordered subsequences and values of variables contained in these subsequences that contribute to each failure. The results of our experimental evaluation suggest that (1) users of Disqover need only 30% of the time needed to identify faults compared to the baseline in a user study, and need to only inspect a very small number of lines of code to find the root cause of a failure compared to other state-of-the-art systems; we show that our number of inspected statements is actually smaller by multiple orders of magnitude and (2) increasing the number and diversity of test cases improves our results by further decreasing the length of output subsequences to be examined.|None|reject|pre-reject
FSE|2015|Mining Performance Specifications|marc brunink,david s. rosenblum|specification mining,performance modeling,modelling|Specifying requirements is hard. Tools and procedures have been developed to support initial elicitation and continuous verification of requirements. Unfortunately even with these tools it is still challenging and costly to collect a correct set of specifications. Specifications are not only used during initial development of a system but also later on during testing, maintenance, and software evolution. We propose to mine performance models from running systems. The mined models describe and specify the acceptable behavior of the system. They can be used as an evidence-based performance specification. We can mine either during in-house testing or during deploy- ment. The resulting models and specification can be used for performance regression testing and performance moni- toring. Inspecting the mined performance specification can lead to crucial new insights into the behavior of the system. Our goal is not to replace formal model-based approaches towards performance verification. Instead we believe our approach can augment these techniques. More importantly we hope that our tool will be helpful for all those systems in which strict performance modeling is currently not used, whether it be due to cost, missing resources, or insufficient knowledge.|None|reject|reject
FSE|2015|Uncertainty-Aware Software Development|takuya fukamachi,naoyasu ubayashi,shintaro hosoai,yasutaka kamei|Uncertainty,Interface,Partial Model|Embracing uncertainty in software development is one of the crucial research topics in software engineering. In most projects, we have to deal with uncertain concerns by using informal ways such as documents, mailing lists, or issue tracking systems. This task is tedious and error-prone. Uncertainty should be applied to all artifacts in software development phases, because uncertain concerns crosscut over multiple phases, e.g. requirements, design, and implementation. Moreover, it is uncertain in which phase uncertain concerns arise. If uncertainty can be dealt with modularly, we can add or delete uncertain concerns to/from models, code, and tests whenever these concerns arise or are fixed to certain concerns. This paper proposes Uncertainty-Aware Software Development (UASD) that can deal with uncertain concerns in a modular and formal way. Agile methods embrace change to accept changeable user requirements. On the other hand, our approach embraces uncertainty to support exploratory software development.|None|reject|reject
FSE|2015|Static Oracle Data Selection|junjie chen,yanwei bai,dan hao,bing xie,lu zhang,hong mei|test oracle,oracle data,static approach|In software testing, testers run the program with test inputs and determine whether the program contains faults based on a test oracle, which is a mechanism to verify whether the program under test behaves as expected. As internal variables are also effective in detecting faults, it is useful to select internal variables as oracle data when constructing a test oracle. In the literature, researchers proposed two dynamic approaches to oracle data selection by analyzing the execution information (e.g., variables' values or interactions between variables) of test cases. However, collecting this information during program execution may incur some extra cost. In this paper, we present a static approach to oracle data selection. In particular, our approach first identifies the substitution relationships between candidate oracle data by constructing a probabilistic substitution graph based on the definition-use chains of the program under test, then estimates the fault-observing capability of each candidate oracle data, and selects a subset of oracle data with strong fault-observing capability. For a program being tested in the JUnit testing framework, we extend this static approach by tailoring the program under test based on the callees of its JUnit tests before constructing its probabilistic substitution graph, because these JUnit tests tend to test the program partially. We conducted an experimental study on eleven projects written in two programming languages (i.e., C and Java). The results demonstrate that our static approaches achieve close effectiveness as the dynamic approaches, but are usually more efficient than the dynamic approaches.|None|reject|reject
FSE|2015|Relationship-Aware Code Search for JavaScript Frameworks|xuan li,zerui wang,qianxiang wang,shoumeng yan,tao xie,hong mei|Code search,JavaScript code mining,natural language processing|JavaScript frameworks, such as jQuery, are widely used for developing web applications. When using these JavaScript frameworks to implement a feature (e.g., functionality), a large number of programmers often search for code snippets that implement the same or similar feature. However, such code search with existing approaches tends to be ineffective, without taking into account the fact that JavaScript code snippets often implement a feature based on various relationships (e.g., sequencing and callback relationships) among the invoked framework API methods. To address this issue, in this paper, we present a Relationship-Aware Code Search (RACS) approach for finding code snippets that use JavaScript frameworks to implement a specific feature. RACS first collects a large number of code snippets that use some JavaScript frameworks in advance, mines API usage patterns from the collected code snippets, and represents the mined patterns with method call relationship (MCR) graphs, which capture framework API methods’ signatures and their relationships. Given a natural-language (NL) search query, RACS conducts NL processing to automatically transform the query to an action relationship (AR) graph, which consists of actions and their relationships inferred from the query. In this way, RACS reduces code search to the problem of graph search: finding similar MCR graphs for a given AR graph. We have conducted evaluations against popular real-world jQuery questions (posted on Stack Overflow), based on 308,294 code snippets collected from over 81,540 files on the Internet. The evaluation results show the effectiveness of RACS: the top 1 snippet produced by RACS matches the target code snippet for 46.7% questions, compared to only 3.3% achieved by an existing relationship-oblivious approach.|None|reject|reject
FSE|2015|Detecting Event Anomalies in Event-Based Systems|gholamreza safi,arman shahbazian,william g. j. halfond,nenad medvidovic|Event-based Systems,Implicit Invocation,Implicit Concurrency,Static Analysis,Race Detection,Android Applications|Event-based interaction is an attractive paradigm because its use can lead to highly flexible and adaptable systems. One problem in this paradigm is that events are sent, received, and processed nondeterministically, due to the systems’ reliance on implicit invocation and implicit concurrency. This nondeterminism can lead to event anomalies, which occur when an event based system receives multiple events that lead to the write of a shared field or memory location. Event anomalies can lead to unreliable, error-prone, and hard to debug behavior in an event based system. To detect these anomalies, this paper presents a new static analysis technique, DEvA, for automatically detecting event anomalies. DEvA has been evaluated on a set of open-source event-based systems against a state-of-the-art technique for detecting data races in multithreaded systems, and a recent technique for solving a similar problem with event processing in Android applications. DEvA exhibited high precision with respect to manually constructed ground truths, and was able to locate event anomalies that had not been detected by the existing solutions.|None|accept|pre-accept
FSE|2015|Improving model-based test generation by model decomposition|paolo arcaini,angelo gargantini,elvinia riccobene|test case generation,model-based testing,state explosion problem,abstraction|One of the well-known techniques for model-based test generation exploits the capability of model checkers to return counterexamples upon property violations. However, this approach is not always optimal in practice due to the required time and memory, or even not feasible due to the state explosion problem of model checking. A way to mitigate these limitations consists in decomposing a system model into suitable subsystem models separately analyzable. In this paper, we show a technique to decompose a system model into subsystems by exploiting the model variables dependency, and then we propose a test generation approach which builds tests for the single subsystems and combines them later in order to obtain tests for the system as a whole. Such approach mitigates the exponential increase of the test generation time and memory consumption, and, compared with the same model-based test generation technique applied to the whole system, shows to be more efficient. We prove that, although not complete, the approach is sound.|None|accept|accept
FSE|2015|Model-driven Declarative Deployment|stijn de gouw,michael lienhardt,jacopo mauro,behrooz nobakht,gianluigi zavattaro|Automatic Deployment,Optimal Deployment,Abstract Behavioral Specification,Cost Annotations|Production of modern software systems  frequently adopts a so-called continuous delivery approach, according to which there is a continuum between the development and the deployment phases. Nevertheless, at the modelling level, the combination between development and deployment is far from being a common practice. In this paper, we address the problem of promoting deployment as an integral part of modelling. To this aim, we adopt the object-oriented ABS language, which supports the modelling of systems composed of concurrent objects running inside deployment components. We extend ABS with class annotations expressing the requirements to be satisfied in order to deploy an object of that class. Then, we define a declarative deployment language and implement a tool that, starting from a high-level declaration of the desired system, computes a main program that instantiates an optimal deployment of the system.|None|reject|pre-reject
FSE|2015|Aligning Business and Software Processes: GQM+Strategies Revisited|luigi lavazza,sandro morasca,davide tosi|Software development process,Software process measurement,GQM,Domain representation|Background – GQM has proven itself useful in supporting the definition and execution of software measurement plans. However, GQM has not been as effective in supporting the need for linking software measurement goals to higher-level goals. Hence, the GQM+Strategies technique has been proposed to explicitly linking software measurement goals to the business world. However, little attention is given to the description of the business world. Objectives – In this paper, based on Jackson’s ideas on domain representation, we propose a method to precisely describe the business domain and its characteristics, the business goals, the strategies, their relationships with the software activities carried out to support the strategies, and how strategies are selected. Method – We propose a way to firstly describe the business world, including business and software processes, and secondly to specify the measurements to be carried out. Results – The proposed approach has been applied to both a case proposed in the literature and a case derived from the authors’ experience. Conclusions – The proposed approach proved very effective in supporting the investigation and descriptions of the business world. The creation of measurement plans according to the GQM+Strategies technique was greatly eased.|None|reject|reject
FSE|2015|UPMOA: An Improved Search Algorithm to Support User-Preference Multi-Objective Optimization|shuai wang,shaukat ali khan,tao yue,oyvind bakkeli,marius liaaen|User-Preference Multi-objective Optimization,Weight Assignment Strategy,Multi-objective Search Algorithms|Multi-objective search algorithms (e.g., non-dominated sorting genetic algorithm II (NSGA-II)) have been applied extensively to solve various multi-objective optimization problems in software engineering such as problems in testing and requirements phases. However, existing multi-objective algorithms usually treat all the objectives with equivalent priorities and do not provide a mechanism to reflect various user preferences when guiding search towards finding optimal solutions. The need to have such a mechanism was observed in one of our industrial projects on applying search algorithms for test optimization of a product line of Videoconferencing Systems (VCSs) called Saturn, where user preferences must be incorporated into optimization objectives, based on test requirements at Cisco. To address this, we propose to extend the most commonly-used multi-objective search algorithm NSGA-II that has also shown promising results in several existing works with user preferences and name it as User-Preference Multi-Objective Optimization Algorithm (UPMOA). Its core idea is to extend NSGA-II with a user preference indicator p, based on existing weight assignment strategies. We empirically evaluated UPMOA with two industrial problems focusing on optimizing the test execution system for Saturn in Cisco. To assess the scalability of UPMOA, in total 1000 artificial problems were created inspired from the two industrial problems. The evaluation includes two aspects: 1) Three weight assignment strategies together with UPMOA were empirically evaluated to identify a best weight assignment strategy for p. Results show that the Uniformly Distributed Weights (UDW) strategy can assist UPMOA in achieving the best performance; 2) UPMOA has been compared (via a comprehensive empirical study) with three representative multi-objective search algorithms (e.g., NSGA-II) and results show that UPMOA can significantly outperform the others and has the ability to solve a wide range of problems with varying complexity.|None|reject|pre-reject
FSE|2015|Modeling Readability to Improve Unit Tests|ermira daka,jose campos,gordon fraser,jonathan dorn,westley weimer|Readability,unit testing,automated test generation|Writing good unit tests can be tedious and error prone, but even once they are written, the job is not done: Developers need to reason about unit tests throughout software development and evolution, in order to diagnose test failures, maintain the tests, and to understand code written by other developers. Unreadable tests are more difficult to maintain and lose some of their value to developers. To overcome this problem, we propose a domain-specific model of unit test readability based on human judgements, and use this model to augment automated unit test generation. The resulting approach can automatically generate test suites with both high coverage and also improved readability. In human studies users prefer our improved tests and are able to answer maintenance questions about them 14% more quickly at the same level of accuracy.|None|accept|pre-accept
FSE|2015|Effective and Precise Dynamic Detection of Hidden Races for Java Programs|yan cai,lingwei cao|Data race,race detection,thread scheduling,hard concurrency bugs,synchronization order|Happens-before relation is widely used by many dynamic data race detectors to precisely detect data races. However, it could easily hide many data races as it is interleaving sensitive. Existing techniques based on randomized scheduling are ineffective on detecting these hidden races. In this paper, we propose DrFinder, an effective and precise dynamic technique to detect hidden races. Given an execution, DrFinder firstly analyzes the lock acquisitions in it and collects a set of "may-trigger" relations. Each may-trigger relation consists of a method and a type of a Java object. It indicates that, during execution, the method may directly or indirectly acquire a lock of the type. In the subsequent executions of the same program, DrFinder actively schedules the execution according to the set of collected may-trigger relations. It aims to reverse the happens-before relation that may exist in the previous executions so as to expose those hidden races. To effectively detect hidden races in each execution, DrFinder also collects a new set of may-trigger relation during its scheduling, which is used in the next scheduled execution. Our experiment on a suite of real-world Java multithreaded programs (including Eclipse, a large-scale one) shows that DrFinder is effective to detect 93 new data races in 10 runs. Many of these races could not be detected by existing techniques (i.e., FastTrack, ConTest, and PCT) even in 100 runs.|None|accept|accept
FSE|2015|iDice: Effective Identification of Emerging Issues|qingwei lin,jian-guang lou,hongyu zhang,chen luo,hongzhi chen,dongmei zhang|Emerging issues,issue detection,effective attribute combination,problem diagnostic,issue reports|One challenge in maintaining a large-scale software system, especially an online service system, is to quickly respond to customer issues; especially when a large number of issue reports are received by the support team. The issue reports typically have many attributes that reflect the characteristics of the issues. While the reported issues are diverse in terms of attribute combination, and many of them have low volume, sometimes, there are emerging issues that exhibit particular patterns of attribution combination leading to significant volume increase. It is important for support engineers to efficiently and effectively identify and resolve such emerging issues, since they have impacted a large number of customers. Currently, the identification of an emerging issue is tedious and error-prone, since it requires support engineers to manually identify a particular attribute combination among a large number of combinations. In this paper, we propose iDice, an algorithm that effectively and efficiently detects emerging issues and their associated attribute combinations. We evaluate the effectiveness and efficiency of iDice through in-house experiments. We have also successfully applied iDice to Microsoft online service systems in production. The results confirm that iDice is able to reduce the maintenance effort and the cost of customer service.|None|reject|reject
FSE|2015|Construction and Utilization of Problem-Solving Knowledge in Open Source Software Environments|hyung-min koo,in-young ko|Software Reuse,Open Source Software,Knowledge-based Software Reuse,Bayesian Network|Open Source Software (OSS) has become an important environment where developers can share reusable software assets in a collaborative manner. Although developers can find useful software assets to reuse in the OSS environment, they may face difficulties in finding solutions to problems that occur while integrating the assets with their own software. In OSS, sharing the experiences of solving similar problems among developers usually plays an important role in reducing problem-solving efforts. We analyzed how developers interact with each other to solve problems in OSS, and found that there is a common pattern of exchanging information about symptoms and causes of a problem. In particular, we found that many problems involve multiple symptoms and causes and it is critical to identify those symptoms and causes early to solve the problems more efficiently. We developed a Bayesian network based approach to semi-automatically construct a knowledge base for dealing with problems, and to recommend potential causes of a problem based on multiple symptoms reported in OSS. Our experiments showed that the approach is effective to recommend the core causes of a problem, and contributes to solving the problem in an efficient manner.|None|reject|reject
FSE|2015|DRIVER - A platform for collaborative framework understanding|nuno flores,ademar aguiar|frameworks,collaborative learning,platform,tools|Application frameworks are a powerful technique for large-scale reuse but often very hard to learn from scratch. Although good documentation helps on reducing the learning curve, it is often found lacking, and costly, as it needs to attend different audiences with disparate learning needs. When code and documentation prove insufficient, developers turn to their network of experts. The lack of awareness about the experts, interrupting the wrong people, and experts unavailability are well known hindrances to effective collaboration. The DRIVER platform is a collaborative learning environment for framework users to share their knowledge. It provides the documentation on a wiki, where the learning paths of the community of learners can be captured, shared, rated, and recommended. This paper presents the assessment of DRIVER using a controlled academic experiment that measured the performance, effectiveness and framework knowledge intake of MSc students, thus providing empirical evidence of its benefits.|None|reject|reject
FSE|2015|Driving νRules to Goals: A Structured Approach to Rule-based Adaptation|tianqi zhao,tao zan,haiyan zhao,zhenjiang hu,zhi jin|view-based rule,rule-based adaptation,goal-based adaptation,bidirectional transformation|Rule-based adaptation provides a powerful mechanism to program self-adaptive systems, where rules specify adaptation logic of what particular action should be performed to react to monitored events. It has advantages of readability and elegance of each individual rule, the efficiency of planning process, and the ease of rule modification. However, adaptation rules pay attention only to local transformation, which makes it difficult to satisfy global user goal. Moreover, they are static in that the adaptation logic defined by the rules cannot change at runtime, which prevents it from being adaptable to dy- namic goal change. In this paper, we propose an integrated approach that employs the global optimization of goal-based approach and the efficiency of rule-based approach to make user goal to be better satisfied efficiently. We present a new concept called vRule for structuring adaptation rules. The structured adaptation rules are not only expressive enough for programming intended adaptation logic, but also have invariants to associate rules with the goal-related states. We have designed and implemented a new view-based adaptation framework for supporting construction of adaptive systems, based on vRule and the feature modeling technique, and successfully applied it to realize a nontrivial smart room system.|None|reject|reject
FSE|2015|Developing an agent-oriented decision support model in a multidisciplinary and distributed clinical environment|liang xiao,john fox|Agent,Clinical Guideline,Distributed Decision Support,Model|The growing specialisation and ever-increasing inter-relationships in medicine today leads to multidisciplinary and distributed decision-making. Supporting these decisions raises some challenges to the current platforms and knowledge models. In this paper, an agent-oriented decision support model is put forward. The model is built around a goal-directed state-transition graph, a pattern-based composition of interaction protocol, and rule-based planning and argumentation. A multi-agent framework will interpret the model and support decision-making on alternative care paths, collaborative patterns, or plans in each step of the path, usually determing on what to believe (for example diagnosis) or what to act (for example treatment). The approach is illustrated using a case study of triple assessment of breast cancer.|None|reject|reject
FSE|2015|Inducing Knowledge Graphs for Computer Programming from Stack Overflow Tags|chunyang chen,zhenchang xing|Tag usage distribution,Tag associative network,Community detection,Knowledge graph structure and evolution|Developers search the web to learn new knowledge, extend their existing knowledge, and clarify technical details.Their web search often involves browsing, filtering, and digesting many online resources before their information needs are satisfied. This complex information seeking process, often referred to as informational search, calls for new methodologies for organizing the wealth of online programming resources. Enlightened by the recent advances in knowledge graph and semantic search, we intend to construct knowledge graph for computer programming that can capture the relationships of programming concepts, platforms, frameworks, and APIs that developers are interested in. This paper reports on our empirical study to investigate whether knowledge graph for computer programming can emerge from the questions in Stack Overflow. In this study, we consider question tags as entities of programming knowledge embedded in millions of questions, and try to mine the important correlations of knowledge entities to construct knowledge graph for computer programming. Our study shows that coherent knowledge graphs for computer programming can be mined from Stack Overflow tags that the askers use to annotate millions of questions.We present future directions in mining semantically rich knowledge graphs from various online program resources that can be leveraged for retrieving, aggregating, and presenting programming knowledge in the developers' informational search.|None|reject|pre-reject
FSE|2015|Scalable and Practical Thread Sharing Analysis for Java|jeff huang|Thread Sharing Analysis,Escape Analysis,Scalability,Record Replay,Race Detection|Localizing shared data accesses in multithreaded programs has many important applications. However, existing techniques such as escape analysis are either imprecise or impractical to scale to large programs. In this paper, we present two practical thread sharing analysis algorithms for Java that scale to real world large multithreaded systems such as Apache Derby and Eclipse. Our static algorithm, though simple, and without performing the expensive whole pro- gram information flow analysis, is much more efficient, less memory-demanding, and even more precise than the classical escape analysis algorithm. Our dynamic algorithm, powered by a location-based approach, achieves significant speedups over a dynamic escape analysis that tracks dynamic memory accesses precisely. Our evaluation results on a set of real world large complex Java systems with over 1.7MLOC show that our static algorithm is able to analyze these programs in a few minutes, whereas a state-of-the-art escape analysis either can not finish in an hour or runs out of memory, and our dynamic algorithm is as much as 730X faster than the precise dynamic escape analysis while achieving close precision. Moreover, we have applied our algorithms in two applications, a record-replay system and a dynamic race detector, and our results show that our algorithms reduce the record- ing overhead of the record-replay system by as much as 16X (on average 9X) and increase the runtime logging speed of the race detector by as much as 52% (on average 32%).|None|reject|reject
FSE|2015|Choose Your Branches Well: The Impact of the Branching Structure on Product Development|brendan murphy,jacek czerwonka,williams laurie|Branching,Software Processes,Version Control,Software Metrics|As a software product scales both in size and complexity, the success of its development becomes increasingly dependent upon its development process. Software projects often use branching structures to manage their development, which governs the process of merging and verifying newly developed code into the product. In spite of the importance of the branch structure, the product teams have little information to assist them in choosing the branching architecture appropriate for their projects. The goal of this research is to aid development teams in organizing their branching structure for overall effectiveness in terms of quality, productivity and speed by performing an analysis of branch and change data. To this end, we have studied the Windows, Bing and Azure engineering teams. Interviews were conducted and project data was analyzed. We have developed branching metrics characterizing their size and cohesion. We also developed metrics to measure the impact of the branching structure on product development, in terms of quality, productivity, and velocity. Through correlating branching metrics with project outcomes, we observed that across all products the larger, more cohesive branches realized improved productivity and quality, but not development speed. For products that have a greater degree of code interdependency, the number of code submitters and source files being developed on the same branch correlates with quality and speed but not productivity. Understanding these relationships is important in choosing an effective branching structure for a software project.|None|reject|reject
FSE|2015|Finding Schedule-Sensitive Branches|jeff huang,lawrence rauchwerger|Schedule-sensitivity Branches,Program Understanding,Concurrency Bugs,Distributed Trace Partition,Scalability|Branches in concurrent programs can be schedule-sensitive: their decisions not only depend on program input, but may also vary in different thread schedules. Automatically identifying schedule-sensitive branches is useful for both program comprehension and concurrency fault detection/localization, as for certain branches the schedule-sensitivity may not be intended but resulted from program faults. However, facing the huge thread scheduling space, it is difficult for programmers to reason about branch schedule-sensitivity. In this paper, we present an automated technique to find schedule- sensitive branches precisely based on symbolic constraint analysis and concolic execution, and uses off-the-shelf SMT solvers to verify if a branch can take a different choice in any feasible schedule. To scale our technique to real world pro- grams with long running executions, we have also developed a novel distributed trace partition approach that achieves good balance between analysis efficiency and effectiveness. We have implemented a prototype tool for Java and evaluated our technique on both popular benchmarks and large complex real applications. Our experimental results show that our technique is effective and has good scalability on real programs. In our study, we found a total of 34 schedule- sensitive branches, which were reported for the first time in this paper, with half of them related to concurrency errors.|None|accept|pre-accept
FSE|2015|Automated Service Selection using Knowledge Graphs|muneera bano,alessio ferrari,didar zowghi,vincenzo gervasi,stefania gnesi|Service selection,Requirements Engineering,Requirements,Knowledge Graphs,Natural Language Processing|With the huge number of services that are available online and offer similar functionality within the same cost range, requirements analysts face a paradox of choice when they have to select the most suitable service for a set of customer requirements, which, in most of the cases, are in natural language. In this context, an automated tool is required that can match customer requirements and service descriptions, and filter out irrelevant options. In this paper, we propose a natural language processing approach based on knowledge graphs that automates the process of service selection by matching customer requirements and service descriptions in natural language. To evaluate the approach, we have performed a case study, where we had 28 customer requirements and had to match them against 91 available service descriptions. Two teams worked separately on the case. One team evaluated all the 91 service descriptions manually against the requirements, whereas the other team applied the knowledge graph approach. We selected the top-15 ranked services in both the manual and automated approach, and found 53% similar results. Manual evaluation took 2 weeks whereas the automated approach produced results in about 25 minutes. Our approach helps analysts in reducing a very large set of available services to a much smaller and manageable set. Moreover, the approach works with natural language and hence does not require any effort of formalization. The results of our experiment are promising. New insights have also emerged for further improvement of the technique to increase the accuracy of the results.|None|reject|reject
FSE|2015|How Are Functionally Similar Code Clones Different?|stefan wagner,asim abdulkhaleq,ivan bogicevic,jan-peter ostberg,jasmin ramadani|functionally similar clones,difference categories,benchmark|Today, redundancy in source code, so-called “clones”, caused by copy&paste can be found reliably using clone detection tools. Redundancy can arise also independently, however, caused not by copy&paste. At present, it is not clear how only functionally similar clones (FSC) differ from clones cre- ated by copy&paste. Our aim is to understand and categorise the differences in FSCs that distinguish them from copy&paste clones in a way that helps clone detection research. We conducted an experiment using known functionally similar programs in Java and C from coding contests. We analysed syntactic similarity with traditional detection tools and explored whether concolic clone detection can go beyond syntax. We ran all tools on 2,800 programs and manually categorised the differences in a random sample of 70 program pairs. We found no FSCs where complete files were syntactically similar. We could detect a syntactic similarity in a part of the files in < 16 % of the program pairs. Concolic detection found 0.1 % of the FSCs. The differences between program pairs were in the categories algorithm, data structure, OO design, I/O and libraries. We selected 58 pairs for an openly accessible benchmark representing these categories. Most of the differences between functionally similar clones are beyond the capabilities of current clone detection approaches. Yet, our benchmark can help to improve detection results.|None|reject|pre-reject
FSE|2015|Search-based Optimal Ordering of Configuration Decisions of Cyber-Physical System Product Lines|tao yue,shaukat ali khan,hong lu,kunming nie|Product Line Engineering,Decision Ordering,Configuration,Search Algorithms,Empirical Study,Controlled Experiments|Product line engineering of Cyber-Physical Systems (CPSs) relies on a large number of various types of constraints to support correct configuration of deployable and operational products. Since industrial CPSs are highly complex in nature, a manual solution is error-prone and inefficient, and warrants the need for an automated solution supporting, for instance, automated value inference and optimizing configuration steps based on constraints. Fully automated solution is often not possible for CPSs since some decisions must be made manually by users and thus requiring an interactive solution. Having an interactive solution with tool support in mind, we propose a search-based solution to support optimal ordering of configuration steps (named as Zen-DO). Our optimization objective has three parts: 1) minimizing overall manual configuration steps: 2) configuring most constraining decisions first: 3) satisfying ordering dependencies among variabilities. We formulated our optimization objective as a fitness function and investigated it along with four search algorithms: Alternating Variable Method (AVM), (1+1) Evolutionary Algorithm (EA), Genetic Algorithm, and Random Search (a comparison baseline). Their performance is evaluated in terms of finding an optimal solution for two real-world case studies of varying complexity and results show that AVM and (1+1) EA significantly outperformed the others. We also conducted a controlled experiment to evaluate whether Zen-DO is effective in terms of reducing manual configuration effort to compare with an approach without the automated decision ordering functionality implemented. Results show that Zen-DO can significantly reduce manual configuration effort by dynamically recommending an optimal decision ordering to configuration engineers (i.e., end users of the interactive configuration tool).|None|reject|reject
FSE|2015|Mining API Usage Expertise in the Wild: A Qualitative Study|senthil mani,rohan padhye,vibha sinha|Mining software repositories,API,expertise,GitHub|Software repositories such as GitHub can be mined to extract a developer's expertise of third-party libraries by analyzing the APIs used in their source code check-ins. Such an API expertise profile can be useful for a wide variety of applications, from expert recommendation to social networking. However, due to variety in code design and other project process conventions, analyzing expertise from a heterogeneous landscape of projects poses many challenges. In this paper, we present a methodology for extracting API usages from Java source code in Git repositories. We evaluate both our methodology and its suitability for various applications by conducting two user surveys with open-source developers on GitHub. In our first survey, 123 developers provided assessment of their inferred API usage expertise on a scale of 1--5 based on our analysis of 70 active GitHub projects. The 916 ratings we received were normally distributed, but in the case of top 3 contributors of the top 3 used libraries in each project, over 85% developers rated themselves as 3 or higher. We then conducted a deeper study by analyzing all GitHub projects for 5 developers whose responses indicated they consider themselves experts in libraries whose APIs they use many times across multiple repositories, and that their self-assessment of expertise is subjective based on the intended application.|None|reject|reject
FSE|2015|Bounds on the Efficiency of Differential Testing|marcel bohme|Testing Theory,Efficient Testing,Partition Testing,Random Testing,Differential Partitioning|Partition-based Regression Verification (PRV) is one of the most effective automated differential testing techniques -- when it terminates it has exposed all functional differences between two versions  and otherwise guarantees functional equivalence. Differential Testing allows to determine with a certain degree of confidence whether and how the behavior of two programs differs. Its applications include regression testing, mutation testing, N-version testing, and the testing of one implementation against a reference implementation. PRV uses a clever mix of symbolic execution and program slicing which takes some time, say c time units per test case. However, the random generation of test cases takes inherently much less time, say 1 time unit per test case. Yet, what _use_ is the most effective technique if it takes too much time to expose more differences than random testing? Here, we study the efficiency of systematic differential testing techniques S (such as PRV) and compute bounds on c such that S remains more efficient than random testing. Given a time budget of n time units, we give the maximum time c0 that S can take per test input to be expected to expose as many or more differences than random testing in n time units. We also prove a sufficiently tight bound on c0 that depends only on n and the efficiency of random testing. In practice, we can estimate the maximum time c0 for S by executing only a few random tests on the given programs. These are strong, elementary, theoretical results that generalize to testing a program to expose a maximal number of errors within a given time budget.|None|reject|pre-reject
FSE|2015|Analytics Without Parameter Tuning Considered Harmful?|wei fu,tim menzies|defect prediction,CART,random forests,differential evolution,search-based software engineering|One of the “black arts” of data mining is setting the tuning parameters that control choices within a data miner. We offer a simple, automatic, and very effective method for finding those tunings. For the purposes of learning software defect predictors this optimization strategy can quickly find good tunings that dramatically change the performance of a learner. For example, in this paper we show tunings that alter detection precision from 2% to 98%. These results prompt for a change to standard methods in software analytics. At least for defect prediction, it is no longer enough to just run a data miner and present the result without first conducting a tuning optimization study. The implications for other kinds of software analytics are now open and pressing questions.|None|reject|reject
FSE|2015|Work Practices and Challenges in Pull-Based Development: The Contributor's Perspective|georgios gousios,alberto bacchelli|pull-based development,collaboration,contribution,pull request,Open Source Software,GitHub|The pull-based development model, which is gaining widespread popularity with open source systems, offers a novel way to contribute to projects. In this paper, we analyze how contributors experience this model investigating the motivations that drive contributors, their work practices behind the scenes, and the challenges they face. We set up an exploratory qualitative study involving a large-scale survey of more than 650 contributors and analyze it in the light of previous research on contributors' practices and the integrator's role in the pull-based model. Our key findings indicate that motivations are similar to those in other models, but the better traceability for contributions make it more appealing to create a code portfolio. In addition, contributors have a controversial relation with awareness, and contribution challenges are mostly social in nature and are exacerbated by the high-volume, sparse, and asynchronous nature of the pull-based model. Communication within pull-requests is limited to low level concerns. On these insights, we provide recommendations to practitioners and discuss implications.|None|reject|pre-reject
FSE|2015|How Do I Ask a Good Question? Assessing the Impact of Affect in Stack Overflow Questions|fabio calefato,filippo lanubile,m. raffaella merolla,nicole novielli|Community-based Q&A,Stack Overflow,Social Media,Successful Questions,Affective Computing,Sentiment Analysis|The growing success of Stack Overflow largely depends on the will of their members to answer others’ questions. Recent research has shown that factors that foster success of questions in online communities encompass time, reputation and presentation quality. Affective factors, instead, have not been investigated as much and, therefore, their impact is not evident so far.  In this paper, we describe an empirical study aimed at assessing whether the emotional style of a technical question has an influence on eliciting a satisfying answer. We focus on actionable factors that can be acted upon when asking for help. We found evidence that affective factors have an impact on success, as well as time and presentation quality. We also found that the probability of getting an accepted answer is influenced by the extent of askers’ participation in follow-up discussions, in which the emotional style plays a role as important as in question formulation.|None|reject|pre-reject
FSE|2015|MetroEyes: A Highly Interactive System for Exploring Multi-Dimensional Data|zhitao hou,hongyu zhang,haidong zhang,dongmei zhang|software analytics,data exploration,direct manipulation,visual analytics,software visualization|The existence of large amount of data, including software engineering data, requires effective tools to help users explore the data and obtain insights and actionable knowledge. Existing data exploration tools, such as Pivot Table in Microsoft Excel or statistical tools, have a high barrier of entrance for ordinary people to conduct data exploration. We propose MetroEyes, which is a visual analytics system for interactive data exploration and manipulation. In MetroEyes, data is represented as visual objects such as a slice in a pie chart, which are touchable and can be directly manipulated via simple and familiar gestures. Internally, the data exploration activities are represented as a composition of algebra operations and transformed to database queries. We apply MetroEyes to the analysis of software engineering data, including the Firefox crash data and GitHub project data. The experimental results show that MetroEyes enables easy, natural, and immersive experiences for data exploration and visualization.|None|reject|reject
FSE|2015|Improving Quality of Use Case Documents through Learning and User Interaction|shuang liu,jun sun,hao xiao,bimlesh wadhwa,jin song dong,xinyu wang|use case,requirement engineering,L*|Use case is a widely used scenario-based technique to capture user requirements. Use cases are mainly documented in natural language and sometimes aided with some graphical illustrations in the form of use case diagrams. Use cases serve as important means to communicate between stakeholders, requirement engineers and system engineers since they are easy to understand and are produced very early in the software development process. Having high quality use cases are beneficial in many ways, e.g., in avoiding inconsistency/ incompleteness in requirements, in guiding the system design, in helping generating quality test cases, etc. In this work, we propose an approach to improving the quality of use case documents using a range of techniques including natural language processing and machine learning. The central idea is to discover potential problems in implementing the use cases and feed back the problem at the level of use cases. We conduct case studies with a real-world use case document and show that our method is helpful in improving use cases with reasonable user interactions|None|reject|reject
FSE|2015|“Look Mum, No History!” Locating Bugs without Looking Back|tezcan dilshener,michel wermelinger,yijun yu|Software maintenance,lexical search,key positions,stack trace,change requests,empirical study|Bug localization is a core program comprehension task in software maintenance: given the observation of a bug, e.g. via a bug report or a faulty execution trace, where is it located in the source code? Information retrieval (IR) approaches see the bug report as the query, and the source code files as the documents to be retrieved, ranked by relevance. Such approaches have the advantage of not requiring expensive static or dynamic analysis of the code. However, current state-of-the-art IR approaches rely on project history, in particular previously fixed bugs and previous versions of the source code. We present a novel approach that directly scores each current file against the given report, thus not requiring past code and reports. The scoring is based on heuristics identified through manual inspection of bug reports. We compare our approach to three others, using their own five metrics on their own four case studies. Out of the 20 performance indicators, we match 2 and improve 14. On average, for 79% of the bug reports we place one or more affected files in the top 10 ranked files. These results show the applicability of our approach to software projects without history.|None|reject|reject
FSE|2015|Causality and Causal Probability : A Systematic Approach for Engineering Elastic Distributed Software|k. r. jayaram|causality,causal probability,elasticity|Engineering robust elasticity management systems is vital to modern "born-on-the-cloud" distributed applications as well as legacy distributed applications when they are migrated to the cloud. While the software engineering process for elasticity management is reasonably well-understood, the mechanisms used are often ad-hoc and not rooted in application semantics or interaction between application components. This goal of this paper is to anchor elasticity in terms of causality in distributed applications. Assuming a large-scale distributed application architected as a set of interacting components, we motivate the need for (1) analyzing *causality* between interactions, and (2) estimating *casual probability* that an increase in the frequency of interaction $i_1$ can increase the frequency of interaction $i_2$ caused by $i_1$. We present algorithms to estimate causality and causal probability by combining well known sampling, path profiling and dynamic slicing algorithms. We apply our algorithms for causal probability to three \emph{generic} distributed applications, to evaluate (a) their effectiveness in the timely provisioning and de-provisioning of compute resources and (b) whether causality and causal probability present a fundamental and widely-applicable way of thinking about and engineering auto-elasticity.|None|reject|pre-reject
FSE|2015|Watch out for This Commit! A Study of Influential Program Changes|daoyuan li,li li,dongsun kim,tegawende bissyande,yves le traon|Program changes,Change prediction,Program evolution,Change influence|One single code change can have a significant influence on a wide range of a software system and its users. For example, 1) adding a new feature can spread defects to the whole program, 2) updating a configuration may fix a problem hidden in all branches, and 3) changing an API method can improve the performance of all client programs. Developers often may not clearly know whether her/his or others’ changes are influential at the commit time. Rather, it turns out to be influential after affecting many aspects of a system later. Since some influential changes have negative effects, it is important to predict whether it is influential immediately after a change is submitted. Even for influential changes with positive effects, it is necessary to identify because other developers can benefit from the changes earlier. This paper proposes an approach to influential change identification. We first conduct a post-mortem analysis to discover existing influential changes by using intuitions such as isolated changes and changes referred by other changes. Based on the influential changes discovered in the post-mortem analysis, we extract features of the changes. These features include several metrics such as the complexity of the changes, terms in commit logs, and their centrality in co-change graphs. Using the feature vectors of the changes, we build a prediction model based on machine learning algorithms. To evaluate the model, we conduct experiments on 10 open source projects. The experiment results show that our prediction model achieves overall 96.1% precision, 94.9% recall, and 95.2% F-measure, respectively. We further demonstrate the capability of our classifiers to perform in the wild for identifying influential changes previously unknown in the results of our observational study.|None|reject|pre-reject
FSE|2015|A Thread Debugger for Testing and Debugging Arbitrary Interleavings on Multi-Threaded Programs|yung-pin cheng,jen wei kuo,wan-yuan chen|multi-threading debugging,thread Debuggers,heisenbugs,break points,concurrency error detection,testing of multi-threaded program,Interleaving|Most applications are multi-threaded nowadays. Concurrency may introduce errors that are created by data and timing dependencies and which are known for being hard to detect, reproduce, and debug. The primary tool for fixing a bug is a debugger. Unfortunately, while using a debugger to fix a concurrency bug, probe effect makes the bug-triggering interleaving unreproducible. In addition, support for debugging threads in existing debuggers is unsatisfactory. The lack of a powerful thread debugger has impeded concurrent programming and makes writing correct concurrent programs a difficult task in practice. In this paper, a novel thread debugger called xThreadDebugger is proposed. xThreadDebugger provides three new kinds of break points: interleaving control point (ICP), logging trace point (LTP), and logging watch point (LWP). When an ICP is hit in an execution, it forces a context switch within threads. A series of ICPs allows arbitrary interleavings to be created to test and debug a multi-threaded program. Besides, a pre-existing interleaving can be deterministically replayed to support the cyclic debugging strategy. LTPs and LWPs are non-stop trace points to record logs into a trace in JSON format. The JSON traces can be used as an open format with which bug detection algorithms can analyze.|None|reject|reject
FSE|2015|Safe Concurrent Resource Access using Shared Ownership|mischael schill,sebastian nanz,bertrand meyer|Concurrency,Shared Resources,Shared Ownership,Data Races,Analysis,Operational Semantics|In shared-memory concurrent programming, shared resources can be protected using synchronization mechanisms such as monitors or channels. The connection between these mechanisms and the resources they protect is, however, only given implicitly; this makes it difficult both for programmers to apply the mechanisms correctly and for compilers to check that resources are properly protected. This paper presents shared ownership as a policy for accessing shared resources. Shared ownership represents resource associations using an ownership graph and defines a set of safe operations over the graph enabling the sharing of resources between processes. In contrast to the traditional ownership methodology, shared ownership offers more flexibility by permitting multiple owners of a resource. The paper introduces an analysis of program traces to check for adherence to the policy. Based on the inference rules of the analysis, the paper shows that the policy ensures freedom from data races. The generation of usable program traces for the analysis is explained for object-oriented languages and various synchronization techniques. Finally, the practicality of the approach is demonstrated using a prototype integrated into the Java programming language.|None|reject|reject
FSE|2015|Binary Code Is Not Easy|xiaozhu meng,barton p. miller|Static binary code analysis,Challenging code constructs,Jump table model|Binary code analysis is an enabling technique for many applications. Binary code analysis tool kits provide several capabilities to automatically analyze binary code, including decoding machine instructions, building control flow graphs of the program, and determining function boundaries. Modern compilers and run-time libraries have introduced significant complexities to binary code. These complexities negatively affect the capabilities of binary analysis tool kits, which may cause tools to report inaccurate information about binary code. Analysts may hence be confused and applications based on binary analysis tool kits may have degrading quality. We examine the problem of constructing control flow graphs from binary code and labeling the graphs with accurate function boundary annotations. We identify eight challenging code constructs that represent hard-to-analyze aspects of the code from modern compilers, and show code examples that illustrate each code construct. As part of this discussion, we discuss how the new code parsing algorithms in the open source Dyninst tool kit support these constructs. In particular, we present a new model for describing jump tables that improves our ability to precisely determine the control flow targets, a new interprocedural analysis to determine when a function is non-returning, and techniques for handling tail calls, code overlapping between functions, and code overlapping within instructions. We created test binaries patterned after each challenging code construct and evaluated how tool kits, including Dyninst, fare when parsing these binaries.|None|reject|reject
FSE|2015|How Hard Does Mutation Analysis Have To Be, Anyway?|rahul gopinath,mohammad amin alipour,iftekhar ahmed,carlos jensen,alex groce|Testing and debugging,Testing tools,Empirical Analysis,Statistical Analysis,Test frameworks,Mutation analysis,Mutation Operators|Mutation analysis is considered the best method for measuring the adequacy of test suites. However, the number of test runs required to do a full mutation analysis grows faster than project size, which makes it an unrealistic metric for most real-world software projects, which often have more than a million lines of code.  It is in projects of this size, however, that developers most need a method for evaluating the efficacy of a test suite. Another impediment to adoption of mutation analysis is the equivalent mutant problem, which makes mutation scores less informative. Various strategies have been proposed to deal with the explosion of mutants, including forms of operator selection and x% stratified sampling based on operators and program elements. However, these strategies at best reduce the number of mutants required to a fraction of overall mutants, which still grows with program size. Running, e.g., 5% of all mutants of a two million LOC program usually requires analyzing well over 100,000 mutants. Similarly, while various approaches have been proposed to tackle equivalent mutants, none completely eliminate the problem, and the fraction of equivalent mutants remaining is hard to estimate, often requiring manual analysis of equivalence. In this paper, we provide both theoretical analysis and empirical evidence that a small constant sample of mutants yields statistically similar results to running a full mutation analysis, regardless of the size of the program being analyzed or the similarity between mutants. We show that a similar approach, of using a constant sample of inputs can estimate the degree of stubbornness in mutants remaining to a high degree of statistical confidence. We also provide a mutation analysis framework for Python that incorporates the analysis of stubbornness of mutants.|None|reject|reject
FSE|2015|How Do You Feel, Developer? An Explanatory Theory of the Impact of Affects on Programming Performance|daniel graziotin,xiaofeng wang,pekka abrahamsson|Affects,emotions,moods,human aspects of software engineering,psychology of programming,performance,productivity,interpretivism,process theory|Affects---emotions and moods---have an impact on cognitive processing activities and the working performance of individuals. Development tasks are undertaken through cognitive processing activities. Yet, software engineering research lacks theory on affects in software development. In this paper, we report an interpretive study aimed to broaden our understanding of the psychology of programming in terms of affects perception and their impact while programming. We conducted a qualitative interpretive study based on face-to-face, open-ended interviews, in-field observations, and e-mail exchanges, which enabled us to construct a novel explanatory theory of the impact of affects on development performance. The theory is explicated using an established taxonomy framework. The proposed theory builds upon the concepts of events, affects, attractors, focus, goal, and performance. Theoretical and practical implications are given.|None|reject|reject
FSE|2015|Towards the Creation of Object Ensembles by Fully Automated Adaptation on the Basis of Test Cases|dominic seiffert|automated object adaptation,signature mismatches,object ensembles,test cases|Software building blocks sometimes can not be connected together because of signature mismatches. This can potentially be solved by adaptation. Our adapter generation tool Cordelia overcomes a subset of signature mismatches on objects by taking simple unit test cases as an input. In the previous version it was possible to adapt a single adaptee only. Therefore, we upgraded its ability to adapt more than one adaptee in order to create object ensembles. Thereby, we tackle the problem of message-splitting and message-merging. We proof Cordelia’s enhanced capabilities using real-world and constructed examples. We conclude that our adapter generation tool provides another step towards the automated creation of object ensembles by fully automated adaptation, but there are still many open research challenges left in order to provide fully automated adaptation.|None|reject|reject
FSE|2015|Multi-view Architecture Dependency Analysis in Safety-Critical Systems|robert nord,raghvinder sangwan,julien delange,peter h. feiler,luke thomas,ipek ozkaya|software architecture,architecture views,architecture analysis,dependency analysis,change propagation,testing,dependency structure matrix|Analysis of structural dependencies among software elements helps developers comprehend the impact of change. Such analysis is mostly limited to module views of a system constructed using dependencies derived from its implementation structure. This practice can miss dependencies associated with views of the software architecture that deal with allocation of modules to implementation packages in the development environment to improve safety-critical testing or allocation of implementation packages to hardware partitions to optimize performance. In this paper, we present an in-depth case study of an industrial, safety-critical control system that endured major refactoring as a result of missed architectural dependencies. We create a unified model for this system using an approach to modeling and analyzing multi-view architecture dependencies—including multiple perspectives incorporating how the system is implemented, tested, and deployed—to uncover critical architectural information. Although existing tools cannot yet incorporate unified multi-view dependency modeling, our findings demonstrate that providing developers with a lightweight framework containing clear descriptions of dependencies enables them to reason about change impact and propagation implications that they might otherwise miss.|None|reject|reject
FSE|2015|Supporting Program Comprehension using Structural Search|venkatesh vinayakarao,rahul purandare,aditya v. nori,matthew b. dwyer|Example Retrieval,Program Comprehension,Code Search,Structural Complexity|Vocabulary used in a program is not always intuitive and looking at variant implementations help in better program comprehension. We present an approach to identify topics associated with snippets in a given input program and show alternate implementations from discussion forums. Unlike existing topic modeling techniques such as Latent Dirichlet Allocation that depend on distribution of words, we model this as a structural code search problem over a repository of code snippets. We  represent structural information in code snippets as searchable text so that Information Retrieval (IR) techniques can be used to query them. We have implemented this approach in a tool called jSense. jSense constructs a repository of variant implementations with a precision of 91.7% and recall of 71%. Using this repository, jSense matching engine is capable of identifying topics in the given input program with a precision of 76.9% and recall of 66.7%. The repository of variant implementations along with the source code for jSense has been released for further research. Our work opens up new possibilities for attacking a wide variety of difficult problems in exploiting structural information which can support applications such as semantic clone detection and defect localization.|None|reject|reject
FSE|2015|A Research in Testing of Functional Programs: a Systematic Mapping|alexandre ponce de oliveira,paulo lopes de souza,simone r. s. souza,julio cezar estrella,sarita bruschi|Testing,Functional programs,Erlang,Testing criteria,Test models,Coverage.|Functional languages, such as Erlang, Haskell and Scala are able to develop real-time and fault-tolerant parallel programs. In general, these programs are used in critical systems such as telephone switching networks and must provide high quality, reliability and efficiency. In this context, validation, verification and testing activities are necessary and contribute to improving the quality of functional programs. This paper presents a systematic mapping concerning the testing of functional programs, considering also their parallel/concurrent aspects. The paper describes the three stages used during the systematic mapping: planning, execution and presentation of results. The mapping was able to identify only twenty-two relevant studies. In these studies, fourteen considered test models, three used data flow testing, twelve used/proposed testing tools and five considered concurrent/parallel aspects of such programs. The results indicate that there are few researchers working on testing of functional programs and that few studies are concentrated almost exclusively in the Erlang language. The main testing technique found in the papers is the structural one, however, it does not properly consider the software testing methodology already established for the imperative programming. Indeed, the results show gaps in the area of testing of functional programs, even for Erlang, the most considered language by the studies. These gaps are presented and discussed at the end of this paper.|None|reject|reject
FSE|2015|Qualitative Study of Successful  Agile Software Development Projects – Competences, Complex Adaptive Systems and Emergent Pattern|martin kropp,andreas meier|Agile software development,complex adaptive systems,cynefin,software process|Various studies show that the agile method has become a mainstream methodology for software development. When agile pioneers introduced this approach, they executed very successful projects and which lead to the enormous popularity of agile development. With becoming mainstream, less experienced teams started to apply the agile approaches and news about failed agile projects appeared. This raises the question, what it needs to conduct successful agile projects. In a qualitative study we asked IT companies about the essential success factors in their successful agile projects. We found that there was a strong focus on engineering and management best practices. We found that when these practices did not work, mature teams sensed that following a recipe is not sufficient, and they started adapting the agile process to their needs.  We observed a constant probing, sensing and appropriate responding. This is the typical pattern for moving forward in complex adaptive systems. Applying a sense-making methodology like the Cynefin framework, theoretically explains our observations in the study. That suggests that companies should be aware, that software projects are often located in the complex domain and that these problems require rather emergent practices instead of good or best practices and an understanding of the implications of complexity theory.|None|reject|reject
FSE|2015|Assertion Guided Symbolic Execution of Multithreaded Programs|shengjian guo,markus kusano,chao wang,zijiang yang,aarti gupta|symbolic execution,partial order reduction,multithreaded program,weakest precondition,predicates,pruning,SMT solvers|Symbolic execution has emerged as a powerful technique for systematic testing of sequential and multithreaded programs. However, its application is limited by the high computational cost of covering all feasible intra-thread paths and inter-thread interleavings. We propose a new assertion guided pruning framework that identifies executions guaranteed not to lead to an error state and removes them during symbolic execution. By summarizing the reasons why previously explored executions cannot reach an error state and using the information to prune redundant executions in the future, we can soundly reduce the number of executed paths and interleavings exponentially. We also use static concurrent program slicing and heuristic minimization of symbolic constraints to further reduce the computational overhead. We have implemented our method in the Cloud9 symbolic execution tool and evaluated it on a large set of multithreaded C/C++ programs. Our experiments show that the new method can reduce the overall computational cost significantly.|None|accept|pre-accept
FSE|2015|Specification and Instantiation of a Design Pattern for Controller in ADAS|hela marouane,achraf makni,bruno sadeg,claude duvallet,rafik bouaziz|Design patterns,UML profile,Reuse,ADAS,Decision subsystem|Advanced Driver Assistance Systems (ADAS) are hard real-time embedded systems in the automotive domain. They consist mainly of data acquisition, decision and action subsystems. The decision subsystem represents the core of ADAS. It is responsible for analyzing data, assessing the situation and taking the decision. The design is a difficult task since it must take into account the specification of real-time constraints related to data and transactions. The design of this subsystem can be facilitated through the reuse of real-time design patterns that improve the quality of the development process and reduces the complexity of design. In this paper, we focus on defining a real-time design pattern named ADAS-Controller. This pattern describes inputs and outputs of the decision subsystem, its behavior interrelationships, its non functional requirements and its real-time aspects. To make it more flexible and understandable, and to facilitate the reuse process, we add specific semantics to some UML concepts, with an appropriate UML profile that takes into account the design of both real-time concepts and variability on patterns. We illustate the reuse of the proposed pattern through the modeling of an application using a CASE toolset we have developped.|None|reject|reject
FSE|2015|Symbolic Execution of Programs with Heap Inputs|pietro braione,giovanni denaro,mauro pezze|Symbolic execution of heap manipulating programs,Symbolic execution with structured inputs,Symbolic execution with lazy initialization|Symbolic analysis is a core component of many automatic test generation and program verification approaches. To verify complex software systems, test and analysis techniques shall deal with the many aspects of the target systems at different granularity levels.  In particular, testing software programs that make extensive use of heap data structures at unit and integration levels requires generating suitable input data structures in the heap. This is a main challenge for symbolic testing and analysis techniques that work well when dealing with numeric inputs, but do not satisfactorily cope with heap data structures yet. In this paper we propose a new way to handle heap structures and structural constraints in symbolic execution. The classic approach of lazy initialization that generalizes symbolic execution to handle the symbolic references to and between the objects in the heap, may result in exploring  large amounts of invalid symbolic states, that is, symbolic states that include objects that violate structural constraints, and in raising false alarms associated with these invalid states. In this paper we propose a language HEX to specify invariants of partially initialized data structures, and a decision procedure that supports the incremental evaluation of structural properties in HEX. Used in combination with the symbolic execution of heap manipulating programs, HEX prevents the exploration of invalid states, thus improving the efficiency of program testing and analysis, and avoiding false alarms that negatively impact on verification activities. The experimental data confirm that HEX is an effective and efficient solution to the problem of testing and analyzing heap manipulating programs, and outperforms the alternative  approaches that have been proposed so far.|None|accept|accept
FSE|2015|Measure Your CRUD: Proposing a Forensic-ability Metric for User Activity Logs Using Black-box Testing|jason s. king,laurie a. williams|Logging mechanism,metric,metric validation,measurement,security,privacy,behavior-driven development,verification,forensic analysis,forensic-ability,accountability,user activity log,log,nonrepudiation|User accountability in software systems is important for mitigating repudiation threats and detecting security or privacy breaches. Logging mechanisms that capture detailed traces of user activity, including creating, reading, updating, and deleting (CRUD) data, enable forensic analysis following a privacy or security breach. However, software requirements often inadequately and inconsistently state “what” user actions should be logged, thus hindering meaningful forensic analysis. The objective of this research is to help software developers improve the forensic-ability of user activity logs by using black-box testing to systematically measure the coverage of mandatory log events captured in user activity logs. In previous work, we describe a heuristics-driven process for identifying mandatory log events (MLEs) described in natural language software requirements artifacts. In this paper, we systematically document an automated test case for each of the identified MLEs for a software system. To evaluate our work, we perform case studies on three open-source software systems.  We implement and run a total of 558 black-box test cases against the iHRIS human-resources management system (58 tests), iTrust electronic health records system (238 tests), and OCS academic conference management system (262 tests). We calculate forensic-ability of the three systems as 0 (0% passing MLE test case), 0.58 (58% passing MLE test cases), and 0.007 (<1% passing MLE test cases), respectively. The forensic-ability values suggest that many malicious activities would be untraceable in these systems. Overall, software engineers should measure forensic-ability throughout the software development lifecycle to help make empirically informed decisions regarding the implementation of logging mechanisms to enable meaningful forensic analysis.|None|reject|reject
FSE|2015|When, How, And Why Developers (Do Not) Test|moritz beller,georgios gousios,annibale panichella,andy zaidman|Developer Testing,Unit Tests,Testing Effort,Field Study,Test-Driven Development (TDD)|The research community in Software Engineering and Software Testing in particular builds its contributions on a set of mutually shared expectations. These expectations often form the motivation for research on the creation of new testing tools and the refinement of existing test processes in an attempt to support practitioners. Despite the fact that they are the basis for many publications as well as open-source and commercial testing applications, these expectations and common beliefs are rarely ever questioned. For example, Frederic Brooks’ statement that testing takes half of the development time seems to have manifested itself within the community since he first made it in the “Mythical Man Month” in 1975. With this paper, we report on the surprising results of a large-scale field study with 416 software engineers whose development activity we closely monitored over the course of five months, resulting in over 13 years of recorded work time. Our findings question several commonly shared assumptions and beliefs about testing and might be contributing factors to the observed bug proneness of software in practice: the majority of developers in our study does not test; developers rarely run their tests in the IDE; Test-driven Development is not widely practiced; and, last but not least, software developers only spend a quarter of their work time engineering tests on average, whereas they think they test half of their time.|None|accept|accept
FSE|2015|Predicting Software Field Reliability|pete rotella,sunita chulani,devesh goyal|software release reliability,prediction,modeling,testing,release quality,error analysis,customer experience|The objective of the work described is to accurately predict, as early as possible in the software lifecycle, how reliably a new software release will behave in the field.  The initiative is based on a set of innovative mathematical models that have consistently shown a high correlation between key in-process metrics and our primary customer experience metric, SWDPMH (Software Defects per Million Hours [usage] per Month).  We have focused on the three primary dimensions of testing – incoming, fixed, and backlog bugs.  All of the key predictive metrics described here are empirically-derived, and in specific quantitative terms have not previously been documented in the software engineering/quality literature.  A key part of this work is the empirical determination of the precision of the measurements of the primary predictive variables, and the determination of the prediction (outcome) error.  These error values enable teams to accurately gauge bug finding and fixing progress, week by week, during the primary test period.|None|reject|reject
FSE|2015|Predicting Release Quality for Multiple Platforms|pete rotella|modeling,software quality,prediction,customer experience,SWDPMH|One difficulty in characterizing the quality of a major feature release is that many releases are  implemented on several platforms, with each platform using a different subset of the new features.  Also, these platforms can have substantially different performance expectations and results.  In order to characterize the entire release adequately in predictive models, we need a robust customer experience metric that is capable of representing many disparate platforms.  Several multi-platform SWDPMH (software defects per million usage hours per month) variants have  been developed in an attempt to anticipate a release's overall field quality.  In addition to predicting the overall release quality, it is critical that we provide guidance to business units concerning remediation of releases predicted to not achieve adequate quality, and also provide guidance regarding how to modify practices so subsequent releases achieve adequate quality.  Models have been developed to both predict MP-SWDPMH and to identify specific in-process drivers that likely influence MP-SWDPMH.  At this time, these modeling results can be available as early as five or six months prior to release to the customers.|None|reject|reject
FSE|2015|Towards Debugging Machine Learning Tasks|aleksandar chakarov,aditya v. nori,sriram k. rajamani,selva samuel,shayak sen,deepak vijaykeerthy|Debugging,Machine Learning,Causality,Development environments and tools|Machine learning tasks, in contrast to traditional applications, employ programs with relatively small amounts of code (implemented  as machine learning libraries), but with  relatively voluminous amounts of data. Just like developers of traditional programs debug errors in their code, developers of machine learning tasks debug and fix errors in their data. However, algorithms and tools for debugging and fixing errors in data are less common when compared to their counterparts for detecting and fixing errors in code. In this paper, we consider classification tasks where errors in training data lead to  misclassifications in test data, and propose an automated method to find the root causes of such misclassifications.  Our analysis is based on Pearl's theory of causality, and uses Pearl's PS (Probability of Sufficiency) as a scoring metric. Our implementation, PSI, encodes the computation of PS as a probabilistic program, and uses recent progress in probabilistic programming efficiently compute PS. In order to avoid expensive retraining, PSI uses approximate models of machine learning classification algorithms, and we show how to contruct such approximate models for any classification algorithm based on gradient descent. PSI is able to identify root causes of data errors in real-world machine learning classification tasks.|None|reject|reject
FSE|2015|Effective Specification of Fault Tolerant Distributed Software|andi bejleri,tzu-chun chen,mohammad quidesat,lukasz ziarek,patrick th. eugster|Session types,Protocol,Fault tolerance,Synchronisation,Nesting,Code quality|Distributed systems are plagued by partial failures, meaning that certain components or interactions may fail, while others remain unaffected. If such failures are improperly handled in message-passing distributed systems, software components may get stuck waiting for messages that will never arrive, or enter an inconsistent state after receiving inappropriate messages. Handling of partial failures is an intrinsic part of many protocols, yet very involving as it depends on many factors including their timing and nature, the affected component(s), and of course application semantics. In this paper, we present a novel specification technique and verification framework for fault tolerant distributed software based on the theory of session types. More precisely we contribute with a specification technique that models structured interactions and fine-grained failure handling, a software framework that can be used to implement fault-tolerant distributed systems in Java, and a static analysis that checks conformance of corresponding software to their specification. Finally, a detailed empirical study, including a code quality analysis comparing with straightforward approaches attempting to achieve the same based on predating techniques without explicit support for failures, demonstrates the usefulness, simplicity, and efficiency of our technique and tools.|None|reject|reject
FSE|2015|Dealing with Incompleteness in Automata-Based Model Checking|claudio menghi,paola spoletini,carlo ghezzi|Incomplete Model Checking,Incremental Verification,Step-wise development|Software development can be structured as a process through which the model of a system and specification of its requirements gradually evolve over time until the final model of the system is produced. During the process, the model is often incomplete: decisions on how to detail of certain functionalities can be postponed to a later stage or they may be delegated to third parties. An incomplete functionality may be later replaced by alternative solutions, which may be iteratively explored to evaluate tradeoffs. Model checking can be used to verify that a model of the system under development is compliant with its specified requirements. However, classical model checking algorithms assume that a complete model of the system and the fully specified requirements are given: they do not support incompleteness. A verification-driven design process would instead benefit from the ability to apply formal verification at any stage, hence also to incomplete models. To achieve this goal, this paper extends the classical automata-based model checking procedure for LTL properties to deal with incompleteness. The proposed model checking approach is able not only to evaluate if a specification holds or not in an incomplete model but, when the satisfaction of the specification depends on the incomplete parts, also to compute a constraint that must be satisfied by future completions of the unknown parts.|None|reject|reject
FSE|2015|MorphDroid: Fine-grained Privacy Verification|pietro ferrara,omer tripp,marco pistoia|Static Analysis,Privacy Analysis,Information-flow Security,Mobile,Android|Mobile devices are rich in sensors, such as a Global Positioning System (GPS) tracker, microphone and camera, and have access to numerous sources of personal information, including the device ID, contacts and social data. This richness increases the functionality of mobile apps, but also creates privacy threats. As a result, different solutions have been proposed to verify or enforce privacy policies. A key limitation of existing approaches is that they reason about privacy at a coarse level, without accounting for declassification rules, such that the location for instance is treated as a single unit of information without reference to its many fields. As a result, legitimate app behaviors — such as releasing the user’s city rather than exact address — are perceived as privacy violations, rendering existing analyses overly conservative and thus of limited usability. In this paper, we present MORPHDROID, a novel static analysis algorithm that verifies mobile applications against fine-grained privacy policies. Such policies define constraints over combinations of fine-grained units of private data. Specifically, through a novel design, MORPHDROID tracks flows of fine-grained privacy units while addressing important challenges, including (i) detection of correlations between different units (e.g. longitude and latitude) and (ii) modeling of semantic transformations over private data (e.g. conversion of the location into an address). We have implemented MORPHDROID, and present a thorough experimental evaluation atop the 500 top-popular Google Play applications in 2014. Our experiments involve a spectrum of 5 security policies, ranging from a strict coarse-grained policy to a more realistic fine-grained policy that accounts for declassification rules. Our experiments show that the gap is dramatic, with the most conservative policy detecting violations in 171 of the applications (34%), and the more realistic policy flagging only 4 of the applications as misbehaved (< 1%). In addition, MORPHDROID exhibits good performance with an average analysis time of < 20 seconds, where on average apps consist 1.4M lines of code.|None|reject|reject
FSE|2015|Turning Programs Against Each Other: High Coverage Fuzz-testing Using Binary-code Mutation and Dynamic Slicing|ulf kargen,nahid shahmehri|fuzz testing,fuzzing,black-box,dynamic slicing,program mutation|Mutation-based fuzzing is a popular and widely employed black-box testing technique for finding security and robustness bugs in software. It owes much of its success to its simplicity; a well-formed seed input is mutated, e.g. through random bit-flipping, to produce test inputs. While reducing the need of human effort, and enabling security testing even of closed-source programs with undocumented input formats, the simplicity of mutation-based fuzzing comes at the cost of poor code coverage. Often millions of iterations are needed, and the results are highly dependent on configuration parameters and the choice of seed inputs. In this paper we propose a novel method for automated generation of high-coverage test cases for robustness testing. Our method is based on the observation that, even for closed-source programs with proprietary input formats, an implementation that can generate well-formed inputs to the program is typically available. By systematically mutating the program code of such generating programs, we leverage information about the input format encoded in the generating program to produce high-coverage test inputs, capable of reaching deep states in the program under test. Our method works entirely at the machine-code level, enabling use-cases similar to traditional black-box fuzzing. We have implemented the method in our tool MutaGen, and evaluated it on 7 popular Linux programs. We found that, for most programs, our method improves code coverage with one order of magnitude or more, compared to two well-known mutation-based fuzzers. We also found a total of 8 unique bugs.|None|accept|accept
FSE|2015|Over-exposed Classes in Java: An Empirical Study|santiago vidal,alexandre bergel,j. andres diaz-pace,claudia marcos|Class Accessibility,Modularity,Over-exposed Classes,Java Systems|Class access modifiers regulate interactions between software components. Such modifiers specify which classes from a component are publicly exposed and therefore belong to the component public interface. Restricting the accessibility specified by a programmer is key to ensure a proper software modularity. Failing to accurately do so is likely to produce architecture decay, maintenance problems and poor system quality. Unfortunately, how developers uses class access modifiers has not been investigated yet in the literature. In this work, we empirically analyze the use of class access modifiers across a basket of 15 Java libraries and 15 applications, totaling over 3.6M lines of code. We have found that an average of 25% of classes are over-exposed, therefore conspiring against a proper system modularity. A number of code patterns involving over-exposed classes have been formalized, characterizing programmers’ habits. We propose an Eclipse plugin to make component public interfaces match with the programmer’s intent.|None|reject|reject
FSE|2015|An Empirical Study on Topic Defect-Proneness and Testedness|tse-hsun chen,stephen w. thomas,hadi hemmati,meiyappan nagappan,ahmed e. hassan|software quality,topic model,bug prediction,software testing|Previous research in defect prediction has focused mainly on determining which files require additional testing resources, by examining quality metrics of the source files (e.g., number of previous defects, LOC, etc.). However, these studies usually do not consider the amount of testing that has already been done in the files. Another disadvantage of the current prediction approaches, from the testing perspective, is that the file-level information is very coarse-grained. A tester would like to know more details of what features of the system, exactly, require more testing. In an attempt to address these issues, in this paper, we use a text mining approach, called topic modeling, to generate topics (i.e., co-occurring words) in source and test files. Such topics can be seen as a lexical approximate of features that the system is implementing. The high-level idea is to determine topics of source code files that are defect-prone but are not well-tested and thus require more testing resources. We propose defect-proneness and testedness metrics per topic and conduct empirical case studies on the multiple versions of Mylyn, Eclipse, and NetBeans. We find that (i) between 34% and 78% of topics are shared between source and test files, indicating that topics can be used as an indicator of how testing resources are allocated; (ii) less-tested topics are usually defect-prone, so allocating more testing resources to such topics may help improve software quality; (iii) we can, with relatively high precision and recall, predict which topics are less-tested and defect-prone; (iv) we can also complement traditional prediction-based approaches in terms of saving testing and code inspection effort, by identifying defect-prone files that are much smaller than the ones identified by traditional defect-prediction models.|None|reject|reject
FSE|2015|Are Vulnerabilities Discovered and Resolved like Other Defects During Software Development?|patrick morrison,rahul pandita,xusheng xiao,ram chillarege,laurie a. williams|Software development,Measurement,Process improvement,Security,Orthogonal Defect Classification (ODC)|Software defect data has long been used to drive software development process improvement. If security defects (vulnerabilities) are discovered and resolved by different software development practices than other defects, that knowledge could be applied to process improvement. The goal of this research is to support technical leaders in making security-specific software development process improvements by analyzing whether defects and vulnerabilities differ in how they are discovered and resolved. We apply Orthogonal Defect Classification (ODC), a scheme for classifying software defects to support software process improvement, extending ODC to capture vulnerability-specific data. We applied our extended ODC scheme to 821 vulnerabilities and defects across 121 releases of two open-source projects (Firefox and phpMyAdmin). In our study’s data, vulnerability discovery differs statistically from defect discovery across all attributes in both projects. In Firefox, test variation is responsible for 50% more vulnerability discoveries than defect discoveries, while adding or correcting conditional logic is the most common means for resolving vulnerabilities. These characteristics can be used to guide design, implementation and testing for security purposes.|None|reject|reject
FSE|2015|Matching Complex Requirements for Software Services using Fuzzy Logic|marie christin platenius,robin senge,eyke hullermeier,wilhelm schafer|Service Matching,Requirements,Fuzzy Logic,Possibility Theory|Service markets provide software components in the form of services. In order to support a process of service discovery that optimally satisfies service requesters and service providers, techniques for automatic service matching are needed. However, a requester's requirements may be vague and the information available about a provided service may be incomplete. As a consequence, fuzziness is induced into the matching result. The contribution of this paper is a formal distinction between different sources and types of fuzziness in the context of service matching and, moreover, the development of a systematic matching procedure that leverages concepts and techniques from fuzzy logic and possibility theory. We demonstrate our approach on the example of specifications for service reputation based on ratings given by previous users. In contrast to existing methods, our approach is able to deal with imprecision and incompletion in complex service specifications and to inform about the extent of induced fuzziness. As a benefit, our approach enables service matching under realistic circumstances with useful feedback.|None|reject|reject
FSE|2015|Performance-Influence Models for Highly Configurable Systems|norbert siegmund,alexander grebhahn,sven apel,christian kastner|configurable systems,performance,prediction,learning,sampling|Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configurations options on performance is unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Such a model shall be useful for automatic performance prediction and optimization, on the one hand, and performance debugging for developers, on the other hand. Our approach combines machine-learning and sampling techniques in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them. Using our approach, we were able to identify a number of real performance bugs and other problems in real-world systems.|None|accept|pre-accept
FSE|2015|Broken Windows: The Effect of Code Quality on Code Quality|diomidis spinellis,panos louridas,maria kechagia|code quality,descriptive vs injunctive norms,broken windows,repository mining|Does the quality of existing code affect the quality of subsequent changes? According to the broken windows theory, disorder sets descriptive norms and signals behavior that further increases it. From a large code corpus, we examine whether code history does indeed affect the evolution of code quality. Then we examine some initial quality metrics in specific files (identifier length, comment density, use of goto, style consistency, etc.), and see whether subsequent commits by developers continue on that path.  Finally, we check whether developers tailor the quality of their commits based on the quality of the file they commit to.  Our results show that history matters, that developers behave differently depending on some aspects of the code quality they encounter, and that programming style inconsistency is not necessarily related to structural qualities. These findings have implications for both software practice and research.|None|reject|reject
FSE|2015|Automatically Detecting Integrity Violations In Database-Centric Applications|boyang li,denys poshyvanyk,mark grechanik|Data-centric applications,Semantic bugs,Fault localization|Database-centric applications (DCAs) are widely used by many companies and organizations to perform various control and analytical tasks using large databases. Real-world databases are described by complex schemas that oftentimes contain thousands of tables consisting of tens of thousands of attributes. However, when software engineers develop DCAs, they may write code that can inadvertently violate the integrity of these databases. Alternatively, business analysts and database administrators can also make errors that lead to integrity violations (semantic bugs). To detect these violations, stakeholders must create assertions that check the validity of the data in the rows of the database tables. Unfortunately, creating assertions is a manual, laborious and error-prone task. Thus, a fundamental problem of testing DCAs is how to find such semantic bugs automatically. We propose a novel solution, namely DACITE, that enables stakeholders to automatically obtain constraints that semantically relate database attributes and code statements using a combination of static analysis of the source code and associative rule mining of the databases. We rely on SAT-solvers to validate if a solution to the combined constraints exists and issue warnings on possible semantic bugs to stakeholders. We evaluated our approach on 7 open-source DCAs and our results suggest that semantic bugs can be found automatically with high precision. The results of the study with developers show that warnings produced by DACITE are useful and enable them to find semantic bugs faster.|None|reject|reject
FSE|2015|Are Inconsistent Names Correlated with Bugs?|lisa hua,na meng,miryung kim,kathryn mckinley|naming inconsistency,defect correlation,empirical study,purity analysis,type use pattern|Identifiers and their semantics define a naming dialect that guides program comprehension and communication. This paper examines: (1) if programs share a consistent naming dialect or have project-specific dialects and (2) if inconsistent identifiers indicate poor code quality. We build a tool called Niche to infer naming dialects and find inconsistent names using two approaches. (1) Niche extracts terms commonly used in impure and pure method names. If an impure method’s name contains a pure term or vice versa, Niche classifies the name as inconsistent. (2) Niche extracts type-use patterns by clustering statements with similar uses of a type. If an object is named very differently from other objects used in the same type-use pattern, we classify the name as inconsistent. We mine naming rules from 39 Java projects with more than 969,000 identifiers from 82,000 revisions and correlate names with bug fixes. Our analysis yields three insights. First, each project shares at most one third of its naming rules with another project, and about one third of its naming rules conflict with other projects. Second, methods with inconsistent names and methods that invoke them both experience more bug fixes than other methods. Third, inconsistent names have shorter lifespans than other names. Overall, each project uses its own unique naming dialect and eliminating inconsistent names quickly with automated name analysis should help improve code quality.|None|reject|reject
FSE|2015|Multi-versions data and Epsilon-serializability for QoS Enhancement in Distributed RTDBMS|malek ben salem,emna bouazizi,rafik bouaziz|DRTDBMS,QoS,epsilon-serializability criterion,multi-versions data|DRTDBMS are characterized by the large amount of distributed data, the unpredictable transactions and the unbalanced workload between different nodes. A lot of work dealing with Quality of Service (QoS) has been done to control the behaviour of DRTDBMS. They are based on feedback control scheduling theory. In this paper, we propose to apply both multi-versions data technique (MVD) and epsilon-serializability criterion (ESR) in distributed feedback control loop by using three data replication policies. The proposed architecture is called MVD-ESR-DFCS architecture. We also show, by a set of simulations, that our proposed architecture can significantly improve the QoS in DRTDBMS by increasing the number of transactions that meet their deadlines.|None|reject|reject
FSE|2015|Incrementally Precise Program Analysis|duc hiep chu,joxan jaffar,vijayaraghavan murali|Program Analysis,Abstract Interpretation,Symbolic Execution,Worst Case Execution Time,Taint Analysis|Program analysis has been dominated by Abstract Interpretation (AI), owing to its scalability. AI is typically not (or only partially) path-sensitive thus the obtained level of accuracy could be arbitrarily low. Recently, there have been works on path-sensitive program analysis, applied to domains where accuracy is critical. However, they suffer from the path explosion problem and are not scalable in general. In this paper, we present a general framework for program analysis that incrementally increases accuracy as it iterates. We start with a formulation of analysis which is both a lower-bound as well as upper-bound analysis of the program's behavior. This duality allows for a specification of precision in the overall analysis. We then define an abstract representation of the program which we can iteratively refine. The iterations allows for early termination when a user-definable level of precision in the analysis has been extracted. The critical performance factors are (a) the incrementality of the refinement step where results from previous iterations are persistent for future iterations, (b) reuse of analysis from subproblems that have precise analysis, and most importantly, (c) a concept of domination which allows a lower-bound analysis to prune away subproblems. Finally, we demonstrate the framework with real programs on a backward counting analysis and a forward data flow analysis. We show that in many cases, our iterative method is in fact superior to both AI as well as algorithms designed to run continuously till an exact analysis is found.|None|reject|reject
FSE|2015|Towards an Automatic Identification of Design Patterns using Graphs : A Case Study|najet zoubeir,adel khalfallah,samir ben ahmed|Software Micro-architecture Optimization,Design Patterns Decomposition,Automatic Identification of Design Patterns,Graph Transformation Rules|Graphs and graph grammars constitute a formally founded formalism that fulfills a compromise in software engineering between the accuracy and exactitude of the formal notations, and the usability and understandability of the semi-formal ones. Through this paper, we aim to evaluate the use of graphs in optimizing software micro-architecture, particularly in recognizing and injecting Design Patterns (DPs). We propose a case study in which the system structure and interactions are described using Graph Transformation Systems (GTSs), and the DPs are decomposed and formalized using Graph Transformation Rules (GTRs). Then we evaluate the automatic identification and injection of the DPs in this system.|None|reject|desk reject
FSE|2015|Error Location in Python: Where the Mutants Hide|joshua campbell,abram hindle,jose nelson amaral|naturalness,n-grams,error-location,python,mutation|Dynamic scripting programming languages present a unique challenge to software engineering tools that depend on static analysis.  Dynamic languages do not benefit from the full lexical and syntax analysis provided by compilers and static analysis tools.  Prior work exploited a statically typed language (Java) and a simple $n$-gram language model to find syntax-error locations in programs.  This work investigates whether $n$-gram-based error location on source code written in a dynamic language is effective without static analysis or compilation.  UnnaturalCode.py is a syntax-error locator developed for the Python programming language.  The UnnaturalCode.py approach is effective on Python code, but faces significantly more challenges than its Java counterpart did.  UnnaturalCode.py generalizes the success of previous statically-typed approaches to a dynamically-typed language.|None|reject|reject
FSE|2015|On how Model-Driven Development can simplify GPGPU Programming|thomas hogg,gunther fiedler,christian kohler,andreas kolb|Domain-specific languages,GPGPU,Modeling,Model-Driven Development,Computer Graphics and Vision|GPGPUs (General Purpose Computation on Graphics Processing Unit) have become the most important invention in the last years in computer graphics and the vision domain. Despite improvement of the two main programming platforms, CUDA (Compute Unified Device Architecture) and OpenCL (Open Computing Language), GPGPU programming and development is still a complex, time consuming and error-prone task. To overcome these problems for general software engineering, the graphical modeling language UML (Unified Modeling Language) was introduced and became the first choice for designing software systems. However, its generic design causes representation of algorithmic problem descriptions to be either limited or too complicated. We present a novel domain-specific language (DSL), including novel modeling concepts (new activity-diagram node types and special language constructs), based on Eclipse Xtext and GMF, adopting and extending class- and activity-diagrams in a textual and graphical form. Furthermore, we present a C++ and OpenCL code generation framework in combination with a heterogeneous C++ GPGPU computing framework allowing for a smooth connection with our DSL and graphical editors.|None|reject|reject
FSE|2015|Data-Driven Synthesis of Web-Crawlers|adi omari,sharon shoham,eran yahav|synthesis,data-extraction,web-crawling|A web-crawler is a program that automatically and systematically tracks the links of a website and extracts information from its pages. Due to the different formats of websites, the crawling scheme for different sites can differ dramatically. Manually customizing a crawler for each specific site is time consuming and error-prone. Furthermore, because sites periodically change their format and presentation, crawling schemes have to be manually updated and adjusted. In this paper, we present a technique for automatic synthesis of web-crawlers from examples. The main idea is to use hand-crafted (possibly partial) crawlers for some websites as the basis for crawling other sites that contain the same kind of information. Technically, we use the data on one site to identify data on another site. We then use the identified data to learn the website structure and synthesize an appropriate extraction scheme. We iterate this process, as synthesized extraction schemes result in additional data to be used for re-learning the website structure. We have implemented our approach and demonstrate its value by automatically synthesizing $22$ crawlers for websites from seven different categories: books, TVs, conferences, universities, cameras, phones and movies.|None|reject|reject
FSE|2015|Nondeterministic Refinement + Tradespace Analysis: System Development from Incomplete Specifications|chong tang,kevin j. sullivan,hamid bagheri|software synthesis,tradespace analysis,non-functional properties,relational logic,experimental software engineering|Incompleteness of software specifications with respect to the totality of relevant systems properties is often unavoidable. Yet, many widely used development tools and methods tacitly assume the opposite. Assuming completeness is attractive because it admits a sharp separation between systems engineering (specification) and software engineering (implementation) roles, and greatly simplifies the process of producing implementations. Given a valid, complete specification, the set of satisfying implementations is an {\em equivalence class} with respect to all properties of value, so any implementation will do. The implementation process reduces to a  refinement {\em function}, and development can proceed without further inputs from the systems engineer.  By contrast, when a specification is incomplete, a more complex process is needed. The first phase involves a non-deterministic refinement {\em relation}, yielding a space of non-equivalent implementation candidates. The second phase involves  {\em optimization} and {\em tradeoffs} with respect to the properties not constrained by the specification. The phase separation is broken because preference information not in the specification is required from the systems engineer to carry out the tradeoff and optimization phase. Functions for measuring or estimating properties based on representations of system implementations are needed; and decisions made at this stage can have systemic effects. This paper addresses the need for concepts, methods, and tools for dealing with incomplete specifications and for enacting the resulting, {\em coupled} implementation processes. The contributions of this paper are (1) the preceding perspective and problem analysis; (2) a formal,  general model of one class of processes involving specification-driven synthesis to compute refinement relations followed by a map-reduce dynamic tradeoff analysis and end-user selection; (3) an object-oriented implementation of this framework in Scala; (4) a specialization of the  framework to automate non-deterministic refinement and performance trade-space analysis for object-relational mappings (ORM), in particular; (5)  data from experiments using this tool that show that even for simple object models, a synthesis+tradespace process tends to significantly outperform widely used ORM tools---namely ones often used with Ruby and Python--which instead implement simple refinement {\em functions} from object-oriented data model specifications.|None|reject|reject
FSE|2015|On the Use of Delta Debugging to Reduce Recordings and Facilitate Debugging of Web Applications|mouna hammoudi,brian burg,gigon bae,gregg rothermel|Delta Debugging,Web Applications,Record/Replay Techniques,Recording Reduction|Recording the sequence of events that lead to a failure of a web application can be an effective aid for debugging.  Users can send a recording that results in a failure to a web application's developer.  The developer can then replay the recording, reproduce the failure, and find the fault(s) that cause it.  Developers can do the same thing when faced with faults encountered in web applications in-house.  Nevertheless, a recording of an event sequence may include many events that are not related to a failure, and this may render debugging tasks more difficult.  To address this problem, we have adapted Delta Debugging to function on recordings of web applications, in a manner that lets it identify and discard portions of those recordings that do not influence the occurrence of a failure.  The resulting recording reduction technique enables developers to locate and correct faults based on reduced recordings instead of initial unreduced recordings, potentially reducing the amount of time and effort required to locate faults.  We present the results of three empirical studies that show that (1)~recording reduction can achieve significant reductions (85\% on average) in recording size and replay time (70\% on average) on actual web applications obtained from developer forums, (2)~reduced recordings do in fact help programmers locate faults significantly more efficiently and no less effectively than non-reduced recordings, and (3)~recording reduction produces even larger reductions (85.8\% in size, 77.8\% in replay time on average) on larger, more complex applications.|None|accept|accept
FSE|2015|Debugging for Reactive Programming|guido salvaneschi,mira mezini|Reactive Programming,Debugging,Tool Support|Reactive programming is a recent programming technique that provides dedicated language abstractions for reactive software. Reacive programming relives developers from manually updating outputs when the inputs of a computation change, it overcomes a number of well-known issues of the Observer design pattern, and it makes program more comprehensible. Unfortunately, complementing the new paradigm with proper tools that support coding activities is a vastly unexplored area. Hence, as of now, developers can embrace reactive programming only at the cost of a more challenging development process. In this paper, we investigate a primary issue in the field: debugging programs in the reactive style. We analyze the problem of debugging reactive programs, show that the reactive style requires a paradigm shift in the concepts needed for debugging, and propose RP Debugging, a methodology for effectively debugging reactive programs. These ideas are implemented in RPeclipse, a debugger for reactive programs integrated with the Eclipse Scala development environment.|None|reject|pre-reject
FSE|2015|JSBoost: Automatically Fixing Recurring JavaScript Performance Bugs|marija selakovic,michael pradel|Performance bugs,Empirical study,Automatic bug fixing,JavaScript|Many programs suffer from performance problems, but unfortunately, finding and fixing such problems is a cumbersome and time-consuming process. This paper focuses on JavaScript, for which little is known about performance bugs and how developers address them, and for which there are few practical techniques to address performance bugs. To address the lack of knowledge about real-world JavaScript performance bugs, we present an empirical study of 52 such bugs. Our results show that many bugs are specific to JavaScript and to built-in JavaScript APIs, and that many bugs are instances of recurring bug patterns. To help developers find and fix such bugs, we present JSBoost, an automatic bug fixing technique for JavaScript performance problems. The key idea is to specify a fix pattern once and for all in a JavaScript-like language, to automatically apply the pattern to a program, and to suggest a performance fix to the developer if and only if the change leads to a significant speedup. We evaluate JSBoost with 6 fix patterns identified in the empirical study and show that it effectively fixes 7 bugs from the study and that it suggests 18 additional fixes, all of which increase the performance of the analyzed program.|None|reject|reject
FSE|2015|SimPerf: An Automated Testing Framework to Identify Synchronization Bottlenecks|tingting yu|software testing,performance testing,bottleneck identification|Writing concurrent programs is a challenging task, because it requires consideration of both functional and performance correctness. In software engineering, numerous program analysis and testing techniques have been proposed to detect functional faults due to incorrect enforcement of synchronizations. Little work, however, has addressed the problem of performance correctness, in which inefficient use of synchronizations can limit the application performance. In this paper, we present SimPerf, a novel testing- based framework that allows engineers to effectively identify synchronization bottlenecks by running existing tests. SimPerf first generates performance test oracles by increasing the likelihood of thread contention. It then employs dynamic analysis to measure performance impact of synchronization code regions on the overall application performance. The measurements can help engineers pinpoint root cause of performance slowdown and identify the relevant code regions that would be most beneficial for optimization. In addition, SimPerf can further provides optimization strategies based on the identified bottlenecks. We implemented SimPerf on a commercial virtual platform that is widely used to support hardware/software co-design. We then evaluated its effectiveness on eleven real-world applications. Our results show that SimPerf was effective at finding both known and unknown synchronization bottlenecks, and led to optimizations that can substantially improve the overall performance of the applications.|None|reject|pre-reject
FSE|2015|Optimizing Energy Consumption of GUIs in Android Apps: A Multi-objective Approach|mario linares vasquez,gabriele bavota,carlos eduardo bernal cardenas,rocco oliveto,massimiliano di penta,denys poshyvanyk|Energy consumption,Mobile applications,Empirical Study|The wide diffusion of mobile devices has motivated research towards optimizing energy consumption of software systems— including apps—targeting such devices. Besides efforts aimed at dealing with various kinds of energy bugs, the adoption of Organic Light-Emitting Diode (OLED) screens has motivated research towards reducing energy consumption by choosing an appropriate color palette. Whilst past research in this area aimed at optimizing energy while keeping an acceptable level of contrast, this paper proposes an approach, named GEMMA (Gui Energy Multi-objective optiMization for Android apps), for generating color palettes using a multi- objective optimization technique, which produces color so- lutions optimizing energy consumption and contrast while using consistent colors with respect to the original color palette. An empirical evaluation that we performed on 25 Android apps demonstrates not only significant improvements in terms of the three different objectives, but also confirmed that in most cases users still perceived the choices of colors as attractive. Finally, for several apps we interviewed the original developers, who in some cases expressed the intent to adopt the proposed choice of color palette, whereas in other cases pointed out directions for future improvement|None|accept|pre-accept
FSE|2015|Measure it? Manage it? Ignore it? Software Practitioners and Technical Debt|neil ernst,stephany bellomo,ipek ozkaya,robert nord,ian gorton|Technical debt,architecture,survey|The technical debt metaphor is widely used to encapsulate numerous software quality problems. The metaphor is attractive to practitioners as it communicates to both technical and non-technical audiences that if quality problems are not addressed, things may get worse. However, it is unclear whether there are practices that move this metaphor beyond a mere communication mechanism. Existing studies of technical debt have largely focused on code metrics and small surveys of developers. In this paper, we report on our survey of 1831 participants, primarily software engineers and architects working in long-lived, software-intensive projects from three large organizations, and follow-up interviews of seven software engineers. We analyzed our data using both non-parametric statistics and qualitative text analysis. We found that architectural decisions are the most important source of technical debt. Furthermore, while respondents believe the metaphor is itself important for communication, existing tools are not currently helpful in managing the details. We use our results to motivate a technical debt timeline to focus management and tooling approaches.|None|accept|pre-accept
FSE|2015|Delta Modeling With Consistency-Preserving Edit Scripts|christopher pietsch,timo kehrer,udo kelter,dennis reuling|Software product line engineering,Model-based development,Delta modeling,Model differencing|Model-based development has become a widely-used approach to implement software, e.g. for embedded systems. Models replace source code as primary executable artifacts in these cases. Software product line technologies for these domains must be able to generate models as instances of an SPL. This need is addressed by an implementation technology for SPLs known as delta modeling. Current approaches to delta modeling require deltas to be written manually using delta languages, and they offer only very limited support for creating and testing a network of deltas. This paper presents a new approach to delta modeling: the abstract notion of a delta is refined to be a consistency-preserving edit script, which is basically a data flow graph of edit steps. Each edit step transforms a model into a new state which can be displayed graphically. An edit script is generated by comparing two models, one with and one without a feature to be implemented. The rich structure of edit scripts allows us to detect dependencies, conflicts and further inferences between deltas statically and to implement restructurings of deltas such as the merger or the intersection of two deltas. All functions are fully generic and can be adapted to new modeling languages with very limited effort.|None|reject|reject
FSE|2015|Not all bugs are same|harold valdivia-garcia,meiyappan nagappan|Empirical Study,Software Bugs,Dependent Variables|There have been decades of research on prioritizing software entities to help practitioners allocate their quality assurance resources using prediction models. However, in most of the past work the dependent variables of the prediction models have been simply the number of bugs in a file. Such a dependent variable assumes that all bugs are the same. In this paper, we first examine if all bugs are the same along three different characteristics of bugs (size of bug-fix, experience of developers fixing the bug, and cost of a bug-fix expressed as the product of the size and experience). Then we examine the impact of using one of these three characteristics as a dependent variable in the prediction model. We measure the impact on prioritization of entities, and the impact on the relationship between previously studied independent variables (such as LOC, Cyclomatic Complexity and Churn) and the dependent variables, when we choose to use the new dependent variables in the model. We find that not all bugs are same in all three characteristics, and that the prioritization of files is considerably different when we use different dependent variables. Hence, we conclude that research needs to be conducted into determining the dependent variables that practitioners most care about, and come up with approaches to capture and mine this information.|None|reject|reject
FSE|2015|BinSim: Obfuscation-Resilient Binary Code Similarity Comparison Using System Call Sliced Segment Equivalence Checking|jiang ming,dongpeng xu,li wang,yufei jiang,dinghao wu|Reverse Engineering,Obfuscation Resilience,Binary Code Similarity,Binary Diffing,System Call Sliced Segment Equivalence Checking,Dynamic Slicing,Symbolic Execution,Weakest Precondition|The efforts to detect differences between two binary executables (binary diffing), first derived from patch analysis, have been widely employed in various reverse engineering tasks, such as ``1-day'' exploit generation, software plagiarism detection and malware analysis. Especially when analyzing malware variants, pervasive code obfuscation techniques have driven recent work towards determining semantic similarity instead of ostensible similarity in syntax. One direction is to measure two programs' dynamic behavior similarity by evaluating system calls or APIs they invoked. Another way is to convert portions of code (e.g., basic blocks) into logical formulas by symbolic execution, and then use constraint solver to verify the equivalence of formulas. Unfortunately, neither approach delivers the expected precision when comparing highly obfuscated programs. In this paper, we propose system call sliced segment equivalence checking, a hybrid method to identify fine-grained semantic similarities between two programs' execution traces. We combine dynamic slicing with symbolic execution to compare the logic of instructions that impact on the observable behaviors. Our approach improves existing semantics-aware binary diffing by inferring whether two programs' behaviors are conditionally equivalent and detecting the similarities or differences whose effects spread across multiple basic blocks. We have implemented a prototype, called BinSim, on top of the BitBlaze binary analysis platform and performed empirical evaluations against a wide variety of obfuscation schemes and malware samples. Our experimental results show that BinSim can successfully identify semantic similarities/differences between obfuscated programs, and outperform other binary diffing tools in terms of better resilience and accuracy.|None|reject|pre-reject
FSE|2015|Discovering Likely Mappings between APIs using Text Mining|rahul pandita,sithu sudarsan,raoul jetley,laurie a. williams|API Documents,Method Mapping,text mining|Developers often migrate applications to release different versions for supporting application programming interfaces (APIs) of various platforms/programming-languages. To migrate an application written using one API (source) to another API (target), a developer needs to know how the methods in the source API map to the methods in the target API. Given a typical platform or language exposes a large number of API methods for developers to reuse, manually writing these mappings is prohibitively resource-intensive and may be error prone. Recently, researchers proposed to automate the process by mining API mappings from existing code-bases. However, these approaches require as input a manually ported (or at least functionally similar) code across source and target API’s. To address the shortcoming, this paper proposes TMAP: Text Mining based approach to discover likely API mappings using the similarity in the textual description of the source and target API documents. To evaluate our approach, we apply TMAP to discover API mappings for 15 classes across: 1) Java and C# API, 2) Java ME and Android API.We next compare the discovered mappings with state-of-the-art source code analysis based approaches: Rosetta and StaMiner. Our results indicate that TMAP on an average found relevant mappings for 57% more methods compared to previous approaches. Furthermore, our results also indicate that TMAP found on average exact mappings for 6.5 more methods per class with a maximum of 21 additional exact mappings for a single class as compared to previous approaches.|None|reject|reject
FSE|2015|Fast and Precise Statistical Code Completion|pascal roos,veselin raychev,martin vechev|code completion,language models,IDE,Eclipse plug-in,big code,statistical models,probabilistic models|This paper presents an end-to-end system for precise and real-time API code completion based on the N-gram statistical language model. This language model is learned from "Big Code" -- massive codebases found in online repositories such as GitHub. We describe: (i) a novel, fast API code completion algorithm based on beam search, (ii) an efficient implementation of the N-gram language model and various smoothing methods, (iii) the first experimental evaluation comparing different smoothing techniques, beam size, N-gram parameters, as well as completion speed and precision, and (iv) an Eclipse plug-in implementation capable of completing multiple method invocations at a time. We show that our statistical completion system is fast, precise and ready for practical use: on 84 realistic Android programs, the desired solution is found in milliseconds and is in the top 3 suggestions for over 89% of programs. Additionally, we find that developers using our system are overall faster in completing tasks with our plug-in when compared to other solutions.|None|reject|reject
FSE|2015|Finding Resume and Restart Errors in Android Applications|zhiyong shan,tanzirul azim,iulian neamtiu|Android analysis,Static analysis,Data loss,Input generation|Smartphone apps create and handle a large variety of ``instance'' data such as workout results in a health&fitness app; alarm settings in an alarm clock app; or game state in a game app. Due to the nature of the smartphone platform, an app can be paused, sent into the background, or killed at any time. If the instance data is not saved, or not saved properly, in addition to the actual loss of data, partially-saved or corrupted data can crash the app upon resume or restart.  While smartphone platforms offer API support for data-saving and data-retrieving operations, the use of this API is ad-hoc: completely left to the programmer, rather than enforced by the compiler.  Our main observation is that several categories of bugs---including data loss, failure to resume/restart or resuming/restarting in the wrong state---are due to the same underlying problem: incorrect handling of instance data.  To help address this problem, we have focused on the Android platform and constructed a tool chain---static analysis and input generator---that helps find and reproduce such incorrect handling, thereby benefiting Android end-users, developers, and researchers. We evaluate our approach on 341 apps chosen from various categories, sizes, and popularity.  Results indicate that our approach is (i) effective, as it has discovered dozens of bugs, including in popular Android apps, and (ii) efficient, completing on average in 95 seconds per app.|None|reject|reject
FSE|2015|Bridging the Gap between User Experience and Privacy Enforcement|abdulbaki aydin,omer tripp,pietro ferrara,marco pistoia|usable security,privacy analysis,mobile,information-flow security|Mobile applications often require access to private user information, such as the user or device ID, the location and the contact list. Usage of such fields varies across different applications. A notable example is advertising. Some applications feature contextual advertising by releasing granular user information, such as the user's precise location and date of birth, whereas other applications release more coarse information, including the user's age and country. Another dimension is the user. Some users are more demanding for privacy than others. Existing solutions for privacy enforcement are neither app- nor user-sensitive. Instead, they perform general tracking of private data into release points, such as the Internet. The main contribution of this paper consists of refining privacy enforcement, in such a way that the user can configure privacy preferences atop a visual capture of the application's screens that is enriched with privacy-relevant information. We focus on (contextual) advertising, which is the de facto monetization model of Android, and outline how our approach can be extended to other clients of private information, such as analytics engines. We have implemented our approach for Android as the SafeAd system for privacy enforcement. SafeAd first runs in off-line mode to collect screen captures and track flows of private data into ad widgets. It then combines these two artifacts to enable user-sensitive visual privacy configuration. Finally, SafeAd instruments the app in order to guarantee that mock---rather than actual---data is released where appropriate.  We have applied SafeAd to 126 free top-popular applications from Google Play, 25 of which were found to release private data for contextual advertising. Experiments over these apps indicate that (i) the SafeAd configuration is lightweight with an average of 7 review actions per app; (ii) the average SafeAd overhead is <5%; and (iii) the code rewriting to enforce the configuration is free of side effects.|None|reject|pre-reject
FSE|2015|Continuous Deployment at Facebook and OANDA|tony savor,mitchell douglas,michael gentili,laurie a. williams,kent beck,michael stumm|Software Deployment,Software Management,Software Process,Software Installation,Software Productivity,Software Quality|Continuous deployment is the software engineering practice of deploying many small incremental software updates into production, leading to a continuous stream of 10s, 100s, or even 1,000s of deployments per day. High-profile Internet firms such as Amazon, Etsy, Facebook, Flickr, Google, and Netflix have embraced continuous deployment. However, little academic study has analyzed whether continuous deployment is a sound practice. In this paper, we present both quantitative and qualitative analyses of the continuous deployment practices at Facebook and OANDA. We show that continuous deployment need not inhibit productivity or quality even in the face of substantial growth of the engineering team and the code size. The evidence suggests that top-level management support of continuous deployment is critical, and that given a choice, developers prefer faster deployment. We identify key elements and key experiences that emerged as central to the viability of continuous deployment. To the best of our knowledge, this is the first study to show it is possible to substantially scale the size of an engineering team and the size of the code base without negatively impacting developer productivity or software quality.|None|reject|reject
FSE|2015|Suggesting Accurate Method and Class Names|miltiadis allamanis,earl t. barr,christian bird,charles sutton|naturalness of software,coding conventions,method naming|Descriptive names are vital part of readable, and hence maintainable, code. Recent progress on automatically suggesting names for local variables tantalizes with the prospect of replicating that success with method and class names.  However, suggesting names for methods and classes is much more difficult. This is because good method and class names need to be functionally descriptive, but suggesting such names requires that the model goes beyond local context. We introduce a neural probabilistic language model for source code that is specifically designed for the method naming problem. Our model learns which names are semantically similar by assigning them to locations, called embeddings, in a high-dimensional continuous space, in such a way that names with similar embeddings tend to be used in similar contexts. These embeddings seem to contain semantic information about tokens, even though they are learned only from statistical co-occurrences of tokens.  Furthermore, we introduce a variant of our model that is, to our knowledge, the first that can propose neologisms, names that have not appeared in the training corpus. We obtain state of the art results on the method, class, and even the simpler variable naming tasks. More broadly, the continuous embeddings that are learned by our model have the potential for wide application within software engineering.|None|accept|accept
FSE|2015|Revisiting the Truisms of Software Engineering: Does Phase Delay Dramatically Increase Fix Time?|tim menzies,william nichols,lucas layman,forrest shull,carter pape|software processes,economics,phase delay,time to fix.|Does repair time increase dramatically the longer an issue persists in a system? Many industrial practitioners and academics this to be true. This belief is used to justify major investments in software development processes and methods. This paper reports on the largest study on phase delay yet published. Based on a sample of 171 software projects from around the world (2006 and 2014), we find that this belief is incorrect. Specifically: the time to resolve an issue does *not* increase the longer the issue remains in the system. This result raises the question: how do widely-held beliefs like phase delay survive so long without critical reassessment? As a community, we need to consider how (and when) we reflect on our prominent hypotheses.|None|reject|reject
FSE|2015|How Developers Optimize Android Apps|mario linares vasquez,christopher vendome,michele tufano,denys poshyvanyk|Optimizations,Mining Software Repositories,Empirical,Android|Optimizing mobile apps early on in the development cycle is supposed to be a key strategy for obtaining higher user rankings, more downloads (and revenue), and higher retention. In fact, mobile platform designers publish specific guidelines, practices, and tools aimed at optimizing apps. However, little research has been done into identifying and understanding actual optimization practices performed by developers. In this paper, we present the results of three empirical studies aimed at investigating practices of Android developers towards improving performance of their apps. We mined the history of 3,513 Android apps to identify most frequent optimization warnings in 297K+ snapshots and to understand if (and when) developers take advantage of these optimization opportunities. Then, we performed an in-depth analysis into whether implementing suggested optimizations can help reduce memory and CPU usage in a subset of Android apps. Finally, we conducted a survey with 425 open-source developers to understand how they optimize Android apps. Surprisingly, our results indicate that al- though open source Android apps have a great number of micro-optimization opportunities throughout their change history, developers rarely take advantage of them. The results from the survey shed light into why this happens as well as which practices Android developers rely upon.|None|reject|reject
FSE|2015|Using N-gram Language Model for Software Bug Detection|devin chollak,song wang,lin tan|Bug Detection,Static Analysis,N-gram,Language Models,Programming Rules|To improve software reliability, many techniques have been proposed to infer programming rules, and to detect violations of these programming rules as software defects. In this paper, we leverage the n-gram model to infer programming rules for software defect detection, which is a new domain for the application of n-gram models. In addition, we combine the n-gram model with two additional techniques to address limitations of existing defect detection techniques. First, we infer combined programming rules—combination of infrequent programming rules with their relevant infrequent or frequent programming rules—to detect new bugs. Second, we integrate control flow into the n-gram models which improves the accuracy of defect detection. We evaluate our approach on 14 open source projects ranging from 36 thousand lines of code (KLOC) to 1 million lines of code (MLOC). Our approach detected 310 violations in the latest version of the projects, 108 of which are useful violations, i.e., 43 bugs and 65 refactoring opportunities. We reported 19 of the 43 bugs to the developers and we are in the process of reporting the rest. Among the reported bugs, 2 have been confirmed by the developers, while the rest await confirmation. For the 108 useful violations, at least 26 cannot be detected by existing techniques.|None|reject|reject
FSE|2015|Using Software Changes to Understand Conformance to Test Driven Development|michael hilton,nicholas nelson,hugh mcdonald,sean mcdonald,ronald metoyer,danny dig|program comprehension and visualization,Agile Development,Visualization,Test-Driven Development,Process Conformance,Human aspects of software engineering,User Study,Cognitive Dimensions,Empirical Studies and metrics|A bad software development process leads to wasted effort and inferior products. In order to improve a software process, it is important to first understand it. Our unique approach in this paper is to use code and test changes to understand conformance to a process. We use Test Driven Development (TDD) as a case study to validate our approach. We designed and implemented TDDViz, a tool that enables developers to better understand how they conform to TDD. TDDViz enables this understanding by providing novel visualizations. We analyze these visualizations using the Cognitive Dimensions framework to discuss some findings and design adjustments. To enable TDDViz’s visualizations, we developed a novel automatic inferencer that identifies the phases that make up the TDD process solely based on code and test changes. We evaluate TDDViz using two complementary methods: a controlled experiment with 35 participants, and a case study with 2601 TDD Sessions. The controlled experiment shows that, in comparison to existing visualizations, participants performed significantly better when using TDDViz to answer questions about code. In addition, the case study shows that TDDViz infers TDD phases with an accuracy of 87%.|None|reject|reject
FSE|2015|Quality and Productivity Outcomes Relating to Continuous Integration in GitHub|bogdan vasilescu,yu yue,huaimin wang,premkumar t. devanbu,vladimir filkov|Continuous Integration,Productivity,Quality,Empirical study,GitHub|Software processes comprise many steps; coding is followed by building, integration testing, system testing, deployment, operations, inter alia. Software process integration and automation have been areas of key concern in software engineering, ever since the pioneering work of Osterweil; market pressures for Agility, and open, decentralized, software development have provided additional pressures for progress in this area. But do these innovations actually help projects? Given the numerous confounding factors that can influence project performance, it can be a challenge to discern the effects of process integration and automation. Software project ecosystems such as GitHub provide a new opportunity in this regard: one can readily find large numbers of projects in various stages of process integration and automation, and gather data on various influencing factors as well as productivity and quality outcomes. In this paper, we use large, historical data on process metrics and outcomes in GitHub projects, to discern the effects of one specific innovation in process automation: continuous integration. Our main finding is that continuous integration improves the productivity of project teams, who can integrate more outside contributions, without an observable diminishment in code quality.|None|accept|accept
FSE|2015|A User-Guided Approach to Program Analysis|ravi mangal,xin zhang,aditya v. nori,mayur naik|user feedback,program analysis,report classification|Program analysis tools often produce undesirable output due to various approximations. We present an approach and a system EUGENE that allows user feedback to guide such approximations towards producing the desired output. We formulate the problem of user-guided program analysis in terms of solving a combination of hard rules and soft rules: hard rules capture soundness while soft rules capture degrees of approximations and preferences of users. Our technique solves the rules using an off-the-shelf solver in a manner that is sound (satisfies all hard rules), optimal (maximally satisfies soft rules), and scales to real-world analyses and programs. We evaluate EUGENE on two different analyses with labeled output on a suite of seven Java programs of size 131–198 KLOC. We also report upon a user study involving nine users who employ EUGENE to guide an information-flow analysis on three Java micro-benchmarks. In our experiments, EUGENE significantly reduces misclassified reports upon providing limited amounts of feedback.|None|accept|accept
FSE|2015|Automated Classification and Program Repair for Introductory Programming Assignments|sumit gulwani,ivan radicek,florian zuleger|introductory programming education,feedback,program repair,classification|Providing feedback on programming assignments is a tedious task for the instructor. In this paper, we present a novel technique for automatic feedback generation: (1) For a given programming assignment, we automatically classify or categorize the correct student attempts based on their underlying high-level strategy. From each class we select one student attempt as a specification for the given strategy. (2) Given an incorrect student attempt we run a repair procedure against all specifications, and automatically generate the minimal repair based on one of them. We evaluated the approach on 18,767 student attempts over 15 problems  from an undergraduate programming course. In our evaluation we first automatically generated specifications from the correct student attempts and then used these specifications to repair the incorrect student attempts without any human intervention, finding repairs for 76% of student attempts in around 8 seconds per attempt.|None|reject|pre-reject
FSE|2015|Hey, You Have Given Me Too Many Knobs! ---Understanding Over-Designed Configuration in System Software|tianyin xu,long jin,xuepeng fan,yuanyuan zhou,shankar pasupathy,rukma talwadker|configuration design,system software,configuration settings characteristics,configuration simplification,configuration navigation|Configuration problems are not only prevalent, but also severely impair the reliability of today's system software. One fundamental reason is the ever-increasing complexity of configuration, reflected by the large number of configuration parameters ("knobs"). With hundreds of knobs, configuring system software to ensure high reliability and performance becomes a daunting, error-prone task. This paper makes a first step in understanding a fundamental question of configuration design: "do users really need so many knobs?" To provide the quantitatively answer, we study the configuration settings of real-world users, including thousands of customers of a commercial storage system (Storage-A), and hundreds of users of two widely-used open-source system software projects. Our study reveals a series of interesting findings to motivate software architects and developers to be more cautious and disciplined in configuration design. Motivated by these findings, we provide a few concrete, practical guidelines which can significantly reduce the configuration space. Take Storage-A as an example, the guidelines can remove 51.9% of its parameters and simplify 19.7% of the remaining ones with little impact on existing users. Also, we study the existing configuration navigation methods in the context of "too many knobs" to understand their effectiveness and to provide practices for building navigation support in system software.|None|accept|pre-accept
FSE|2015|Proactive Self-Adaptation under Uncertainty: a Probabilistic Model Checking Approach|gabriel a. moreno,javier camara moreno,david garlan,bradley r. schmerl|self-adaptation,latency-aware adaptation,proactive adaptation,probabilistic model checking|Self-adaptive systems tend to be reactive and myopic, adapting in response to changes without anticipating what the subsequent adaptation needs will be. Adapting reactively can result in inefficiencies due to the system performing a suboptimal sequence of adaptations. Furthermore, when adaptations have latency, and take some time to produce their effect, they have to be started with sufficient lead time so that they complete by the time their effect is needed. Proactive latency-aware adaptation addresses these issues by making adaptation decisions with a look-ahead horizon and taking adaptation latency into account. In this paper we present an approach for proactive latency-aware adaptation under uncertainty that uses probabilistic model checking for adaptation decisions. The key idea is to use a formal model of the adaptive system in which the adaptation decision is left underspecified through nondeterminism, and have the model checker resolve the nondeterministic choices so that the accumulated utility over the horizon is maximized. The adaptation decision is optimal over the horizon, and takes into account the inherent uncertainty of the environment predictions needed for looking ahead. Our results show that the decision based on a look-ahead horizon, and the factoring of both tactic latency and environment uncertainty, considerably improve the effectiveness of adaptation decisions.|None|accept|accept
FSE|2015|Precondition-Guided Search Strategy in Concolic Testing|hyunmin seo,sunghun kim|Concolic testing,symbolic execution,search strategies|Concolic Testing, an automatic test generation technique based on symbolic execution has received much interest recently and yields reasonable code coverage. Concolic testing generates an input vector for the opposite branch of a branch in the previous execution paths. However, generating input to cover a specific branch in a program is still challenging due to the path explosion, especially when the target branch requires a complicated conditions to be satisfied by the input. Several heuristic-based search strategies have been introduced to deal with the path explosion but most strategies focused on improving coverage in general not covering a specific branch. In this paper, we introduce a precondition-guided search (PGS) strategy. Each time an execution of concolic testing fails to cover a target branch, we calculate the reasons not covering the target in the form of precondition and save the precondition at each branch along the path. Later, before we generate new input, we check if the current state implies the preconditions stored at the branch. If the implication holds, we do not generate input since the new input can not reach the target branch for the same reason. We evaluate PGS on selected subjects from the literature. The experimental results show that PGS can cover the target branches fast which are hard to be covered by other strategies.|None|reject|reject
FSE|2015|Developer Migration in the GitHub Ecosystem|casey casalnuovo,bogdan vasilescu,premkumar t. devanbu,vladimir filkov|GitHub Ecosystem,Developer Migration,Social Links|The team aspects of software engineering have been a subject of great interest since early work by Fred Brooks and others: how well do people work together in teams? why do people join teams? what happens if teams are distributed? Such questions have received a great deal of attention. Recently, the emergence of project ecosystems such as GitHub have created an entirely new, higher level of organization. GitHub supports numerous teams; they share a common technical platform (for work activities) and a common social platform (via following, commenting, etc). This gives rise to a range of new phenomena. People collaborate, talk, build reputations and skills, and make contributions to different projects within the ecosystem. Individuals benefit by the opportunity to make friends, build reputations, and learn, within a much larger community than a single project; teams benefit from being to able draw from a much larger pool of potential contributors. We are specifically interested in migration between projects, the causes, and effects: people are the life-blood of projects, and thus immigration is a critical factor in the success of a project. We find that migration in GitHub is strongly affected by pre-existing relationships; furthermore, we find that the presence of past social connections combined with prior experience in languages dominant in the project leads to higher productivity both initially and cumulatively. Interestingly, we also find stronger social connections are associated with slightly less productivity initially, but slightly more productivity in the long run.|None|accept|accept
FSE|2015|FreshDoc: Automatic API Update in Human-written Documentation by Analyzing Code Revision|seonah lee,rongxin wu,shing-chi cheung,sungwon kang|Human-written documentation,Outdated APIs,API updates,Software revision histories|API Implementations continually evolve to meet ever changing user needs. API documentation provides an authoritative and timely reference of their usages. However, the documentation is commonly outdated because its updates are tedious and mostly done manually. In this paper, we propose FreshDoc, an approach that detects outdated APIs in documentation and updates them automatically when API implementations have changed. The insight of FreshDoc is that the detection and update of outdated APIs in documentation can be driven by APIs’ code revisions in the code repository. To evaluate FreshDoc, we applied it to four open source projects. The evaluation result shows that FreshDoc can detect and update outdated APIs in documentation with 77% accuracy, outperforming the state-of-the-art technique, DocRef.|None|reject|reject
FSE|2015|Data-Driven Language Design: Syntactic Rule Usage in Java|dong qiu,bixin li,earl t. barr,zhendong su|Language Syntax,Empirical Study,Practical Language Usage|Syntax is fundamental to any programming language: syntax defines valid programs. In the 1970s, computer scientists rigorously and empirically studied programming languages to guide and inform lan- guage design. Since then, language design has been artistic, driven by aesthetics concerns and the intuitions of language architects. Despite recent studies on small sets of selected language features, we lack a comprehensive, quantitative, empirical analysis of how modern, real-world source code exercises the syntax of its program- ming language. This paper presents our results on the first such study on Java, a modern, mature, and widely-used programming language. Our corpus contains over 5,000 open-source Java projects, totalling 150 million source lines of code (SLoC). We study both independent ( i.e. applications of a single syntax rule) and dependent ( i.e. applications of multiple syntax rules) rule usage, and quantify their impact over time and project size. In particular, our study (1) confirms the conventional wisdom that the usage of syntax rules is Zipf; (2) shows that the adoption of new rules and their impact on the usage of pre-existing rules vary significantly over time; and (3) shows that rule usage is highly contextual. Our findings suggest potential applications across language design, code suggestion and completion, automatic syntactic sugaring, and language restriction.|None|reject|reject
FSE|2015|How Developers Search for Code: A Case Study|caitlin sadowski,kathryn stolee,sebastian g. elbaum|code search,empirical studies,human factors,logs analysis|With the advent of large code repositories and sophisticated search capabilities, code search is increasingly becoming a key software development activity. Yet, we do not know much about the context in which search is conducted and what search patterns developers use. This is important as context and patterns can drive future techniques and tools underlying not just code search but also software development environments and workflow. In this work we shed some light into code search context and patterns through a case study performed at Google, using a combination of survey and log-analysis methodologies. Our study provides insights into what developers are doing and trying to learn when performing a search, search scope, query properties, and what a search session under different contexts usually entails. Our results indicate that programmers search for code very frequently, conducting an average of five search sessions with 12 total queries each workday. Programmers are typically looking for code with which they are somewhat familiar and are seeking answers to questions about how to use an API, what code does, why something is failing, or where code is located.|None|accept|accept
FSE|2015|MFix: Automatically Learning Fix Patterns from Past Fixes to Generate Method Fixes|jinqiu yang,quinn hanam,mike chong,nasir ali,taiyue liu,lin tan|automatic program repair,fix patterns,recurring fixes|A large portion (17–45%) of bug fixes are recurring fixes—fixes that are identical or similar to another fix. We propose MFix— a new approach to learn fix patterns at the Abstract Syntax Tree level from past fixes automatically and leverage them to generate recurring fixes automatically. Guided by our empirical study of recurring fixes, MFix proposes a novel technique, which includes a new context matching algorithm to match similar contexts (both syntactic and semantic similarities) of recurring fixes to identify relevant locations for applying fix patterns accurately. The evaluation on 20 recurring bugs from 5 projects—Eclipse JDT, Eclipse SWT, ZK Web Framework, OpenJPA and Wicket— shows that MFix complements existing automated program repair techniques (the state-of-the-art techniques—RSRepair and PAR). Specifically, MFix generates 20 high-quality method fixes (fixes that are identical to developer fixes) that RSRepair and PAR cannot generate. For the 20 recurring bugs, MFix learns 3,838 fix patterns automatically and uses them to generate 31 method fixes, 27 of which (87.1%) are correct; 85.2% (23 out of 27) are identical to the fixes generated by developers.|None|reject|reject
FSE|2015|Multi-Representational Security Analysis|eunsuk kang,aleksandar milicevic,daniel b. jackson|Security,Specification,Verification,Model composition|Many security attacks arise from unanticipated behaviors that are inadvertently introduced by the designer at various stages of the development. We propose a novel approach to formal analysis called the multi-representational security analysis. This approach can be used by the designer to incrementally explore the impact of design decisions on security, and discover attacks that span multiple layers of the system. We describe Poirot, a prototype implementation of this approach, and report on our experience on applying Poirot to detect security flaws in publicly deployed systems.|None|reject|reject
FSE|2015|Guiding Dynamic Symbolic Execution toward Unverified Program Executions|maria christakis,peter muller,valentin wustholz|dynamic symbolic execution,static analysis,condition inference|Most techniques to detect program errors, such as testing, code reviews, and static program analysis, do not fully verify all possible executions of a program. They leave executions unverified when they do not check an execution path, check it under certain unjustified assumptions (such as the absence of arithmetic overflow), or fail to verify certain properties. In this paper, we present a technique to complement partial verification results by automatic test case generation.  We annotate programs to reflect which executions have been verified, and under which assumptions. These annotations are then used to guide dynamic symbolic execution toward unverified program executions. Our main contribution is a code instrumentation that causes dynamic symbolic execution to abort tests that lead to verified executions, to prune parts of the search space, and to prioritize tests that lead to unverified executions.  We have implemented our technique for the .NET static analyzer Clousot and the dynamic symbolic execution tool Pex. Compared to directly running Pex on the annotated programs without our instrumentation, our approach produces smaller test suites (by up to 19.2%), covers more unverified executions (by up to 7.1%), and reduces testing time (by up to 52.4%).|None|reject|reject
FSE|2015|Regular Open APIs|diego calvanese,giuseppe de giacomo,maurizio lenzerini,moshe y. vardi|Regular Path Queries,View-based query answering,View-based query rewriting,Service composition,Finite state automata|An Open API is a software intermediary that makes it possible for application programs to interact with each other and cooperate. APIs are strongly related to services: indeed, services are modeled by specifying the functionality they realize, and by associating with each of them an API that exposes the service to clients. In many scenarios, when one wants to obtain or publish a new service, one would like to check whether the new functionality can simply be obtained by suitably composing existing services. In this paper we study this problem by distinguishing between two settings, called data-centric and process-centric, respectively. In the former, each API is an abstraction of a query specified on a database, and service composition amounts to building a new query by using the available APIs as views over the database. In the latter, each API abstracts a process made up by sequences of atomic actions, and service composition means realizing a new process by suitably using the APIs exposed by the available services. Since our goal in this work is to address fundamental issues of service composition, we make the assumption that the semantics of services is specified by means of one of the most basic formalisms used in Computer Science, namely, regular languages. Even with this simplifying assumption, our investigation illustrates the richness of the composition problem, by considering several relevant variants of the two settings, and describing principles and techniques for each of them.|None|reject|pre-reject
FSE|2015|An in-depth study of the impact of two non-functional requirements, traceability and detailing, on software maintenance|eduardo almentero,julio leite,carlos jose pereira de lucena|Software Maintenance,Detailing,Traceability,Case Study|Software maintenance is an important activity in the software development process. It has a significant impact on the software quality and consumes a huge amount of resources. It is a consensus between researches of this area that it is a complex activity, which needs to be supported to be performed efficiently. In this research, we elaborated a study to investigate the impact of two non-functional requirements, detailing and traceability, on maintenance tasks. The study was based on comparing the execution of maintenance tasks in controlled environments: with and without the traceability and detailing. Two groups of subjects, with different levels of expertise participated in the study and two metrics were used to evaluate their performance: correctness and execution time. The results demonstrated that the presence of these two qualities improves the software maintenance activities and, depending on the level|None|reject|reject
FSE|2015|Auto-Completing Bug Reports for Android Applications|kevin moran,mario linares vasquez,carlos bernal cardenas,denys poshyvanyk|Bug Reports,Android,Reproduction Steps,Auto-completion,Program Analysis|The modern software development landscape has seen a shift in focus toward mobile applications as smartphones and tablets near ubiquitous adoption. Due to this trend, the complexity of mobile applications has been increasing, making development and maintenance challenging. One of the most important processes related to software maintenance is the prompt fixing of bugs. However, current bug tracking systems do not effectively support reporting of bug reports with actionable information that will directly lead to a bug’s resolution. To address the need for an improved reporting system, we introduce a novel solution, called FUSION, that helps users auto-complete reproduction steps in bug reports for mobile apps. FUSION links information, that users provide, to program artifacts extracted through static and dynamic analysis performed beforehand. We evaluate FUSION by conducting a study that quantitatively and qualitatively measures the user experience of the system and the quality of the bug reports it produces. In a study involving 28 participants we apply FUSION to 15 real-world bugs found in 14 open source Android apps. Our results demonstrate that FUSION allows for more reliable reproduction of bugs from reports by suggesting more detailed application-specific information compared to traditional bug tracking systems such as the Google Code Issue Tracker.|None|accept|accept
FSE|2015|A Lightweight Dependability Case Language for Checking End-to-End Safety Properties|calvin loncaric,stuart pernsteiner,emina torlak,zachary tatlock,xi wang,jonathan jacky,michael d. ernst|dependability case,alloy,verification|A  dependability case establishes end-to-end safety properties for a system through evidence-based arguments.  Typical dependability cases rely heavily on manual reasoning, which threatens their validity.  This paper proposes a lightweight dependability case language, DCL, that aids in the construction of  dependability cases.  DCL is designed and implemented as a shallow embedding in the Alloy modeling language.  A DCL compiler invokes external, domain-specific analyzers to construct machine-checkable evidence. We applied DCL to constructing a dependability case for the Clinical Neutron Therapy System at the University of Washington Medical Center. In particular, we integrated DCL with domain-specific analyzers for two key components, the therapy software written in the EPICS dataflow language, and the Programmable Logic Controller that implements hardware interlocks.  Checking the end-to-end safety property of the system with DCL revealed non-trivial flaws in a pre-release version of the therapy software and in the EPICS implementation.  Our experience with DCL shows that it is both practical and effective in improving the reliability of safety-critical systems.|None|reject|reject
FSE|2015|Assertions Are Strongly Correlated with Test Suite Effectiveness|yucheng zhang,ali mesbah|Test suite effectiveness,Assertions,Coverage|Code coverage is a popular test adequacy criterion in practice. Code coverage, however, remains controversial as there is a lack of coherent empirical evidence for its relation with test suite effectiveness. More recently, test suite size has been shown to be highly correlated with effectiveness. However, previous studies treat test methods as the smallest unit of interest, and ignore potential factors influencing this relationship. We propose to go beyond test suite size, by investigating test assertions inside test methods. We empirically evaluate the relationship between a test suite’s effectiveness and the (1) number of assertions, (2) assertion coverage, and (3) different types of assertions. We compose 6,700 test suites in total, using 24,000 assertions of five real-world Java projects. We find that the number of assertions in a test suite strongly correlates with its effectiveness, and this factor directly influences the relationship between test suite size and effectiveness. Our results also indicate that assertion coverage is strongly correlated with effectiveness and different types of assertions can influence the effectiveness of their containing test suites.|None|accept|pre-accept
FSE|2015|Query-based Configuration of Text Retrieval Solutions for Software Engineering Tasks|laura moreno,gabriele bavota,sonia haiduc,massimiliano di penta,rocco oliveto,barbara russo,andrian marcus|Text-Retrieval in Software Engineering,Text-Retrieval Configuration,Feature and Bug Localization|Text Retrieval (TR) approaches have been used to leverage the textual information contained in software artifacts in order to address a multitude of software engineering tasks. In order to lead to good results, TR approaches need to be configured properly. Current approaches for automatic TR configuration in SE configure a single TR approach and then use it for all possible queries that can be formulated. In this paper, we show that such a configuration strategy leads to suboptimal results and propose QUEST, the first approach bringing TR configuration selection to the query level. QUEST recommends the best TR configuration for a given query, based on a supervised learning approach which determines the TR configuration that performs the best for each query based on its properties. We evaluated QUEST in the context of feature and bug localization, using a dataset with more than 1,000 queries, and we found that QUEST is able to recommend one of the top three TR configurations for a query with a 69% accuracy, on average. We compared the results obtained with the configurations recommended by QUEST for every query with those obtained using a single TR configuration for all queries in a system and in the entire dataset. We found that using QUEST we obtain better results than with any of the considered TR configurations.|None|reject|reject
FSE|2015|Crash Search Inspector|geore han,sang cha,arthur lee|Crash,Duplicate bug,Crash life cycle,QA system,Agile development|Building innovative software in agile development requires frequent releases to meet the end user's demands in a competitive market. Development process cannot afford losing much time in handling crashes in system software like a Database Management System (DBMS). An integrated crash life cycle management system with near real-time responses needs to meet such demand.  We developed such a system called Crash Search Inspector (CSI) to allow tight interaction between development and field services.  Duplicate bugs are common in a software life cycle and identifying duplicates is difficult and costly.  We describe an algorithm that identifies duplicate bugs by comparing crash call stacks.  It uses Levenshtein distance modified with fuzzy scores in comparing stack frames and exploits stack frame pruning and weighting to improve accuracy. This algorithm is the basis of CSI, which is integrated with other systems used for quality assurance. CSI has become an integral part of our crash life cycle management scheme. With CSI integrated into the system over 600 searches have been made per month on average and of those about 180 were identified as duplicates reducing much of human efforts in dealing with new bugs.|None|reject|reject
FSE|2015|MemInsight: Platform-Independent Memory Debugging for JavaScript|simon jensen,manu sridharan,koushik sen,satish chandra|debugging,dynamic analysis,profiling,JavaScript|JavaScript programs often suffer from memory issues that can either hurt performance or eventually cause memory exhaustion.  While existing snapshot-based profiling tools can be helpful, the information provided is limited to the coarse granularity at which snapshots can be taken.  We present MemInsight, a tool that provides detailed, time-varying analysis of the memory behavior of JavaScript applications, including web applications.  MemInsight is platform independent and runs on unmodified JavaScript engines.  It employs tuned source-code instrumentation to generate a trace of memory allocations and accesses, and it leverages modern browser features to track precise information for DOM (document object model) objects. It also computes exact object lifetimes without any garbage collector assistance, and exposes this information in an easily-consumable manner for further analysis. We describe several client analyses built into MemInsight, including detection of possible memory leaks and opportunities for stack allocation and object inlining.  An experimental evaluation showed that with no modifications to the runtime, MemInsight was able to expose memory issues in several real-world applications.|None|accept|accept
FSE|2015|Systematic Testing of Asynchronous Reactive Systems|ankush desai,shaz qadeer,sanjit a. seshia|Systematic Testing,Asynchronous Programs,Concurrency Testing,sampling concurrent executions,Model checking,Distributed Systems|We introduce the concept of a delaying explorer with the goal of performing prioritized exploration of the behaviors of an asynchronous reactive program. A delaying explorer stratifies the search space using a custom strategy, and a delay operation that allows deviation from that strategy. We show that prioritized search with a delaying explorer performs significantly better than existing prioritization techniques. We also demonstrate empirically the need for writing different delaying explorers for scalable systematic testing and hence, present a flexible delaying explorer interface. We introduce two new techniques to improve the scalability of search based on delaying explorers. First, we present an algorithm for stratified exhaustive search and use efficient state caching to avoid redundant exploration of schedules. We provide soundness and termination guarantees for our algorithm. Second, for the cases where the state of the system cannot be captured or there are resource constraints, we present an algorithm to randomly sample any execution from the stratified search space. This algorithm guarantees that any such execution that requires $d$ delay operations is sampled with probability at least $1/L^d$, where $L$ is the maximum number of program steps. We have implemented our algorithms and evaluated them on a collection of real-world fault-tolerant distributed protocols.|None|accept|accept
FSE|2015|Theoretical foundations for the difference of programs in testing|donghwan shin,doo-hwan bae|Theory,Testing,Debugging,Measurement|In software testing, there are theoretical foundations, such as fundamental factors of testing and their relations, which provide a solid background for testing. However, these foundations focus on the correctness of a program. We find that the problem of the correctness of a program is subsumed by the problem of the difference of programs in testing. To shift the paradigm of testing from correctness to difference, we define novel theoretical foundations for the difference of programs in testing. Further, we define the mathematical distance of programs to provide the degree of program difference in testing. We believe that the difference-based theoretical foundations provide useful perspectives in testing research, both in terms of theoretical and experimental results. We formally analyze the existing mutation testing and mutation-based fault localization techniques as applications and indicate novel implications for future work.|None|reject|reject
FSE|2015|Kmax: Finding All Linux Compilation Units|paul gazzillo|Linux,C,Kbuild,Kconfig,Makefiles,Build Systems|Having an accurate list of compilation units for the Linux kernel is useful for static analysis tools and software engineering research. Finding the compilation units accurately is deceptively difficult. Linux's build system specification files are designed to evaluate one configuration at a time and are not amenable to formal analysis.  With over 14,000 configuration variables, trying configurations one a time is impossible.  Kmax solves this problem by evaluating the build system files across all configurations simultaneously, yielding the complete set of compilation units.  It first analyzes the configuration variables described by the Linux feature model.  It then processes build system files with a configuration-preserving Makefile evaluator that stores variables in a conditional symbol table and hoists conditionals around complete statements of the build system language.  Kmax's results are evaluated empirically for correctness and completeness on the Linux kernel source code.  It is also compared to previous work for correctness and running time.  The added complexity of a complete solution is shown to incur only a relatively small latency compared with approximate solutions.|None|reject|reject
FSE|2015|The Charming Code that Error Messages are Talking About|joshua campbell,abram hindle|Mutation,Testing,Software metrics,Complexity|The intent of high test coverage is to ensure that the dark nooks and crannies of code are exercised and tested. In a language like Python this is especially important as syntax errors can lurk in unevaluated blocks, only to be discovered once they are finally executed.  Bugs that present themselves as error messages mentioning a line of code which is unrelated to the cause of the bug can be difficult and time-consuming to fix when a developer must first determine the actual location of the fault. A new code metric, charm, is presented.  Charm can be used by developers, researchers, and automated tools to gain a deeper understanding of source code and become aware of potentially hidden faults, areas of code which are not sufficiently tested, and areas of code which may be more difficult to debug. Charm quantifies the property that error messges caused by a fault at one location don't always reference that location. In fact, error messages seem to prefer to reference some locations far more often than others. The quantity of charm can be estimated by averaging results from a random sample of similar programs to the one being measured by a procedure of random-mutation testing. Charm is estimated for release-quality Python software, requiring many thousands of similar Python programs to be executed.  Charm has some correlation with a standard software metric, cyclomatic complexity.  21 code features which may have some relationship with charm and cyclomatic complexity are investigated, of which five are found to be significantly related with charm.  These five features are then used to build a linear model which attempts to estimate charm cheaply.|None|reject|pre-reject
FSE|2015|How Does the Way that Software Metrics are Aggregated Impact Defect Prediction Models?|feng zhang,ahmed e. hassan,shane mcintosh,ying zou|defect prediction,aggregation scheme,software metrics|Defect prediction models leverage the relationship between historical defect data and software metrics to help software organizations to anticipate where defects will appear in the future. In order to train defect prediction models, historical defect data is often mined from a Version Control System (VCS, e.g., Subversion), which records software changes at the file-level. Software metrics, on the other hand, are often calculated at the method- or class-level (e.g., McCabe’s Cyclomatic Complexity). To address the disagreement in granularity, the method- and class-level software metrics are aggregated to file-level, often using summation. Recent study shows that summation significantly inflates the correlation between lines of code (SLOC) and cyclomatic complexity (CC) in Java projects. While there are many other aggregation schemes (e.g., central tendency, dispersion), they have remained unexplored in the scope of defect prediction. In this study, we set out to investigate how different aggregation schemes impact defect prediction models. Through an analysis of 11 aggregation schemes using data collected from 255 open source projects, we find that: (1) aggregation schemes can significantly alter correlations among metrics, as well as the correlations between metrics and the defect count; (2) applying all aggregation schemes significantly improves the performance of models that predict defect proneness for 36% of the studied systems, and the performance of models that rank files based on their defect proneness for 9% of systems; (3) choosing an appropriate aggregation scheme significantly improves the performance of effort-aware defect prediction models for 8% of systems; and (4) summation is sufficient for building models that predict the count of defects in files. Given the potential benefit of applying various aggregation schemes and the negligible cost of computing them, we suggest that future defect prediction models experiment with all possible aggregation schemes.|None|reject|reject
FSE|2015|Explaining Concurrency Bugs with Interpolants|andreas holzer,daniel schwartz-narbonne,mitra tabaei befrouei,georg weissenbacher,thomas wies|interpolation,error explanation,concurrency bugs|Debugging is the process of reproducing an error in a program and subsequently isolating the code fragments responsible for the bug. This challenging task is even exacerbated by the presence of threads. We present an automated technique to generate an error explanation comprising of a code fragment that causes the problem as well as annotations that reflect the erroneous program states. Our framework generalizes existing interpolation-based approaches and enables concise and informative explanations of errors in multi-threaded programs. By providing the option to include in the explanations varying levels of details - such as hazards and branching information - our approach allows the programmer to focus on individual aspects of explanations, enabling a systematic analysis. We evaluate our technique on a number of case studies, including concurrent data structures and programs with locks.|None|reject|reject
FSE|2015|Let me in! Supporting Newcomers to Make Their First Contribution to Open Source Software Projects|igor steinmacher,tayana conte,marco gerosa|Newcomers,Open Source Software,Barriers,Onboarding,Joining,Diaries,Self-efficacy,Technology Acceptance Model|Community-based Open Source Software (OSS) projects are usually self-organized and dynamic, receiving contributions from distributed volunteers. These communities’ survival, long-term success, and continuity demand a constant influx of newcomers. However, newcomers face many barriers when making their first contribution to an OSS project, leading in many cases to dropouts. Therefore, a major challenge for OSS projects is to provide ways to support newcomers during their first contribution. In this paper, our goal was to propose and evaluate a FLOSScoach, a portal created to support newcomers to OSS projects. The portal was implemented based on a conceptual model of barriers created from the analysis of data collected from: semi-structured interviews with 36 developers from 14 different projects; 24 answers to an open questionnaire conducted with OSS developers; feedback from 9 graduate and undergraduate students after they tried to join OSS projects; and 20 primary studies gathered via a systematic literature review. To assess the portal, we conducted a study with students, relying on qualitative data from diaries, self-efficacy questionnaires, and the Technology Acceptance Model. The results indicate that the portal played an important role guiding newcomers and lowering barriers related to the orientation and contribution process, whereas it was not efficient in lowering technical barriers. We also found that the portal is useful, easy to use, and increased newcomers’ confidence to contribute.|None|reject|reject
FSE|2015|Obfuscation-Resilient, Efficient, and Accurate Detection and Family Identification of Android Malware|joshua garcia,mahmoud hammad,bahman pedrood,ali bagheri-kaligh,sam malek|Android,malware,security,supervised learning|The number of Android malware apps are increasing very quickly. Simply detecting and removing malware apps is insufficient, since they can damage or alter other files, data, or settings; install ad- ditional applications; etc. To determine such behavior, a security engineer can significantly benefit from identifying the specific fam- ily to which an Android malware belongs. Techniques for detecting Android malware, and determining their families, lack the abil- ity to deal with obfuscations (i.e., transformations of application to thwart detection). Moreover, some of the prior techniques are highly inefficient, making them inapplicable for real-time detec- tion of threats. To address these limitations, we present a novel machine learning-based Android malware detection and family iden- tification approach, RevealDroid, that provides selectable features. We assess RevealDroid to determine a selection of features that enable obfuscation resiliency, efficiency, and accuracy for detection and family identification. We assess RevealDroid’s accuracy and obfuscation resilience on an updated dataset of malware from a diverse set of families, including malware obfuscated using various transformations, and compare RevealDroid against an existing An- droid malware-family identification approach and another Android malware detection approach.|None|reject|pre-reject
FSE|2015|Generating Integration Test Suites with Oracles|mauro pezze,cheng zhang,lei ma|Test case generation,Test oracles,Unit testing,Integration testing|Automatic test case generation can significantly reduce the cost of software testing, but faces two main difficulties: generating complex test cases that reveal non-trivial faults and generating useful test oracles. In this paper we propose a fully automated technique to generate test cases with useful oracles. Our technique is grounded on the observation that the many simple unit test cases often available in software projects form a valuable source of information that can be exploited to generate more complex test cases. From existing test cases, our approach extracts assertion templates, which represent core steps relevant to each assertion, and then combines assertion templates with complex objects that are randomly generated and selected according to properties of the assertions. The resultant test cases generally involve more classes under test and contain strong assertions. The preliminary experimental results indicate that our approach achieves higher code coverage and mutation score compared to the seed test cases, while producing few false positives.|None|reject|pre-reject
FSE|2015|Fuzzy History Analysis: Improving the Accuracy of Fine-Grained Code-Evolution Analysis|francisco servant,james a. jones|Software evolution,Software history analysis,Mining software repositories|Existing software-history techniques represent source-code evolution as an absolute and unambiguous mapping of lines of code in prior revisions to lines of code in subsequent re- visions. However, the true evolutionary lineage of a line of code is often complex, subjective, and ambiguous. As such, existing techniques are predisposed to, both, overestimate and underestimate true evolution lineage. In this paper, we seek to address these issues by providing a more expressive model of code evolution, the fuzzy history graph, by rep- resenting code lineage as a continuous (i.e., fuzzy) metric rather than a discrete (i.e., absolute) one. Using this more descriptive model, we additionally provide a novel multi- revision code-evolution analysis — fuzzy history slicing. In our experiments over three real-world software systems, we found that the fuzzy history graph provides a tunable bal- ance of precision and recall, and an overall improved accu- racy over existing code-evolution models. Also, we found that fuzzy history slicing provided improved accuracy over absolute history slicing.|None|reject|reject
FSE|2015|Mutation Testing-Aware Debugging for Ajax Applications|yuta maezawa,kohsuke yatoh,shinichi honiden|Ajax,JavaScript,Web application,Mutation testing,Debugging,Program repair|Although Asynchronous JavaScript and XML (Ajax) technologies make Web applications responsive, their event-driven and asynchronous features impose the challenge of debugging nondeterministic faults on developers. Mutation testing helps developers design adequate test sets to detect faults; however, developers need to fix the detected faults, which is beyond the scope of mutation testing. We propose a mutation testing-aware debugging method implemented in a tool called RevAjaxMutator. Since faulty versions of a test program, called mutants, are provided during mutation testing, our key idea is that the change from a mutant to a test program can reveal how to fix the detected faults; therefore, we define these changes as debugging operators. When developers input i) the test program, ii) test sets improved during mutation testing, and iii) a commit history containing results of mutation testing, RevAjaxMutator suggests fault-fixing methods from successful debugging operators to fix the detected faults, as output. Finally, developers confirm whether the suggestions are correct. The results of our case studies on several real-world Ajax applications shows that RevAjaxMutator suggests correct fault-fixing methods, and we conclude it can help developers build reliable Ajax applications.|None|reject|reject
FSE|2015|Test Report Prioritization to Assist Crowdsourced Testing|yang feng,zhenyu chen,james a. jones,chunrong fang,baowen xu|Crowdsourcing Testing,Test Report Prioritization,Testing|In crowdsourced testing, users can be incentivized to per- form testing tasks and report their results, and because crowdsourced workers are often paid per task, there is a fi- nancial incentive to complete tasks quickly rather than well. These reports of the crowdsourced testing tasks are called “test reports” and are composed of simple natural language and screenshots. Back at the software-development organi- zation, developers must manually inspect the submitted test reports to judge their value for revealing faults. Due to the nature of crowdsourced work, the number of test reports are often difficult to comprehensively inspect and process. In order to help with this daunting task, we created the first technique of its kind, to the best of our knowledge, to pri- oritize test reports for manual inspection. Our technique utilizes two key strategies: (1) a diversity strategy to help developers inspect a wide variety of test reports to avoid duplicates and wasted effort on falsely classified faulty be- havior, and (2) a risk strategy to help developers identify test reports that foretell future fault-revealing based on past test reports. Together, these strategies form our DivRisk strategy to prioritize test reports in crowdsourced testing. Three industrial projects of crowdsourced testing have been used to evaluate the effectiveness of test report prioritization methods. The results of empirical study show that: (1) Di- vRisk can significantly outperform random prioritization; (2) DivRisk can approximate the best theoretical result for a real-world industrial mobile application. In addition, we provide some practical guidelines of test report prioritiza- tion for crowdsourced testing based on the empirical study and our experiences.|None|accept|pre-accept
FSE|2015|ARROW: Automated Repair of Races on Client-Side Web Pages|weihang wang,yunhui zheng,peng liu,lei xu,xiangyun zhang,patrick th. eugster|automatic repair,constraint solving,static analysis,web application|Modern browsers have a highly concurrent page rendering process in order to be more responsive. However, such a concurrent execution model leads to various race issues. In this paper, we present ARROW, a static technique that can automatically, safely, and cost effectively patch race issues on client side pages. It works by statically modeling a web page as a causal graph denoting happens-before relations be- tween page elements, according to the rendering process in browsers. Races are detected by identifying inconsistencies between the graph and the dependence relations intended by the developer, which are extracted from the page source. Detected races are fixed by leveraging a constraint solver to add a set of edges with the minimum cost to the causal graph so that it becomes consistent with the intended de- pendences. The input page is then transformed to respect the repair edges. ARROW has fixed 151 races from 20 real world commercial web sites.|None|reject|reject
FSE|2015|Test Case Prioritization using Weight-Based Search Algorithms in a Maritime Application|dipesh pradhan,shaukat ali khan,tao yue|Test Prioritization,Search Algorithms,Maritime,Test Execution|Due to limited time and resources available for test execution, test prioritization always remains crucial for cost-effective testing. It is even more prominent when test cases have to be executed manually, e.g., operating physical equipment or devices. Test prioritization must take into consideration complicated tradeoff relationships between cost (e.g., execution time) and effectiveness (e.g., number of faults caught by a test case). Based on our industrial collaboration within the Maritime domain, we identified a real-world and multi-objective test prioritization problem in the context of robustness testing, where test cases require human involvement in certain steps such as turning on the power supply to a control module and recording observations. The high-level objective is to prioritize test cases for execution within allowed time budget, where test engineers, depending on the testing requirements, provided weights for various objectives. We defined a fitness function based on the various objectives that were identified together with test engineers and empirically evaluated it with four search algorithms: Alternating Variable Method (AVM), Genetic Algorithm (GA), (1+1) Evolutionary Algorithm (EA) and Greedy Algorithm (Greedy). In addition, we used Random Search (RS) as the comparison baseline. We conducted the following three sets of empirical evaluations: 1) Using real data from the industrial partner (i.e., an industrial problem), 2) Simulating the industrial problem to a larger scale to assess the scalability of the search algorithms, 3) Simulating the industrial problem to a larger scale and assessing the improvement of the performance of the search algorithms along with the increase of the number of iterations. Results show that (1+1) EA performed the best when the number of test cases is small, whereas AVM performed the best (followed by (1+1) EA), when the number of test cases is large. Furthermore, we observed that the performance of GA and (1+1) EA improved along with the increase in the number of iterations.|None|reject|reject
FSE|2015|An Empirical Study of the Bias and Variance of Model Validation Techniques for Defect Prediction Models|chakkrit tantithamthavorn,shane mcintosh,ahmed e. hassan,ken-ichi matsumoto|Defect Prediction,Model Validation,Bias and Variance|Defect prediction models help software quality assurance teams to effectively allocate their limited resources to the most defect-prone software modules. These defect prediction models are trained using historical data. Model validation techniques, such as k-fold cross-validation, use this historical data to estimate how well a model will perform in the future. However, little is known about how accurate the performance estimates of these model validation techniques tend to be. While prior studies have explored the accuracy of model validation techniques in other domains (e.g., clinical and genomics data), they have arrived at inconsistent conclusions about which techniques tend to perform the best. Indeed, domain characteristics appear to play a key role. In this paper, we set out to investigate the bias (i.e., how much do the performance estimates differ from the ground truth?) and variance (i.e., how much do performance estimates vary when the experiment is repeated?) of 10 commonly used model validation techniques spread across the holdout, cross-validation, and bootstrap families. Through a case study of 18 systems spanning both open source and proprietary domains, we derived the following practical guidelines for future defect prediction studies: (1) the holdout family of model validation techniques should be avoided, since we find that they have 29%-130% more bias and 49%-487% more variance than the most accurate model validation techniques; (2) researchers should use advanced bootstrap validation techniques instead of cross-validation or holdout techniques, since we find that advanced bootstrap validation is less prone to bias and variance in the sparse data contexts that are present in many publicly-available defect datasets; and (3) calibration slope should be measured in future studies, since it captures the direction and spread of the predicted probabilities, which is important when the probabilities are used to make decisions.|None|reject|reject
FSE|2015|A Shape Neutral Analysis for Graph-based Data-structures|gregory duck,joxan jaffar,roland yap|Program Verification,Program Analysis,Memory Safety,Data-structure Correctness,Bug Detection|Malformed data-structures can lead to runtime errors such as arbitrary memory access or corruption.  Low level languages make it easy to have bugs which can lead to malformed data structures as they allow direct pointer manipulation -- making reasoning over data-structure properties challenging.  In this paper we present a constraint-based program analysis that checks data-structure integrity, w.r.t. given target data-structure properties, suitable for detecting data-structure bugs in low level code.  A key property of our analysis is that it is shape neutral, i.e. the analysis does not check for properties relating to a given data-structure graph shape, such as doubly-linked-lists versus trees.  As a result, the analysis can be applied to a wide range of data-structure manipulating programs, including those that use lists, trees, DAGs, etc., all of which are specific kinds of graph data-structures.  Furthermore, the analysis is powerful enough to detect certain classes of critical memory errors that can lead to data corruption or information leaks.  Our analysis is modular and can be used to analyze code in libraries and components without requiring whole programs.  Experimental results show that our approach works well in practice.|None|reject|reject
FSE|2015|Automatically Computing Path Complexity of Programs|lucas bang,abdulbaki aydin,tevfik bultan|Path Complexity,Path Coverage,Automated Testing|Recent automated software testing techniques focus on achiev- ing path coverage. We present a complexity measure that provides an upper bound for the number of paths in a pro- gram, and hence, can be used for assessing the difficulty of achieving path coverage for a given method. We define the path complexity of a program as a function that takes a depth bound as input and returns the number of paths in the control flow graph that are within that bound. We show how to automatically compute the path complexity function in closed form, and the asymptotic path complexity which identifies the dominant term in the path complexity function. Our results demonstrate that path complexity can be computed efficiently, and it is a better complexity mea- sure for path coverage co|None|accept|pre-accept
FSE|2015|Tracing Software Developers’ Eyes and Interactions for Change Tasks|katja kevic,braden m. walters,timothy r. shaffer,bonita sharif,thomas fritz,david shepherd|eye-tracking,change task,study|What are software developers doing during a change task? While an answer to this question opens countless opportunities to support developers in their work, only little is known about developers' detailed navigation behavior for realistic change tasks. Most empirical studies on developers performing change tasks are limited to very small code snippets or are limited by the granularity or the detail of the data collected for the study. In our research, we try to overcome these limitations by combining user interaction monitoring with very fine granular eye-tracking data that is automatically linked to the underlying source code entities in the IDE. In a study with 12 professional and 10 student developers working on three change tasks from an open source system, we used our approach to investigate the detailed navigation of developers for realistic change tasks. The results of our study show, amongst others, that the eye-tracking data captures indeed different aspects than user interaction data and that developers focus on only small parts of methods that are often related by data flow. We discuss our findings and their implications for better developer tool support.|None|accept|accept
FSE|2015|A Systematic Literature Review on Feedback Control-Based Adaptation in Software Systems|stepan shevtsov,mihaly berekmeri,danny weyns|Software,control theory,systematic literature review,controller,adaptive system,self-adaptation,control-based adaptation|Software systems inevitably change. For over a decade, researchers have argued to apply principles from control theory to build adaptive software systems to deal with such change. However, most control theoretic research focuses on managing resources (CPU, storage, etc.). Efforts that apply feedback control to \textit{software} are scattered and consequently, their is no clear view on the state of the art. This paper reports the results of a systematic literature review that aims to identify: the focus of research on control-based adaptation of software; how software is modeled and adapted with feedback controllers; and what qualities are considered. To answer these questions we extracted data from 25 primary studies selected from 1476 papers that resulted from an automatic search. The results show that: (i) simple linear, time-invariant models are used, while software is usually considered highly non-linear; (ii) traditional types of sensors and actuators used in control-based solutions are combined with new types of sensors and actuators to control software; (iii) the primary focus of the studies is on software qualities; classic controller qualities such as stability and settling time are poorly exploited. From the study, we identify open challenges for control-based adaptation of software.|None|reject|reject
FSE|2015|Hidden Truths in Dead Software Paths|michael eichberg,ben hermann,mira mezini,leonid glanz|Java,Finding Bugs,Scalable,Abstract Interpretation|Approaches and techniques for statically finding a multitude of issues in source code have been developed in the past. A core property of these approaches is that they are usually targeted towards finding only a very specific kind of issue and that the effort to develop such an analysis is significant. This strictly limits the number of kinds of issues that can be detected. In this paper, we discuss a generic approach that is not targeted towards any specific issue, but which can still discover a  wide range of issues that ranges from useless code that hinders comprehension to real bugs. The approach relies on the detection of dead paths in code  by calculating the difference between the control-flow graph that contains all technically possible edges and a corresponding graph recorded while performing an abstract interpretation of the code. We have evaluated the approach using the Java Development Kit and the Qualitas Corpus and were able to find thousands of issues across a wide range of categories.|None|accept|accept
FSE|2015|A Method to Identify and Correct Problematic Software Activity Data: Exploiting Capacity Limits and Data Redundancies|qimu zheng,audris mockus,minghui zhou|data quality,mining software repositories,capacity constraint|Mining software repositories to understand and improve software development is a common approach in research and practice. The operational data obtained from these repositories often do not faithfully represent the intended aspects of software development and, therefore, may jeopardize the conclusions derived from it. We propose an approach to identify problematic values based on the constraints of software development and to correct such values using data redundancies. We investigate the approach using issue and commit data of Mozilla project. In particular, we identified problematic data in four types of events and found the fraction of problematic values to exceed 10% and rapidly rising. We found the corrected values to be 50% closer to the most accurate estimate of task completion time. Finally, we found that the models of time until fix changed substantially when data were corrected, with the corrected data providing a 20% better fit. We discuss how the approach may be generalized to other types of operational data to increase fidelity of software measurement in practice and in research.|None|accept|accept
FSE|2015|Identifying Inline Library Functions in Binary Code|jing qiu,xiaohong su,peijun ma|Reverse engineering,static analysis,library identification,graph isomorphism|Identifying inline functions in binary code is a great challenge to analysts because discontinuity and polymorphism of inline functions make them more difficult to identify than full library functions. To solve this problem, we developed a novel approach for identifying inline functions. A new intermediate representation named the Execution Dependence Graph (EDG) is introduced to describe the execution semantics of binary code. Then, an inline function is identified by searching EDG subgraphs within target functions. Two straightforward filters are developed to avoid unnecessary subgraph isomorphism testings. This method can identify inline functions that contain discontinuous bytes by analyzing the execution dependence within the instructions. It can also identify variants of an inline function by instruction numbering. A prototype tool is implemented. Experiments on open sources show that the proposed method is capable of identifying inline functions in high precision and recall.|None|reject|reject
FSE|2015|Sound Mutation of Concurrent Execution|peng liu,xiangyun zhang,omer tripp,yunhui zheng|race detection,soundness,coverage,mutation of concurrent execution|Sound analysis for concurrent real world programs with thread schedules in consideration is challenging due to the sheer number of schedules and the complexity of programs. The state-of-the-art, predictive analysis, mitigates these challenges by starting with a concrete trace of concurrent execution and mutating the trace (e.g. reordering trace entries) to expose bugs. However, due to the difficulty of reasoning about trace changes in a sound fashion, existing predictive analysis are very restrictive in the possible mutations. In this paper, we formally study the set of possible sound trace mutations and identify the bound of such mutations. We develop Commutator that allows mutating paths, program states, and memory references of a trace. It even allows additional heap allocations. As a result, Commutator can explore a much larger set of new executions from a given seed execution, while ensuring soundness. The application of Commutator on race detection shows that the technique can detect 2.5 times more races than predictive analysis.|None|reject|reject
FSE|2015|Witness Validation and Step-Wise Testification across Software Verifiers|dirk beyer,matthias dangl,daniel dietsch,matthias heizmann,andreas stahlbauer|Counterexample Validation,Model Checking,Program Analysis,Software Verification|It is commonly understood that a verification tool should provide a counterexample (error witness) if the specification is violated, such that the system developer can use it to better understand the problem. Until recently, software verifiers were dumping error witnesses in proprietary formats, which are often neither human- nor machine-readable, and an exchange of witnesses between different verifiers was impossible. To close this gap in software-verification technology, we have defined a format for error witnesses that is easy to read by verification tools for further processing (e.g., witness validation) and that is easy to convert into visualizations that conveniently let the developers inspect the path to the error. There is a strong need for eliminating manual inspection of false alarms, which is now better supported in the following way: an efficient but perhaps imprecise verifier finds a problematic program path and constructs a witness for it; then a precise but expensive verifier re-verifies that the witness indeed violates the specification. We have implemented the technique of error-witness-driven program analysis in two state-of-the-art verification tools, CPAchecker and Ultimate Automizer, and show by experimental evaluation that the approach is applicable to a large set of verification tasks.|None|accept|pre-accept
FSE|2015|Code Similarity via Natural Language Descriptions|meital ben sinai,eran yahav|Code Similarity,Natural Language,Big Code,Program Analysis,Semantic Similarity|Code similarity is a central challenge in many programming related applications, such as code search, automatic translation, and programming education. In this work, we present a novel approach for establishing the similarity of code fragments by computing textual similarity between their corresponding textual descriptions. In order to find textual descriptions of code fragment, we leverage the collective knowledge captured in question-answering sites, blog posts and other sources. Because our notion of code similarity is based on similarity of corresponding textual descriptions, our approach can determine semantic relatedness and similarity of code across different libraries and even across different programming languages, a task considered extremely difficult using traditional approaches. To experiment with our approach, we implemented it using data obtained from the popular Q&A site, Stackoverflow, and used it to determine the similarity of 100000 pairs of code fragments. We developed a crowdsourcing system that allows users to label the similarity of code fragments and utilized it to build a massive corpus of 6500 labeled program pairs. Our results show that our technique is effective in determining similarity, and presents high precision, recall and accuracy.|None|reject|pre-reject
FSE|2015|P3: Partitioned Path Profiling|mohammed afraz,diptikalyan saha,aditya kanade|Path Profiling,Parallelization,Static Analysis|Acyclic path profile is an abstraction of dynamic control flow paths of procedures and has been found to be useful in a wide spectrum of activities. Unfortunately, the runtime overhead of obtaining such a profile can be high, limiting its use in practice. In this paper, we present partitioned path profiling (P3) which runs K copies of the program in parallel, each with the same input but on a separate core,  and collects the profile only for a subset of intra-procedural paths in each copy, thereby, distributing the overhead of profiling. P3 identifies ``profitable'' procedures and assigns disjoint subsets of paths of a profitable procedure to different copies for profiling. To obtain exact execution frequencies of a subset of paths, we design a new algorithm, called PSPP. All paths of an unprofitable procedure are  assigned to the same copy. P3 uses the classic Ball-Larus algorithm for profiling unprofitable procedures. Further, P3 attempts to evenly distribute the profiling overhead across the copies. To the best of our knowledge, P3 is the first algorithm for parallel path profiling. We have applied P3 to profile several programs in the SPEC 2006 benchmark. Compared to sequential profiling, P3 substantially reduced the runtime overhead on these programs averaged across multiple inputs. The reduction was 28%, 47% and 60% on average for 2, 4 and 8 cores respectively. P3 also performed better than a coarse-grained approach that treats all procedures as unprofitable and distributes them across available cores. For 2 cores, the profiling overhead of P3 was on an average 12% lesser compared to the coarse-grained approach across these programs. For 4 and 8 cores, it was respectively 24% and 32% lesser.|None|accept|accept
FSE|2015|A Framework for Evaluating the Accuracy of the SZZ Approach for Identifying Bug-Introducing Changes|daniel alencar da costa,shane mcintosh,weiyi shang,uira kulesza,roberta coelho,ahmed e. hassan|SZZ,Bug detection,Empirical study|The approach proposed by Sliwerski Zimmermann, and Zeller (SZZ) for identifying bug-introducing changes is important because it provides the foundation for several research areas within the software engineering discipline. Despite the foundational role of SZZ, little effort has been made to evaluate its accuracy. Such an evaluation is a challenging task because the ground truth is not readily available. By acknowledging such challenges, we propose a framework to evaluate the accuracy of a given SZZ implementation. The framework factors in the following criteria: (1) the knowledge of domain experts, (2) the future impact of changes, and (3) the realism of bug introduction. We use the proposed framework to compare the accuracy of five SZZ implementations using data from ten open source systems. We find that previously proposed improvements to SZZ tend to inflate the number of incorrectly identified bug-introducing changes. We also find that a single bug-introducing change may induce multiple future bugs reported in a 437 day window from one another on average (median). Furthermore, we find that at least 44% of the bugs are caused by bug-introducing changes that are years apart by one another. Such results suggest that current SZZ implementations still lack mechanisms to accurately identify bug-introducing changes. The framework that we propose provides a systematic means of evaluating and renfining a given SZZ implementation.|None|reject|reject
FSE|2015|Dynamic Analysis of User Reviews on Mobile Apps|cuiyun gao,baoxiang wang|User reviews,mobile apps,dynamic analysis|User reviews on mobile apps are a wealth of valuable repositories for the developers and have been extensively studied in academia recently. Similar to other types of software repositories, the history of app reviews could directly facilitate the bug-fixing or feature-designing process for app development. Previous studies were mostly focused on extracting static topics and ranking reviews using the whole dataset collected. However, mobile apps are basically utility-driven with the characteristics of strong timeliness. That is to say, user reviews are typically effective in a specific period of time. Practically, in order to establish a macro view of their products, developers care more about the current user-concerned issues or the history of user feedback, which is usually ignored by existing studies.\par In this paper, we extract the principal topics of user reviews from a dynamic perspective and visualize the dynamics based on ThemeRiver, a tool for analyzing data with time. As far as we know, this paper is the first to visualize dynamic topics of user reviews on mobile apps. Moreover, the previous research tasks on app reviews usually demand researchers an immense amount of time and efforts to interpret the extracted topics. In our framework, to expedite the analysis for developers, we automate the topic-labeling process by employing an optimization method and a neural network method respectively. The generated labels are then fed to the visualization process. To verify the effectiveness of the dynamic analysis, we conduct experiments on four popular apps and the results demonstrate the consistency of our approach with the official announcement.|None|reject|withdrawn
FSE|2015|Characterizing and Repairing Unwanted Behaviors for Android Apps|yafeng lu,wei yang,xusheng xiao,cong liu,lingming zhang|program repair,mobile computing,malware|With the increase popularity of smartphones, mobile threats of malicious apps and potentially unwanted apps also grow dramatically. The unwanted behaviors of these apps steal users’ sensitive information and control various resources(e.g., communication expenses). Manual repair of the unwanted behaviors can be very tedious and expensive. Although a number of automated program repair techniques have been proposed recently, none of them can be directly applied to malicious app repair. In this work, we take the first step towards automated mobile app repair. We first conduct an extensive study to characterize unwanted behaviors of various real-world apps at four levels of granularity(“where”, “when”, “what”, and “how”). Based on the characterization, we propose a suite of repair strategies to repair the apps at all four levels. Finally, we propose a general framework, SMAR(Systematic Mobile App Repair) that iteratively repairs unwanted behaviors at all four levels. At each level, the repair goes through four major steps: detection, repair, validation, and impact analysis. SMAR considers an app being successfully repaired if the app passes all four steps at a level. Our evaluation results show that even the most coarse-grained strategies of SMAR are able to fix 70.7% of the studied unwanted behaviors, demonstrating the promising direction of automatically repairing unwanted behaviors in mobile apps.|None|reject|pre-reject
FSE|2015|Requirements Reference Models Revisited - Accommodating Hierarchy in System Design|anitha murugesan,mats per erik heimdahl,sanjai rayadurgam,michael w. whalen|Requirements Reference model,Hierarchical decomposition,Formal Analysis|Reference models such as Parnas et al's Four Variable Model and Gunther et al.'s WRSPM model provide abstract frameworks that define necessary system development artifacts, such as requirements, assumptions, and specifications, as well as essential relations between these artifacts necessary for reasoning about system correctness. While these reference models are useful for understanding systems, they assume a specific system vantage point at a single level of abstraction. However, in practice, large-scale systems are hierarchically constructed; design decisions that allocate responsibilities to subcomponents (i.e. architectural design choices) at one level of abstraction become requirements for components at the next lower level of abstraction. We argue that a requirements reference model intended for large-scale systems must account for hierarchy, and that system structure (architecture) and the relationships between artifacts within this structure become essential concepts for the model. In this paper, we introduce a hierarchical reference model that provides a generic framework to help understand and reason about complex systems.|None|reject|reject
FSE|2015|A Critical Evaluation of Spectrum-Based Fault Localization Techniques on a Large-Scale Software System|fabian keller,antonio filieri,andre van hoorn,lars grunske|Debugging,Fault Localization,Cast study|In the past, various spectrum-based fault localization (SBFL) techniques  have been developed to pinpoint a fault location in a program given a set of failing and successful test executions. Most of the algorithms use similarity coefficients and have only been evaluated on established but small benchmark programs from the Software-artifact Infrastructure Repository. In this paper, we evaluate the feasibility of applying 33 state-of-the-art SBFL techniques to a large real-world project, namely AspectJ. From an initial set of 350 faulty version from the iBugs repository of AspectJ we manually classified 88 bugs where SBFL techniques can be applied. Notably, only 12 bugs can be found after examining the 1000 suspicious lines and on average 250 source code files need to be investigate per bug. Based on these and other results, the study showcases the limitations of SBFL on a larger program with the help of traditional and newly developed effectiveness metrics.|None|reject|reject
FSE|2015|Users Beware: Preference Inconsistencies Ahead|farnaz behrang,myra b. cohen,alessandro orso|Configurable systems,Program analysis,Software evolution|The preference structure of modern highly-configurable software systems has become very complex, usually consisting of multiple layers of access that go from the user interface through to the source code. This complexity can lead to inconsistencies between layers, especially during software evolution. For example, there may be preferences that users can change through the GUI but have no ef- fect on the actual behavior of the system because the related source code is not present or has been removed going from one version to the next. These inconsistencies may result in unexpected pro- gram behaviors, which range in seriousness from mild annoyances to more serious security or performance problems. To address this problem, we present SCIC (Software Configuration Inconsistency Checker), a static analysis technique that can automatically detect these kinds of inconsistencies. Unlike other configuration analysis tools, SCIC can handle software that (1) is written in multiple pro- gramming languages and (2) has a complex preference structure. In an empirical evaluation that we performed on 10 years of the widely used Mozilla Core and the 35 Firefox versions that appeared dur- ing that timeframe, SCIC was able to find 40 real inconsistencies, whose lifetime spanned multiple versions, and whose detection in- deed required the analysis of code written in multiple languages. Our further analysis of the inconsistencies also shows that some of these can potentially result in serious issues for users.|None|accept|accept
FSE|2015|To UML or not to UML? - Empirical Study on the Approachability of Software Architecture Diagrams|veli-pekka eloranta,essi isohanni,samuel lahtinen,outi sievi-korte|software architecture,UML,diagrams|Software architecture design is key to building systems that meet quality demands. In order to successfully communicate the architecture to different stakeholders, and thus ensure quality throughout, the description must be well-thought. Choosing the appropriate way to model the architecture ensures it is rightly understood by everyone involved. UML diagrams are commonly used in software engineering, but free form diagrams are almost as common. In this paper we study what are the factors influencing the approachability of diagrams, whether there is a difference in the understandability between UML and non-UML diagrams, and what kind of diagrams would be best suited to communicate architectures, especially in the case where the interpreter does not have domain knowledge of the system. Our results show that colors do not necessarily increase the approachability of diagrams, as their semantics may be difficult to infer. Furthermore, UML diagrams were preferred over free-form diagrams, as free-form diagrams more often suffer from ambiguousness. We conclude that simplicity and correctness are key factors when modeling architectures.|None|reject|reject
FSE|2015|A Study of Collaborative Development in Apache Software Foundation Project Teams|mohammad gharehyazie,vladimir filkov|Teamwork,Collaborative Development,OSS|Large software systems get the attention of multiple developers; some files regularly get touched by several people at nearly the same time. To ensure the viability of their collaboration, when multiple people work on the same chunk of code at the same time, they communicate and employ safeguards in various ways than when working alone. Recent studies have considered group co-development in OSS projects and found that it is an essential part of many projects, with benefits to coding efficiency. However, those studies were limited to groups of size two, i.e., pairs of developers. In this paper we describe an algorithm for capturing group collaboration beyond size two, based on synchronized commit activities among multiple developers. We use it to characterize group co-development in 26 OSS projects from the Apache Software Foundation, with respect to developer focus and productivity. After identifying potential collaborative groups, we find that developers’ behavior while in them is different than when programming alone, e.g., high developer focus on specific code packages associates with lower team participation, while packages with higher ownership get less attention from groups than from individuals. Finally, we show that productivity effort during co-development is more often lower for developers while they co-develop in groups. We make use of several qualitative methods, including an ASF developer survey to verify our results. We conclude that these methods and results can be used to understand the effects of the collaborative dynamic in OSS teams on the software engineering process. Our code, along with our datasets and survey is available at http://csiflabs.cs.ucdavis.edu/~ghareh/supplementary/teamwork/|None|reject|reject
FSE|2015|Developer Turnover in Open-Source Software|matthieu foucault,marc palyart,xavier blanc,gail c. murphy,jean-remy falleri|mining software repositories,qualitative analysis,software metrics|Turnover is the phenomenon of continuous influx and retreat of human resources in a team. Despite being well-studied in many settings, turnover has not been characterized for open-source software projects. We study 5 open source projects to characterize patterns of turnover and to determine the effects of turnover on software quality. We define the base concepts of both external and internal turnover, which are the mobility of developers in and out of a project, and the mobility of developers inside a project, respectively. We provide a qualitative analysis of turnover patterns. We also found, in a quantitative analysis, that external newcomers negatively impact module quality. We did not find, however, consistent impact of external leavers or internal turnover across projects.|None|accept|accept
FSE|2015|Clone-based and Interactive Recommendation for Modifying Pasted Code|yun lin,xin peng,zhenchang xing,diwen zheng,wenyun zhao|copy and paste,code clone,recommendation,differencing,reuse|Developers often need to modify pasted code when reusing code by copy and paste. Some modifications on pasted code could involve lots of editing efforts, and any missing or wrong edits could incur bugs. In this paper, we propose a clone-based and interactive approach to recommending where and how to modify the pasted code. In our approach, we regard the clones of the pasted code as the results of historical copy-and-paste operations and their differences as historical modifications on the same piece of code. Our approach retrieves clones of pasted code from a clone repository and detects syntactically complete differences across them. Then our approach transfers each clone difference into a modification slot on the pasted code, suggests options for each slot, and further mines modifying regulations from the clone differences. Based on the mined modifying regulations, our approach dynamically updates the suggested options and their ranking in each slot according to developer's modification on the pasted code. We implement a proof-of-concept tool CCDemon based on our approach and evaluate its effectiveness based on code clones detected from five open source projects. The results show that our approach can identify 96.9% of the to-be-modified positions in pasted code and suggest 75.0% of the required modifications. Our human study further confirms that CCDemon can help developers to accomplish their modifications of pasted code more efficiently.|None|accept|accept
FSE|2015|Cross-language Program Slicing for Dynamic Web Applications|hung nguyen,christian kastner,tien n. nguyen|Program slicing,Dynamic web applications,Embedded code analysis,Cross-language|During software maintenance, program slicing is a useful technique to assist developers in understanding the impact of their changes. While different program-slicing techniques have been proposed for traditional software systems, program slicing for dynamic web applications is challenging since the client-side code is generated from the server-side code and data entities are referenced across different languages and are often embedded in string literals in the server-side program. To address those challenges, we introduce WebSlice, an approach to compute program slices across different languages for web applications. We first identify data-flow dependencies among data entities for PHP code based on symbolic execution. We also compute SQL queries and a conditional DOM that represents client-code variations and construct the data flows for embedded languages: SQL, HTML, and JavaScript. Next, we connect the data flows across different languages and those across PHP pages. Finally, we compute a program slice for any given entity based on the established data flows. Running WebSlice on five real-world PHP systems, we found that out of 40,670 program slices, 10% cross languages, 38% cross files, and 13% cross string fragments, demonstrating the potential benefit of tool support for cross-language program slicing in web applications.|None|accept|accept
FSE|2015|CLAPP: Characterizing Loops in Android Applications|yanick fratantonio,aravind machiry,antonio bianchi,christopher krugel,giovanni vigna|static analysis,empirical study,mobile security|When performing program analysis, loops are one of the most important aspects that needs to be taken into account. In the past, many works have been proposed to analyze loops to perform different tasks, ranging from compiler optimization to Worst-Case Execution Time (WCET) analysis. While these approaches are powerful, they focus on tackling very specific categories and known patterns of loops, such as the ones for which the number of iterations can be statically determined. In this work, we developed a static analysis framework to characterize and analyze generic loops, without relying on techniques based on pattern matching. For this work, we focus on the Android platform, and we implemented a prototype, called CLAPP, that we used to perform the first large-scale empirical study on the usage of loops in Android applications. In particular, we used our tool to analyze a total of 4,110,510 loops found in 15,240 Android applications. As part of our evaluation, we provide the detailed results of our empirical study, we show how our analysis was able to determine that the execution of 63.28% of the loops is bounded, and we discuss several interesting insights related to the performance and security aspects.|None|accept|accept
FSE|2015|Can Novice Developers Program Defect-free? An Empirical Study|guoping rong,he zhang,dong shao,qi shan|PSP (personal software process),defect-free programming,controlled experiment|Quality of software intensive systems is the priority concern and focus in industry and research community. In practice, the increasing demand for experienced software developers in industry requires novice developers mature themselves in a timely manner to be able to produce high quality program. It has become a realistic challenge to both software engineering educators and researchers. To address this challenge, we devised the PSP+ process, in particular for novice developers, that enhances the original PSP (Personal Software Process) with the ultimate goal at defect-free programming. Based on the original PSP, the enhanced process (PSP+) incorporates a set of explicitly defined practices to facilitate experience sharing among developers with the concern on defect-free programming. This paper elaborates the proposed PSP+ process and reports a controlled experiment that was designed and executed to investigate the effectiveness of PSP+ within an educational setting. The experiment results indicate that students using PSP+ are more likely to perform defect-free programming without extra effort. They also gain higher confidence with defect-free programming compared to those using the original PSP.|None|reject|reject
FSE|2015|Synthesizing Tests for Detecting Atomicity Violations|malavika samak,murali krishna ramanathan|atomicity violation,concurrency,dynamic analysis,test synthesis|Thread-safe libraries can help programmers avoid the complexities of multithreading. However, designing libraries that guarantee thread-safety can be challenging. Detecting and eliminating atomicity violations when methods in the libraries are invoked concurrently is vital in building reliable client applications that use the libraries. While there are dynamic analyses to detect atomicity violations, these techniques are critically dependent on effective multithreaded tests. Unfortunately, designing such tests is non-trivial. In this paper, we design a novel and scalable approach for synthesizing multithreaded tests that help detect atomicity violations. The input to the approach is the implementation of the library and a sequential seed testsuite that invokes every method in the library with random parameters. We analyze the execution of the sequential tests, generate variable lock dependencies and construct a set of three accesses which when interleaved suitably in a multithreaded execution can cause an atomicity violation. Subsequently, we identify pairs of method invocations that correspond to these accesses and invoke them concurrently from distinct threads with appropriate objects to help expose atomicity violations. We have incorporated these ideas in our tool, named INTRUDER, and applied it on multiple open-source Java multithreaded libraries. INTRUDER is able to synthesize 33 multithreaded tests across nine classes in less than two minutes to detect 58 harmful atomicity violations, including previously unknown violations in thread-safe classes. We also demonstrate the effectiveness of INTRUDER by comparing the results with other approaches designed for synthesizing multithreaded tests.|None|accept|accept
FSE|2015|Distributed Model Transformation on MapReduce|amine benelallam,abel gomez-llana,massimo tisi,jordi cabot|Model Transformation,Distributed Computing,MapReduce|Efficient processing of very large models is a key requirement for a wider adoption of Model-Driven Engineering (MDE) in industrial contexts.  One of the central operations in MDE is rule-based Model Transformation (MT). However, MT languages are facing issues on both memory occupancy and execution time while dealing with the increasing model size and complexity. One way to overcome these issues is to exploit the wide availability of distributed clusters in the Cloud for the distributed execution of MT. In this paper, we propose an approach to automatically distribute the execution of model transformations written in a popular MT language, ATL, on top of a well-known distributed programming model, MapReduce. We show how the execution semantics of ATL can be aligned with the MapReduce computation model. We describe the extensions to the ATL transformation engine to enable distribution, and we experimentally demonstrate the scalability of this solution.|None|reject|reject
FSE|2015|CrowdService: Serving the Individuals through Mobile Crowdsourcing and Service Composition|xin peng,wenyi qian,tian huat tan,jun sun,jingxiao gu,wenyun zhao|mobile crowdsourcing,service composition,reliability,Quality of Service (QoS)|Some user needs in real life can only be accomplished by leveraging the intelligence and labor of other people. Existing crowdsourcing platforms cannot support the seamless integration of reallife tasks with computational services. In this paper, we develop a framework named CROWDSERVICE which supplies crowd intelligence and labor as publicly accessible crowd services and supports the integration of computational services and crowd services. A crowd service meets specific individual needs via mobile crowdsourcing which is wrapped as a computational service. To cope with the highly dynamic and uncertain nature of crowd services, CROWDSERVICE dynamically synthesizes/updates constraints on each involved computational service and crowd service. When a crowd service is to be executed, CROWDSERVICE generates an open call to potential workers and selects an optimal set of workers from those respond based on the synthesized constraints. We develop an implementation of CROWDSERVICE on the Android platform and evaluate it with both a user study and computer simulation. The results confirm the feasibility and scalability of providing reliable crowd services and applying them for individual needs in real life.|None|reject|reject
FSE|2015|Mimic: Computing Models for Opaque Code|stefan heule,manu sridharan,satish chandra|Synthesis,Search,Program Analysis|Opaque code, which is executable but whose source is unavailable or hard to process, can be problematic in a number of scenarios, such as program analysis.  Manual construction of models is often used to handle opaque code, but this process is tedious and error-prone. (In this paper, we use model to mean a representation of a piece of code suitable for program analysis.)  We present a novel technique for automatic generation of models for opaque code, based on program synthesis.  The technique intercepts memory accesses from the opaque code to client objects, and uses this information to construct partial execution traces. Then, it performs a heuristic search inspired by Markov Chain Monte Carlo techniques to discover an executable code model whose behavior matches the opaque code.  Native execution, parallelization, and a carefully-designed fitness function are leveraged to increase the effectiveness of the search.  We have implemented our technique in a tool Mimic for discovering models of opaque JavaScript functions, and used Mimic to synthesize correct models for a variety of array-manipulating routines.|None|accept|accept
FSE|2015|Rule-based Extraction of Goal-Use Case Models from Text|tuong huan nguyen,john c. grundy,mohamed almorsy|Goal-Use Case modeling,extraction,semantic parameterization|Goal and use case modeling has been recognized as a key approach for understanding and analyzing requirements. However, in practice, goals and use cases are often buried among other types of contents in requirements specifications documents and written in unstructured styles. It is thus a time-consuming and error-prone process to identify such goals and use cases. In addition, having them embedded in natural language documents greatly limits the possibility of formally analyzing the requirements for problems. To address these issues, we have developed a novel rule-based approach to automatically extract goal and use case models from natural language requirements documents. Our approach is able to automatically categorize goals and ensure they are properly specified. We also provide automated semantic parameterization of artifact textual specifications to promote further analysis on the extracted goal-use case models. Our approach achieves 85% precision and 82% recall rates on average for model extraction and 88% accuracy for the automated parameterization|None|accept|accept
FSE|2015|Generating Performance Distributions via Probabilistic Symbolic Execution|bihuan chen,yang liu,wei le|performance analysis,symbolic execution,probability analysis|Analyzing performance and understanding the potential best-case, worst-case and distribution of program execution times are an important software engineering task. There have been model-based as well as program analysis-based approaches for performance analysis. Model-based approaches rely on analytical or design models derived from mathematical theories or software architecture abstraction, which are coarse-grained and can be imprecise. Program analysis-based approaches typically aim to generate test inputs and profiling information to identify performance bottlenecks. They fail to capture the overall performance of a program. In this paper, we present PerfPloter, a performance analysis framework that takes program source code and usage profile as inputs and generates a performance distribution of the program, including the best-case and worst-case execution times. It first applies probabilistic symbolic execution to explore high-probability and low-probability paths. Once a path is explored, it generates and executes test inputs to model the performance of the path. Finally, it generates the performance distribution for the program. We have implemented our framework using Symbolic PathFinder and demonstrated using experiments that PerfPloter can accurately predict the distribution, best-case and worst-case of program execution times.|None|reject|reject
FSE|2015|Multi-Objective Test-Suite Prioritization for Time-Aware Product-Line Testing|hauke baller,isabella urbanke,johannes burdek,malte lochau,ina schaefer|Product-Line Testing,Test Suite Optimization,Search-based Optimization|Software product lines comprise families of similar program variants derived from a common code base, whose variable parts are defined in terms of features. Unfortunately, testing every member of real-world product-lines individually is impracticable due to the large number of derivable program variants. Most recent strategies for efficient product-line testing use (1) sampling, e.g., combinatorial feature interaction coverage, to reduce the number of variants under test, and/or (2) similarity-based prioritization to detect interaction faults as early as possible. However, those strategies constitute pure black-box approaches, neither taking test artifacts within the solution space, nor additional fine-grained domain knowledge into account. In this paper, we present a search-based multi-objective test-suite prioritization approach for efficiently covering entire product-line implementations in the presence of limited testing-time budgets. The approach is independent of the concrete testing methodology applied. Our experimental results, obtained from product-lines of varying size, show remarkable improvements concerning effectiveness of product-line testing.|None|reject|pre-reject
FSE|2015|What Code Implements Such Feature? A Behavior Model Based Feature Location Approach|guangtai liang,yabin dang,hao chen,lijun mei,shaochun li,yi-min chee|Behavior models,feature requests,feature-relevant code entries,feature-relevant code units|Enterprises today are keen to unlock new business values from their applications through embracing systems of engagement with mobile, social, cloud and analytics techniques. To accelerate agile transformations from their existing application portfolio, automatic feature location techniques play an important role for the rapid locating and understanding of implementations corresponding to features of interests (e.g., features to transform or improve). Existing feature location techniques [1-10, 32] provide a good foundation for code understanding and analysis. However, we observe some key limitations when applying them in real-world systems: the inadequacy about considering only single information source (e.g., code repository), the insufficiency of internal behavior modeling, and the ineffectiveness in prioritizing feature-relevant code entries. Such limitations are major obstacles for highly automated transformation. To address the limitations of existing techniques, we propose a behavior model based feature location approach. In this approach, we apply Natural Language Processing (NLP) techniques and static code analysis to extract “behavior models” of code units via considering multiple information sources. During the runtime, given a feature request, BMLocator first extracts its behavior model and then recommend feature relevant code units and entries by matching its behavior model with code units under analysis. Through evaluations on eight public requests from open-source projects (e.g., Tomcat, Hadoop), we demonstrate that our approach is more effective in recommending feature relevant code entries (e.g., most of entries are prioritized as the first ones) than existing techniques (i.e., TopicXP[37], CVSSearch[6]).|None|reject|reject
FSE|2015|A Deep Neural Network Language Model for Java Source Code|hoan anh nguyen,tien n. nguyen|Deep neural network-based language model,Code suggestion and completion,Syntactic and semantic contexts|N-gram statistical language model has been applied in software engineering to capture programming patterns in source code and to support code completion/suggestion. Despite its success, the n-gram language model (LM) has the key weakness on taking only a window of prior lexical tokens as the history, thus having a limited context for suggestion. In this paper, inspired by the success of Deep Neural Network (DNN) in natural language processing, we present Dnn4C, a DNN language model for source code that complements the local history of lexical tokens with both syntactic and semantic contexts for higher accuracy. With DNN, Dnn4C captures the discriminative information on features to learn patterns at higher levels of abstraction. Our novel incorporating method using syntactic and semantic annotations via a training objective with DNN allows Dnn4C to learn to distinguish the lexical tokens in different syntactic and semantic contexts. Our empirical evaluation on several real-world projects shows that Dnn4C relatively improves up to 43.5% and 26% top-1 accuracy over the n-gram LM and a state-of-the-art semantic LM for source code. The result also shows that our incorporating technique for syntactic/semantic contexts improves 3.2–19.1% accuracy than an existing incorporating technique using Bayesian inference.|None|reject|reject
FSE|2015|Tackling the Deployment Problem of Embedded Systems|stefan kugele,gheorghe pucea,ramona popa,laurent dieudonne,horst eckardt|deployment,design space exploration,domain-specific language,embedded systems,optimization|The quality of today's embedded systems e.g. in vehicles, airplanes, or automation plants is highly influenced by their architecture. In this context, we study the so-called deployment problem. The question is where (i.e., on which  execution unit) to deploy which software application or which sensor/actuator shall be connected to which device in an automation plant. First, we introduce language extensions to our domain-specific optimization and constraint language fitting the needs of our partners. Second, we investigate different approaches to tackle the deployment problem even for industrial size systems. Therefore, we developed different solving strategies using (i) multi-objective evolutionary algorithms, (ii) an SMT-based approach, and (iii) an ILP-based solving approach. Furthermore, a combination of them is used. We investigate the proposed methods and demonstrate their feasibility using two realistic systems: a civil flight control system (FCS), and a seawater desalination plant. The findings of our experiments are very promising.|None|reject|pre-reject
FSE|2015|DSpot: Test Amplification for Automatic Assessment of Computational Diversity|benoit baudry,simon allier,marcelino rodriguez-cancio,martin monperrus|test amplification,software diversity,source code transformation|In this work, we characterize a new form of software diversity: the existence of a set of variants that (i) all share the same API, (ii) all behave the same according to an input-output based specification and (iii) exhibit observable differences when they run outside the specified input space. We quantify computational diversity as the dissimilarity between execution traces on inputs that are outside the specified domain. Our technique relies on test amplification. We propose source code transformations on test cases to explore the input domain and systematically sense the observation domain. We run our experiments on 472 variants of 7 classes from open-source, large and thoroughly tested Java classes. Our test amplification multiplies by ten the number of input points in the test suite and is effective at detecting software diversity.|None|reject|pre-reject
FSE|2015|Clotho:Saving Programs from Malformed Strings and Incorrect String-handling|aritra dhar,rahul purandare,mohan dhawan,suresh rangaswamy|Automatic Program Repairing,runtime exception,software patching,Hybrid Program Analysis,Strings|Programs are susceptible to malformed data coming from untrusted sources. Occasionally the programming logic or constructs used are inappropriate to handle all types of constraints that are imposed by legal and well-formed data. As a result programs produce unexpected results or even worse, they may crash. Program behavior in both of these cases is highly undesirable. In this paper, we present a novel hybrid approach that saves programs from crashing when the failures originate from malformed strings or inappropriate handling of strings. Our approach statically analyses a program to identify statements that are vulnerable to failures related to associated string data. It then generates patches that are likely to satisfy constraints on the data, and in case of failures produce program behavior which would be close to the expected. The precision of the patches is improved with the help of a dynamic analysis. The patches are activated only after a failure is detected, and the technique incurs no runtime overhead during normal course of execution, and negligible overhead in case of failures. We have experimented with Java String API, and applied Clotho to several hugely popular open-source libraries to patch 30 bugs, several of them rated either critical or major. Our evaluation shows that Clotho is both practical and effective. The comparison of the patches generated by our technique with the actual patches developed by the programmers in the later versions shows that they are semantically similar.|None|accept|accept
FSE|2015|The Uniqueness of Public Identifiers in Libraries: the Case of C and Java|tetsuya kanda,daniel m. german,takashi ishio,raula gaikovina kula,katsuro inoue|program identifiers,library,software uniqueness|An identifier is a token that names entities in source code, such as functions and variables. Identifier names are one of the main resources for developers to understand what the software is doing. In this paper, we explore the uniqueness of public identifiers in libraries. If a public identifier is defined only in one library, this implies that given the identifier one can determine what library it belongs to. Our goal is to evaluate the uniqueness of public identifiers within a corpus of software libraries. We performed an empirical study of the corpus of two programming languages (C and Java) and determined that approximately 80% of public identifiers belong to only one single software library. We demonstrate the benefits of uniqueness by implementing fast and effective applications for detecting library copies and code snippet analysis.|None|reject|pre-reject
FSE|2015|AutoBuilder: Towards Automatic Building of Java Projects to Support Analysis of Software Repositories|xiaoying wang,shaikh mostafa|Software Building,Mining Software Repositories,Tools|Nowadays, many software developers choose to host their projects in open software repositories. These repositories have become valuable resources of software engineering research. Many emerging software engineering research efforts are based on analyzing and mining these repositories, or are evaluated on the software projects collected from these repositories. However, a major obstacle of effectively leveraging these repositories is that, most of repositories contain only source code, while a large portion of program analysis techniques target at object code only. Thus, most software engineering researchers have to either manually build a small number of software projects to enable object-code analysis on them, or use only source-code analysis techniques. In this paper, we propose to bridge this gap by automatic compiling and building software projects in software repositories. Specifically, we present a fully automatic building tool called AutoBuilder, that detects build configuration files, downloads dependencies, and resolves conflicts and incompatibilities. To evaluate our technique, we apply our approach on a randomly collected set of 1,000 software projects with 23 million lines of code from Github. The experimental results show that our approach is able to successfully build 58.0% of the projects and compile 59.0% of the code (in lines), compared with building 32.6% of the projects, and compiling 32.3% of the code (in lines) by a baseline approach.|None|reject|reject
FSE|2015|Improving Failure Detection by Generating Test Cases Near the Boundaries|min zhou,ke ma,ming gu,hongyu zhang,xiaoyu song|Random Testing,Memetic Algorithm,Failure Detection,Test Case Generation|Random testing is a simple yet effective method for software testing. It is generally accepted that adaptive random testing, which samples evenly within the input domain, has a larger probability of revealing failures than ordinary random testing. In this paper, we show that the performance of random testing can be further improved by considering the structure of the input domain. We propose a search-based method, which automatically samples test data along the boundaries of semantic regions in the input domain. Compared with conventional random testing techniques, our method can generate a smaller set of test data that reveal more faulty programs. The experiments on mutated programs show the effectiveness and efficiency of the proposed method.|None|reject|pre-reject
FSE|2015|Is My Failing Test Flaky?|lamyaa eloussi,darko marinov|flaky tests,non-determinism,coverage|Automated regression testing is widely used in modern software development. Whenever a developer pushes some changes to the repository, tests are run to check whether the changes broke some functionality. If some test fails, ideally it would be due to the latest changes, and the developer should debug them. Unfortunately, the test failure may not be due to the latest changes at all but due to non-deterministic tests, popularly called flaky tests, that can pass or fail even for the same code under test. Many projects have such flaky tests that make developers quickly lose confidence in the results of test runs. As a result, developers need techniques that help them determine whether a test failure is due to their latest changes and warrants their debugging, or whether it is due to a flaky test that should be potentially debugged by someone else. The most widely used technique to determine whether a test failure is due to a flaky test is to rerun the failing test multiple times immediately after it fails: if some rerun does pass, the test is definitely flaky, but if all reruns still fail, the status is unknown. We propose several improvements to this technique; (1) postponing the reruns, (2) rerunning in a new runtime environment (e.g., a new JVM for Java tests), and (3) intersecting the test coverage with the latest changes. We evaluate the cost of (1) and (2) and evaluate the applicability of (3) on 15 projects with a total of 2717 test classes, of which 10 contain previously known flaky tests. Our results show that the proposed improvements are highly applicable and would be able to find more flaky tests for the same or somewhat higher cost as rerunning immediately after failure.|None|reject|reject
FSE|2015|Scope-aided Test Suite Prioritization Because Not All Faults Are Created Equal|breno miranda,antonia bertolino|Fault Detection Rate,Test Coverage,Test Scope,Test Suite Prioritization|Many approaches for test suite prioritization have been proposed with the aim of accelerating fault detection when the test suite is re-executed. Among them, white-box approaches order test cases based on how the entities in the program flowgraph are covered. In this paper we introduce scope-aided prioritization that takes into account also the relevance of the entities to be covered with respect to possible constraints delimiting the input domain scope, and is proposed as a boost to existing approaches. Our approach provides a way to take into account fault relevance to the context scope, going towards multi-criteria prioritization as advised by several recent empirical studies. We apply here scope-aided prioritization to boost greedy total, greedy additional and similarity-based approaches. A first empirical evaluation shows that for such approaches in several cases we can improve the average rate of faults detected when considering faults that are in scope, while remaining competitive when all faults are considered equal.|None|reject|reject
FSE|2015|Raters' Reliability in Clone Benchmarks Construction|alan charpentier,jean-remy falleri,floreal morandat,laurent reveillere|Code clone,Empirical study,Software metrics|Context: Cloned code often complicates code maintenance and evolution and must therefore be effectively detected. One of the biggest challenge for clone detectors is to reduce the amount of irrelevant clones they found, called false positives. Several benchmarks of true and false positive clones have been introduced, enabling tool developers to compare, assess and fine-tune their tools. Nevertheless, such benchmarks are always manually build by raters that do not have any expertise of the underlying code, making their reliability questionable. Objective: Our goal is to investigate the reliability of rater judgments about clones. Method: We randomly select about 600 clones from two projects and ask several raters, including experts of the projects, to manually classify these clones. Results: We observe that judgments of non expert raters are not always repeatable. We also observe that they seldomly agree with each others and with the expert. Finally, we find that the project and the fact that a clone is a true or false positive has a significant influence on the agreement between the expert and non experts. Therefore, using non experts to produce clone benchmarks is unreliable.|None|reject|reject
FSE|2015|Comparing and Combining Test-Suite Reduction and Regression Test Selection|august shi,tifany yung,alex gyori,darko marinov|Regression test selection,Test-suite Reduction,Software evolution|Regression testing is a widely-used approach to check that changes made to software do not break existing function- ality. Unfortunately, regression test suites often grow at a rate that makes running them fully rather expensive. Re- searchers have proposed test-suite reduction and regression test selection as approaches to mitigate the growth of the test suite size by not running some of the tests in the test suite. However, previous research has not empirically eval- uated how the two approaches compare to each other, and how well a combination of these approaches could perform. This paper presents the first study that comprehensively compares test-suite reduction and regression test selection approaches individually. This paper is also the first to ask the question of whether the combination of the two ap- proaches results in significant advantages. We propose a new metric to compare the quality of tests with respect to software changes. Our large-scale experiments on 15 open- source projects show that regression test selection on average runs fewer tests by 37.46pp than using test-suite reduction would. We also find that a reduced test suite can result in a high loss in fault-detection capability if considering the changes made to software. We find that a combination of the two approaches leads to greater savings in tests to run, running on average 5.77pp fewer tests than using regression test selection would. However, these tests still suffer a loss in fault-detection capability with respect to changes.|None|accept|accept
FSE|2015|Breaking and Fixing Builds: When and Why Do They Happen?|michele tufano,fabio palomba,gabriele bavota,massimiliano di penta,rocco oliveto,andrea de lucia,denys poshyvanyk|Building System,Breaking Changes,Mining Software Repositories,Software Quality,Empirical Studies|In this paper we systematically study the phenomenon of build breaking and fixing changes in 219,395 snapshots of 100 Java projects from the Apache Software Foundation, all relying on Maven as an automated build tool. We investigated build breaking changes from four different perspectives: (i) how frequently they happen, (ii) what are likely causes behind them, (iii) when they occur and when they are fixed, and (iv) who are the developers introducing and fixing those. The results of the study suggest that build breaking changes occurred in most (96%) of the projects we studied, they were mainly due to problems related to dependencies and usually introduced large changes closer to releases. We also found that breaking changes tend to be quickly fixed often by the same developer who introduced them. However, the substantial number of breaking snapshots (median 30%) could still represent a threat for commit-based analysis of the project’s change history when this requires compiling and building each snapshot of a system.|None|reject|pre-reject
FSE|2015|Beyond API Signatures: An Empirical Study on Behavioral Backward Incompatibilities of Java Software Libraries|eric ruiz,shaikh mostafa,xiaoying wang|Empirical Study,Backward Incompatibilities,Bug Reports|To make sure that existing client software applications are not broken after a library update, backward compatibility has always been one of the most important requirements during the evolution of software libraries. However, due to various reasons, backward compatibility is seldom fully achieved in practice, so it is important to understand the status, major reasons, and impact of backward incompatibilities in real world software. Previous studies related to this topic mainly focus on API signature changes between consecutive versions of software libraries, while in this paper, we mainly consider behavioral changes of APIs. Speci- cally, we performed large-scale cross-version regression testing on 68 consecutive version pairs from 15 most popular Java software libraries, and inspected the detected test errors / failures. Furthermore, we collected and studied xxx real world software bugs caused by backward incompatibilities of software libraries. Our major ndings include: (1) more than 1,000 test failures / errors and 280 groups of behavioral backward incompatibilities are detected; (2) a large portion backward incompatibilities causing real-world bugs are related to user interface, which may be dicult to be detected by current automatic regression testing techniques; (3) the majority of backward incompatibilities are not well documented; and (4) there exists a number of x patterns for behavioral backward incompatibilities.|None|reject|pre-reject
FSE|2015|PersisDroid: Android Performance Diagnosis via Anatomizing Asynchronous Executions|yu kang,yangfan zhou,hui xu,michael r. lyu|performance diagnosis,mobile application performance,dynamic analysis,performance profiling|Rapid UI responsiveness is a key consideration to Android app developers. However, the complicated concurrency mod- el of Android makes it hard for the developer to understand and therefore diagnose the UI performance. This paper presents PersisDroid, a tool specically designed for An- droid UI performance diagnosis. The key notion of Per- sisDroid is that the UI-triggered asynchronous executions contribute to the UI performance, and hence their perfor- mance and their execution dependency should be properly captured to facilitate performance diagnosis. However, there are tremendous ways to start the asynchronous execution- s, posing a great challenge to proling such executions and their dependency. Fortunately, we nd out that asynchron- ous executions can be properly grouped into ve categories. As a result, they can be tracked and proled based on the specics of each category with a dynamic instrumentation approach carefully tailored for Android. PersisDroid can then accordingly prole the asynchronous executions in a task granularity, which equips it with low-overhead and high compatibility merits. We code and open-source release Per- sisDroid. The tool is applied in diagnosing 30 real-world open-source apps, and we nd 20 of them contain potential performance issues. It shows the eectiveness of our tool in UI performance diagnosis for Android apps.|None|reject|reject
FSE|2015|String Analysis for Java and Android Applications|ding li,yingjun lyu,mian wan,william g. j. halfond|String analysis,Static analysis,Java application|String analysis is critical for many verification techniques. However, accurately modeling string variables is a challeng- ing problem. Current approaches are generally customized for certain problem domains or have critical limitations in handling loops, providing context-sensitive inter-procedural analysis, and performing efficient analysis on complicated apps. To address these limitations, we propose a general framework for string analysis that allows researchers to more flexibly choose how they will address each of these challenges by separating the representation and interpretation of string operations. We implemented our approach as a prototype named Violist and evaluated it with different types of ap- plications. In our evaluation, we showed that our approach can achieve high accuracy on both Java and Android apps in a reasonable amount of time. We also compared our ap- proach to a popular and widely used string analyzer and found that our approach has higher precision and shorter execution time while maintaining the same level of recall.|None|accept|accept
FSE|2015|Incremental Assume-Guarantee Reasoning for Evolving Systems|fei he,shu mao|Assume-guarantee reasoning,Regression verification,Model checking,Learning algorithm,Evolving system|Assume-guarantee reasoning has been shown a promising technique for verifying large-scale systems. We consider in this paper the incremental assume-guarantee reasoning for evolving systems. A novel technique is proposed to reuse the assumptions generated in the previous reasoning run, thus leads to an incremental fashion of assume-guarantee reasoning. This paper contributes to the assume-guarantee reasoning by providing a new fine-grained learning-based verification technique. It also contributes to the regression verification by enabling its integration to the automated assume-guarantee reasoning. Experimental results suggest promising outlooks for our incremental technique.|None|reject|reject
FSE|2015|Practical Methods for Automatic MC/DC Test Cases Generation of Boolean Expressions|sekou kangoye,alexis todoskoff,mihaela barreau|MCDC,Tes cases Generation,Binary tree|Modified Condition/Decision Coverage (MC/DC) is a structural coverage criterion that aims to prove that all conditions involved in a Boolean expression can influence the result of that expression. In the context of automotive, MC/DC is highly recommended and even required for most security and safety applications testing. However, due to complex Boolean expressions that often embedded in those applications, generating a set of MC/DC compliant test cases for any of these expressions is a non trivial task and can be time consuming for testers. In this paper we present an approach to automatically generate MC/DC test cases for any Boolean expression. We introduce novel techniques, essentially based on binary trees to quickly and optimally generate MC/DC test cases for the expressions. Thus, the approach can be used to reduce the manual testing effort of testers.|None|reject|reject
FSE|2015|Automatic MC/DC Test Data Generation from Software Module Description|sekou kangoye,alexis todoskoff,mihaela barreau|Test Data Generation,MC/DC,Domain Specific Language|Modified Condition/Decision Coverage (MC/DC) is a structural coverage criterion that is highly recommended or required for safety-critical software coverage. Therefore, many testing standards include this criterion and require it to be satisfied at a particular level of testing (e.g. validation and unit levels). However, an important amount of time is needed to meet those requirements. In this paper we propose to automate MC/DC test data generation. Thus, we present an approach to automatically generate MC/DC test data, from software module description written over a dedicated language. We introduce a new merging approach that provides high MC/DC coverage for the description with only a little number of test cases.|None|reject|reject
FSE|2015|A Large-Scale Study On Repetitiveness, Containment, and Composability of Routines in Source Code|hoan anh nguyen,hoan anh nguyen,tien n. nguyen|Repetitiveness and containment and composability of routines,Large-scale study on source code,Repeated PDG and API usages|Source code in software systems has been shown to have a good degree of repetitiveness at the lexical, syntactical, and API usage levels. This paper presents a large-scale study on the repetitiveness, containment, and composability of source code at the semantic level. We collected a large data set consisting of 9,224 Java projects with 2.79M class files, 17.54M methods with 187M SLOCs. For each method in a project, we build the intra program dependency graph (PDG) to represent a routine, and compare PDGs with one another as well as the subgraphs within them. We found that within a project, 12.1% of the routines are repeated, and most of the repeated routines occur only 1–7 times. Across all projects, 3.3% of the routines are repeated mostly in 1–4 other projects. We also found that 26.1% and 7.27% of the routines are contained in other routine(s), i.e., implemented as part of other routine(s) elsewhere within a project and in other projects, respectively. Defining a subroutine via a per-variable slicing subgraph in a PDG, we found that 86.5% of the routines have at least one subroutine that has repeated elsewhere. 14.3% of them have all of their subroutines repeated. We also provide practical implications of our findings to automated tools.|None|reject|reject
FSE|2015|Abstraction of Formal Aspect-Oriented Model of Dynamically Changing Systems|yasuyuki tahara,akihiko ohsuga,shinichi honiden|dynamically changing systems,aspect-oriented programming,runtime verification,rewriting logic,reflection,model checking,abstraction|Aspect-oriented approaches have recently been used to model and implement dynamically changing systems such as self-adaptive systems. Although these approaches have some advantages, the system behaviors tend to become more complicated than conventional approaches such as dynamic architecture reconfiguration. As a result, it is difficult to apply formal verification because of the state space explosion problem. Abstraction is a well-known technique to cope with this problem. However, abstraction of aspect-oriented systems that dynamically change is not known so far. In this paper we propose a formal model of those systems to which we can apply abstraction. The main feature of our approach is the use of rewriting logic and the Maude specification language based on the logic. Our approach enables us to handle dynamic changes based on aspects by utilizing reflection that is an advanced feature of rewriting logic. In addition, we can use the abstraction functionality of rewriting logic for our model. We evaluate the applicability of our model to an example of specification of online storage service with cache.|None|reject|reject
FSE|2015|Barista: Effective Android Testing through Record/Replay|mattia fazzini,eduardo noronha de a. freitas,shauvik roy choudhary,alessandro orso|Android testing,Automated testing,Multi-platform testing|Nowadays, it is common to use and rely on mobile apps for many of our daily activities. For this reason, companies that provide apps to their users invest a great deal of resources in testing these apps. This is particularly important in the Android apps, which must run on a myriad of devices and versions of the operating systems. Unfortunately, as we confirmed in many interviews with QA professionals, app testing is today a very human intensive, and thus tedious and error prone, activity. To address this problem, and better support testing of Android apps, we propose a novel technique that allows for (1) recording interactions with an app in a minimally-intrusive way, (2) easily specifying expected results while recording, (3) automatically generating platform-independent test scripts based on the recorded interactions and the specified expected results, and (4) automatically running the generated test scripts on multiple platforms. We implemented our technique in a tool, Barista, and used it to evaluate the practical usefulness and applicability of our approach. Our results show that Barista can faithfully record and encode user defined test cases, including oracles, can generate test cases that run on multiple platforms, and can outperform a state-or-the-art-tool with similar functionality.|None|reject|reject
FSE|2015|Scout: Discovering Access Control Vulnerabilities within MongoDB-Backed Web Applications|shuo wen,yuan xue,jing xu,xiaohong li,wenli song,xiaowei li,guannan si|Access Control,Web Application Security,MongoDB,Black-Box Analysis|Access control is an extremely important and error-prone practice during web application development. The emergence of NoSQL databases and the flexible data models they bring impose new challenges on the implementation of access control within web applications. In this paper, we present Scout, a novel methodology for discovering access control vulnerabilities in existing web applications. Meanwhile (1) addressing features of NoSQL database and (2) requiring neither application source code nor server-side session information from the developers. We implement a prototype of Scout, which targets MongoDB-backed web applications. By automatically discovering the protocol layer in the web application stack, Scout introduces a data access operation model precisely representing the MongoDB actions performed in the web application, as well as inferring the access control policies. The prototype is shown to be able to identify comprehensive access control vulnerabilities in MongoDB-backed web applications, and generate detailed report as the facilitator to manually fix the identified vulnerabilities.|None|reject|reject
FSE|2015|Information Retrieval and Spectrum Based Bug Localization: Better Together|tien-duy b. le,richard j. oentaryo,david lo|Bug Localization,Information Retrieval,Program Spectra|Debugging often takes much efforts and resources. To help developers debug, numerous information retrieval (IR)-based and spectrum-based bug localization techniques have been proposed. IR-based techniques process textual information in bug reports, while spectrum-based techniques process program spectra. Both eventually generate a ranked list of program elements that are likely to contain the bug. However, these techniques only consider one source of information, either bug reports or program spectra, which is not optimal. To deal with the limitation of existing techniques, in this work, we propose a new multi-modal technique that considers both bug reports and program spectra to localize bugs. Our approach adaptively creates a bug-specific model to map a particular bug to its possible location, and introduces a novel idea of suspicious words that are highly associated to a bug. We evaluate our approach on 157 real bugs from four software systems, and compare it with a state-of-the-art IR-based bug localization method, a state-of-the-art spectrum-based bug localization method, and three state-of-the-art multi-modal feature location methods that are adapted for bug localization. Experiments show that our approach can outperform the baselines by at least 47.62%, 33.33%, 27.78%, and 28.80% in terms of number of bugs successfully localized when a developer inspects 1, 5, and 10 program elements (i.e., Top 1, Top 5, and Top 10), and Mean Average Precision (MAP) respectively.|None|accept|pre-accept
FSE|2015|Partition-based Coverage Metrics and Type-guided Search in Concolic Testing for JavaScript Applications|sora bae,joonyoung park,sukyoung ryu|coverage metrics,concolic testing,static analysis,JavaScript|JavaScript broadens its uses from client-side web applications to mobile and smart appliance applications, but testing JavaScript programs is not yet satisfactory.  While researchers have not spent much attention on testing JavaScript programs, existing coverage metrics and testing mechanisms for C and Java may not be applicable to JavaScript because of its quirky features like extremely dynamic semantics and exception flows.  Because, in JavaScript, any variable may have six kinds of types and most statements may throw exceptions, test cases with 100\% coverage levels in terms of the existing coverage metrics for statically typed languages may miss test cases revealing faults in JavaScript. In this paper, we identify characteristics of JavaScript that make thorough testing of JavaScript applications more difficult than testing C and Java programs.  To address such characteristics, we propose new coverage metrics that partition input spaces and control flows of programs using types and exception flows.  To generate effective test cases first, we develop prioritized search strategies for concolic testing using static analysis results.  We evaluate the new coverage metrics and search strategies for concolic testing with open-source Tizen sample web applications.  The experimental results show that our partition-based coverage metrics are practically usable in JavaScript concolic testing and the type-guided search strategies generate test cases that reveal bugs effectively.|None|reject|pre-reject
FSE|2015|A Paradigm to Code Smells Detection Based on Dendritic Cell Algorithm|tao lin,jianhua gao,xue fu,yan ma,yan lin|Software refactoring,dendritic cell algorithm,software bug,software quality,artificial immune systems,danger theory|Software refactoring is increasingly significant in software engineering, but also it is a fundamental task for the detection of code smells being indefinite and non-quantitative for the purpose of refactoring. This paper contend detection features, such as differentiation, antigen and signal processing. Essential concepts and signals are migrated to software engineering. The paper presents a detection paradigm, where the algorithm is based on dendritic cell algorithm in danger theory, which regards code smells as antigen. Software metrics values convert to the danger signal and the safe signal for processing, in which mature signal and semi-mature signal is calculated by weight equation. Code smells can be confirmed in comparison of relative values. Variety of code smells’ priority is determined by mature context antigen value. There are lower false positive rates in the paradigm. The experiment proves that this approach is competitive effectiveness in F-score(0.784) as well as Kappa analysis（0.756）and outperformance compared to other detection technique.|None|reject|withdrawn
FSE|2015|Extracting Code from Programming Tutorial Videos|shir yadid,eran yahav|big code,statistical language models,video,information extraction,code extraction,tutorials|The amount of programming tutorial videos on the web increases on a daily basis. Video hosting sites such as YouTube host millions of video lectures, with many programming tutorials for various languages and platforms. Automatically understanding the content of such videos is desirable for many purposes, including search, targeting of ads, and referrals to semantically related content. We present a novel approach for extracting code from videos. Our technique extracts and recognizes code directly from the video, and is based on the following ideas: 1.consolidating code across frames to improve precision of extraction, 2. a combination of statistical language models for applying corrections at different levels, allowing us to perform corrections by choosing the most likely token, combination of tokens that forms a likely line structure, and combination of lines that lead to a likely code fragment in the language. We have implemented our approach in a tool called ACE, and used it to extract code from 40 Android video tutorials on YouTube. Our experimental evaluation shows that ACE extracts code with high precision, enabling deep indexing of video tutorials.|None|reject|pre-reject
FSE|2015|Using Task Context to Suggest Fine-grained Source Code Changes|hoan anh nguyen,hoan anh nguyen,tien n. nguyen|Fine-grained source code change suggestion,Task context,Latent Dirichlet allocation|Prior research has shown that source code and its changes are repetitive. Several approaches have leveraged this information to detect change/fix patterns and support software engineering tasks. An important task among them is suggesting relevant changes/fixes. In this paper, we propose TasC, a novel statistical model that leverages the context of change tasks in software development history to suggest fine-grained code change/fix at the program statement level. We use latent Dirichlet allocation (LDA) to capture the task context where topics are used to model change tasks. We also propose a novel technique for measuring the similarity of code fragments and code changes using the task context. We conducted an empirical evaluation on a large dataset of 88 open-source Java projects containing more than 200 thousand source files and 3.5 million source lines of code (SLOCs) in their last revisions. In terms of changes, our dataset contains 88 thousand code revisions, of which 20 thousand are fixing changes. We extracted almost 500 thousand statement-level changes and almost 100 thousand statement-level fixes from 423 thousand changed methods with a total of 55 million AST nodes. Our result shows that TasC improves the suggestion accuracy relatively up to 130%–250% in comparison with the base models that do not use contextual information. Compared with other types of contexts, it outperforms the models using structural and co-change contexts.|None|reject|reject
FSE|2015|Integration Testing Based on Extended Interface Automata|zhang chao,xiaoying bai,jiaguang sun|integration testing,component-based,composition|This paper proposes a method modelling and testing component-based systems. A definition of formalism is given to model components and describe data operations and interface behaviours of components. Composition rules are defined to compose component models and calculate the producing model corresponding to the composed system of the components. Based on the models, a method of test case generation is proposed to generate test cases of integration testing.|None|reject|reject
FSE|2015|AppVerifier: Towards Multi-Source Analysis for Risk Assessment of Mobile Applications|le yu,xiapu luo,chenxiong qian|Android app,Privacy policy,Code analysis|Since more than 96\% of mobile malware targets on Android platform, various detection techniques based on permission analysis, static code inspection, and dynamic behavior monitoring have been proposed. Unfortunately, without well-defined signatures, it is non-trivial to differentiate between malware and benign apps because they may have the same functionality. Recent research suggested a promising approach that detects malware by checking whether it behaves as advertised. However these studies only use an app's description and required permissions to model the app's expected behaviors and real behaviors. We found that it is not sufficient to only use descriptions and permissions because of high false alerts rate. In this paper, we propose a multi-source analysis approach for risk assessment, which considers an app's privacy policy and code in addition to description and permissions. To handel these new sources and contrast them to identify inconsistency, we tackle a number of changeling issues and develop a new system, name AppVerifier, in 6,381 lines python code and 11,078 lines java code. The extensive evaluation using real apps shows that AppVerifier can accurately analyze privacy policies with $98.2\%$ precision and $97.7\%$ recall, and brings new insights.|None|reject|reject
ICSE|2015|A Theory of Software Complexity|arbi ghazarian|Software Complexity,Software Design,Design Decisions,Information Volume,Metrics,Theory|We propose an information volumetric measure of software complexity that, compared to previous measures, is both more accurate and robust to errors.|Reflections (on the past):|reject|reject
ICSE|2015|Effects of Sentiment on Code Review Outcomes|barbara russo,jeff carver,amiangshu bosu,taufan ardhinata|Sentiment analysis,Code Review,Text mining|Sentiment can affect the way developers  collaborate and influence the development outcome. New automated tools allows to mine text and evaluate sentiment in textual conversations. In this paper, we present a novel method to mine code review repositories and analyse the effect of sentiment of web professional conversations in software development. We have designed and applied a protocol to customise the dictionary used by a text mining tool for sentiment analysis. We have defined a measure to collect sentiment score in a code review pairs and evaluate the effect of sentiment on typical measures of code review outcome. We applied our novel method to three OSS projects. We found that sentiment  impacts on  time to review and rate of change request acceptance.|Visions (of the future):|reject|reject
ICSE|2015|k-bisimulation: a bisimulation for measuring the dissimilarity between processes|giuseppe lettieri,antonella santone,gigliola vaglini|formal mehods,equivalence checking,process similarity|We propose the use of a bisimulation to quantify dissimilarity between processes: we call it k-bisimulation. Two processes p and q, whose semantics is given trough transition sys- tems, are k-bisimilar if they differ from at most k moves, where k is a natural number. Roughly speaking, the k-bisimulation captures the extension of the dissimilarity between p and q when they are neither strong nor weak equivalent. The importance of a formal concept like as the k-bisimulation can be seen in several application fields, such as clone detection, process mining, business-IT alignment. Moreover, we give some hints on heuristics that can be used inside a procedure that efficiently checks that bisimulation. The approach can be applied to different specification languages (CCS, LOTOS, CSP) provided that the language semantics is based on the concept of transition systems.|Visions (of the future):|reject|reject
ICSE|2015|Software Modelling Deserves Language|brian henderson-sellers,cesar gonzalez-perez,owen eriksson,par agerfalk|Foundational ontologies,Metamodelling,Conceptual modelling|Contemporary software engineering modelling tends to rely on general-purpose languages, such as the Unified Modeling Language. However, such languages are practice-based and seldom underpinned with a solid theory – be it mathematical, ontological or concomitant with language use. The future of software modelling deserves a language base that is compatible with these various elements as well as being philosophically coherent.|Visions (of the future):|reject|reject
ICSE|2015|New Initiative: The Naturalness of Software|premkumar t. devanbu|Naturalness of Software,Large Corpus Statistics,New Initiative.|This paper describes a new research consortium, studying the Naturalness of Software. This initiative is supported by a pair of  grants by the US National Science Foundation, totaling $2,600,000: the first, exploratory (``EAGER'')  grant of $600,000 helped kickstart an inter-disciplinary effort, and demonstrate feasibility; a follow-on full  grant of $2,000,000 was recently awarded. The initiative is led by the author, who is at UC Davis, and includes investigators from Iowa State University and Carnegie-Mellon University (Language Technologies Institute).|Initiatives (recently funded research projects):|accept|accept
ICSE|2015|Explaining Agility with a Process Theory of Change|michael wufka,paul ralph|Process Theory,Agile Development,Agility|The term agile denotes two fundamentally different things: the ability to move and respond to change quickly and an anti-bureaucracy sociotechnical movement. Labeling various practices as agile undermines the responsibility to empirically evaluate whether they actually increase the ability to move and respond to change quickly. To untangle the overloading of “agile”, this paper formulates a preliminary process theory that may explain how software teams respond to change. The theory specifically adopts a dialectical approach, which explains change through evolving conflicts between actors with varying power levels.|Visions (of the future):|reject|reject
ICSE|2015|Language Engineering for Emergent Behavior: A Vision for Role-based Self-adaptive Software|somayeh malakuti|Emergent behavior,events,roles|Emergent behavior is generally defined as the appearance of complex behavior out of multiplicity of relatively simple interactions. In most cases of emergent behavior, there is a feedback process to which the constituents of emergent behavior must adapt by changing their roles. In this paper, we argue the need for dedicated module abstractions and programming languages for implementing software dealing with emergent behavior such that desired quality attributes such as reusability and maintainability are fulfilled in the implementations. Along this line, we identify the characteristic features of emergent behavior, and explain our proposed module abstractions named as (emergent )gummy modules. We outline our envision research directions to develop gummy modules for implementing self-adaptive software dealing with emergent behavior.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Is Requirements Engineering Inherently Counterproductive?|paul ralph,rahul mohanani|Requirements Engineering,Sensemaking,Problem Structuring,Domain Knowledge,Design|This paper explores the possibility that requirements engineering is, in principle, detrimental to software project success. Requirements engineering is conceptually divided into two distinct processes: sensemaking (learning about the project context) and problem structuring (specifying problems, goals, requirements, constraints, etc.). An interdisciplinary literature review revealed substantial evidence that while sensemaking improves design performance, problem structuring reduces design performance. Future research should therefore investigate decoupling the sensemaking aspects of requirements engineering from the problem structuring aspects.|Visions (of the future):|reject|reject
ICSE|2015|Variability-based Approach for Minimizing Under Design and Over Design|yucong duan,nanjangud narendra,xiaobing sun,honghao gao,yan tang,chengxiang ren|Under Design,Over Design,Software Economics,Variability Construction,Quality lifecycle|During the software development life cycle, there are gaps between technical decisions and targets of value creation processes. This gap usually originates in that they are considered separately in a project management perspective and a classic economic analysis, in which technical decisions and value creation means how to create optimal value. Thus, over Design and under design occur as two basic forms of negative variability construction that lead to both functional incompleteness and unexpected software quality degradation during design time and run time. In this paper, we proposed an approach called Problem-Quality-Constraint (PQC) as a solution strategy to help minimize the OD and UD problems in quality driven/aware software modeling. PQC is a constraint based modeling solution to manage variability towards satisfying process and product quality requirements, which helps minimize over design and under design, thereby maximizing overall process and product quality.|Visions (of the future):|reject|reject
ICSE|2015|Modularity for Uncertainty|takuya fukamachi,naoyasu ubayashi,shintaro hosoai,yasutaka kamei|Modularity,Uncertainty,Model-Driven Development|Uncertainty can appear in all aspects of software development: uncertainty in requirements analysis, design decisions, implementation, and testing. As the research on uncertainty is so young, there are many issues to be tackled. "Modularity for Uncertainty" is one of them and the most important issue. If uncertainty can be dealt with modularly, we can add or delete uncertain concerns to/from models, code, and tests whenever these concerns arise or are fixed to certain concerns. To deal with this challenging issue, we started a new research project "Model-Driven Development Embracing Uncertainty" from April 2014. This is the four-year project and is being conducted under the support of the Grant-in-aid for Scientific Research by the Ministry of Education, Culture, Sports, Science and Technology, Japan. Agile methods embrace "change" to accept changeable user requirements. On the other hand, our approach embraces "uncertainty" to support exploratory development. This paper sets out a focused research agenda for uncertainty in terms of the new modularity vision.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|An Interactive Visual Workflow Modeling Approach for Inspecting Software System Measurements|taimur khan,shah rukh humayoun,henning barthel,achim ebert,peter liggesmeyer|Software Comprehension,Software Maintenance,Software Measurement,Software Visualization,Query formulation.|In recent times, visual analysis is becoming more and more important especially in the area of software measurement, as most of the data from software measurement is multivariate. In this regards, standard software analysis tools are limited by their lack of ability to process huge collections of multidimensional data sets; current tools are designed to either support only well-known metrics or are too complicated to use in generating custom software metrics. Further, the analyst requires extensive knowledge of the underlying data schemata and the relevant querying language. To address these shortcomings, we propose an interactive visual workflow modeling approach that focuses on visual elements, their configurations, and inter-connectivity rather than the data ontology and querying language. In order to test and validate our methodology, we have developed a prototype tool called VIMETRIK (Visual Specification of Metrics). Our preliminary evaluation study illustrates the intuitiveness and easy-to-use means of our approach to understand software measurement and analysis data.|Visions (of the future):|reject|reject
ICSE|2015|Towards Explicitly Elastic Programming Frameworks|k. r. jayaram|elasticity,distributed applications,cloud computing|Elasticity is a key driver of cloud computing. It is a widely held view that software engineers should not be ``burdened'' with the responsibility of making their application components elastic; and that elasticity should be either be implicit and automatic in the programming framework; or that it is the responsibility of the cloud provider's operational staff (DevOps) to make distributed applications written for dedicated clusters elastic and execute them on cloud environments. In this paper, we argue the opposite -- we present a case for explicit elasticity, where software engineers are given the flexibility to explicitly engineer elasticity into their distributed applications. We present several scenarios where elasticity retrofitted to applications by DevOps is ineffective, present preliminary empirical evidence that explicit elasticity improves efficiency, and argue for elastic programming languages and frameworks to reduce programmer effort in engineering elastic distributed applications. We also present a bird's eye view of ongoing work on two explicitly elastic programming frameworks -- ElasticThrift (based on Apache thrift for cross-language elastic RPCs) and ElasticJava, an extension of Java with support for explicit elasticity.|Visions (of the future):|accept|accept
ICSE|2015|Light-Weight Rule-Based Test Case Generation for Detecting Buffer Overflow Vulnerabilities|bindu padmanabhuni,hee beng kuan tan|buffer overflows,vulnerability,detection,static analysis,test inputs,data and control dependency|Buffer overflow exploits form a substantial portion of input manipulation exploits as they are commonly found and are easy to exploit. Despite existence of many detection solutions, buffer overflow bugs are widely being reported in multitude of applications suggesting either inherent limitations in current solutions or problems with their adoption by the end-users. To address this, we propose a light-weight rule-based test case generation approach for detecting buffer overflows. The proposed approach uses information collected from static program analysis and pre-defined rules to generate test cases. Since the proposed approach uses only static analysis information and does not involve any constraint solving it is termed as light-weight. Preliminary experiments show that the test inputs generated by the proposed approach were effective in detecting known bugs along with reporting some new bugs.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|DICE: Quality-Driven Development of Data-Intensive Cloud Applications|giuliano casale|Big data,quality,reliability,efficiency,safety,Cloud computing|The rapid increase in demand for data-intensive Big Data applications calls for a new generation of software engineering methods. In particular, meeting quality goals for business-critical data-intensive applications that run on the cloud is a complex challenge. This paper describes the research agenda of DICE, a novel European collaboration that will define a quality-driven development methodology to accelerate the implementation and testing of data-intensive cloud applications. Building on the principles of model-driven development (MDD), the DICE research vision intends to define a novel MDD methodology to design and evaluate the quality properties of data and Big Data technologies in cloud applications. A quality engineering tool chain offering simulation, verification, and numerical optimization will use these extensions to drive the early design of the application and guide the evolution of its quality characteristics. DevOps-inspired methods for deployment, testing, continuous integration and monitoring feedback analysis will be adopted to accelerate the incorporation of quality in data-intensive cloud applications.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Highly Reliable Agile Formal Engineering Methods for Future Software Engineering|shaoying liu|Software development paradigm,requirements analysis and specification,software design,formal specification,specification patterns,specification animation,agile methods|In spite of the extensive efforts made by researchers and practitioners over the last forty years, software engineering is still facing tremendous challenges. Several well-recognized software development paradigms have been studied and applied, but unfortunately none of them can offer assurance in attacking the problem of software crisis. In this paper, we present a summary of our large five-year KAKENHI research project involving eight research groups from two national universities, two private universities and one national research institute in Japan, recently funded by the Ministry of Education, Culture, Sports, Science and Technology of Japan under Grant-in-Aid for Scientific Research A. The title of the grant is "Highly Reliable Agile Formal Engineering Methods", and its purpose is to develop a new software development paradigm with more effective technologies than the existing ones for future software engineering. We will focus our elaboration on the motives and potential solutions, and point out several challenging but realizable research topics for future study, not only by our own project, but hopefully also by other researchers in the field.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|A control-flow-aware analysis model for spectrum-based fault localization|zhiqiang zhang,jian zhang|spectrum-based fault localization,execution trace aligning,trace alignment diagram,test augmentation|Spectrum-based fault localization (SBFL) is a kind of fault localization techniques that analyse program execution profiles (usually statement coverage or branch coverage information) to compute the suspicious of program entities, and produce a ranked list of suspicious code. One common drawback of SBFL techniques is that they are usually based on hit-based or count-based spectra, which ignore a lot about the program structure, thus the ranked list lacks explanations about the bug. In this paper, we propose a novel analysis model for SBFL. Our model is based on aligning execution traces, which preserves the dynamic control flow information, and makes the bug diagnosis more explanatory. Our new analysis model is compatible with most existing SBFL techniques. It can also guide generating additional test cases to help improving the data quality for SBFL.|Reflections (on the past):|reject|reject
ICSE|2015|Comprehension-Driven Program Analysis for Malware Detection in Android Apps|suresh kothari,tom deering,ben holland,ahmed tamrawi,akshay deepak,sandeep krishnan,damanjit singh,vani bojja,ganesh ram santhanam,jeremias sauceda,jon mathews,nikhil ranade|Android Malware,Program Analysis,Program Comprehension,Graphical Analysis|This paper presents our recent research project funded by the DARPA Automated Program Analysis for Cybersecurity (APAC) initiative. It is a 42-month project with Iowa State University as the prime institution and EnSoft Corp as a subcontractor. The goal is to develop techniques and tools to keep malicious code out of DoD Android-based mobile application marketplaces. With multiple initiatives to set up application marketplaces in DoD underway, and numerous such marketplaces recently established in commercial industry, there is a clear and present demand for this technology and many potential avenues for technology transition. The adversarial challenge ("Red") teams in the DARPA APAC program are tasked with designing sophisticated malware to test the bounds of malware detection technology being developed by the research and development ("Blue") teams. Our research group, a Blue team in the DARPA APAC program, proposed a “human-in-the-loop program analysis” approach to detect malware given the source or Java byte code for an Android app. Our malware detection apparatus consists of two components: a general-purpose graphical program analysis platform called Atlas, and an Android-specific security toolbox built on the Atlas platform. Our project team includes the PI, a research scientist, three postdocs, four graduate students at Iowa State University, and a Co-PI and two senior engineers from EnSoft.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Mining Software Repositories for Social Norms|khanh hoa dam,bastin tony roy savarimuthu,daniel avery,aditya k. ghose|Social norms,Mining software repositories,open source software development communities|Social norms facilitate coordination and cooperation among individuals, thus enable smoother functioning of social groups such as the highly distributed and diverse open source software development (OSSD) communities. In these communities, norms are mostly implicit and hidden in huge records of human-interaction information such as emails, discussions threads, bug reports, commit messages and even source code. This paper aims to introduce a new line of research on extracting social norms from the rich data available in software repositories. Initial results include a study of coding convention violations in JEdit, ArgoUML and Glassfish projects. It also presents a new life-cycle model for norms in OSSD communities and demonstrates how a number of norms extracted from the Python development community follow this life-cycle model.|Visions (of the future):|accept|accept
ICSE|2015|Vision for Software Engineering Systems of Systems|vanea chiprianov,katrina falkner,claire ingram,richard payne|Systems of systems,Roadmap,Challenges,Life-cycle,Requirements,Architecture,Implementation,Verification,Release,Framework|One of the current major software and system challenges deals with complex systems, composed of other systems, called Systems of Systems (SoS). SoS comprise, among others, defence and national security, intelligent transportation systems, electric power grid. SoS have specific characteristics that differentiate them from monolithic systems, such as operational and managerial independence, evolutionary development, emergent behaviour and geographic distribution. These characteristics make it so that SoS challenges cannot be completely met by current paradigms, languages and tools for system development. In this vision paper we therefore identify challenges to engineering SoS and propose a roadmap and a framework for addressing them. This research agenda will structure the field of SoS and help researchers better position themselves and interact with other pieces of complementary research.|Visions (of the future):|reject|reject
ICSE|2015|Ramp-up Journey of New Hires: Do strategic practices of software companies influence productivity?|ayushi rastogi,suresh thummalapenta,thomas zimmermann,nachiappan nagappan,jacek czerwonka|Ramp-up journey,New hires,Strategic practices,Productivity,Software developers|Software companies regularly recruit skilled and talented employees to meet evolving business requirements. Although companies expect early contributions, new hires often take several weeks to reach the same productivity level as existing employees. We refer to this transition of new hires from novices to experts as ramp-up journey. There can be various factors such as lack of technical skills or lack of familiarity with the process that influence the ramp-up journey of new hires. The goal of our work is to identify those factors and study their influence on the ramp-up journey. As a first step towards our goal, this paper explores the impact of two strategic practices namely distributed development and internship on the ramp-up journey of new hires. Our results show that, initially, new hires who are in proximity to the core development team perform better than others. Similarly, new hires who had prior internship experience perform better than their counterparts during the beginning of their journey. However, in the overall ramp-up journey the effect of the two practices attenuates or becomes insignificant.|Visions (of the future):|reject|reject
ICSE|2015|Behavioral Types for Component-based Development of  Cyberphysical Systems|jan olaf blech,peter herrmann|behavioral types,cyber-physical systems,component-based development|Spatial behavioral types encode information on the tempo-spatial behavior of components acting in the physical space. That makes it possible to utilize the well established concept of type systems with its well studied benefits for  programming languages, e.g., fast automatic detection of incompatibilities and coercion, also in the cyber-physical world of domains such as embedded systems.So, spatial behavioral types support development and better maintenance of systems leading to a reduction of errors, improvement of safety and, in consequence, lower expenditure. In this paper, we summarize existing work and develop our ideas for a spatial behavioral type concept.|Visions (of the future):|reject|reject
ICSE|2015|GuideSE: Annotations for Guiding Concolic Testing|koushik sen,haruto tanno,xiaojing zhang,takashi hoshino|symbolic execution,testing,concolic|Dynamic symbolic execution or concolic testing has been proposed recently to effectively generate test inputs for real-world programs.  Unfortunately, dynamic symbolic execution techniques do not scale well for large realistic programs, because often the number of feasible execution paths of a program increases exponentially with the increase in the length of an execution path. We believe that programmers usually have a good understanding of programs they write and could help concolic testing to guide path exploration effectively so that concolic testing can achieve the desired testing goal quickly.  In this paper, we propose a simple annotation mechanism, called GuideSE, that enables a programmer to guide concolic testing by annotating the program under test.  We have currently identified two mechanisms for providing these annotations: control annotations and data annotations. Controls annotations allow a programmer to specify the paths of interest using regular expressions.  Data annotations allow a programmer to abstract portions of the program under test and to explore the reduced path space of the abstract program.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Serious Potential: Exploring Serious Games  in 21st Century Classrooms|rashina hoda,stuart mcnaughton,rebecca jesson,elaine reese,cynthia greenleaf|Serious Games,Education,21st Century Classrooms,Experiential Learning,Design,Learning Experience,Large-Scale Study|As digital technologies and gaming become pervasive, there is growing interest in exploring the potential of serious games in education. However, research into the design and use of serious games in mainstream classrooms is in its infancy. This paper presents a summary of a recently funded, large-scale, four-year, multi-disciplinary research initiative that involves the design of serious games to study their potential to achieve increased student engagement and participation, attain learning objectives, and overall, enhance student learning experiences in modern classrooms. The project focuses on cognitive skills such as critical thinking, critical literacy, and argumentation and social skills such as self-regulation, collaborative reasoning and pro-social skills. This paper presents an overview of our research project in terms of its aims, team composition and design and a discussion of foreseeable design challenges. It also discusses opportunities and proposes hypotheses around the potential of serious games in modern 21st century classrooms as we embark on this exciting journey.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Free Hugs — Praising Developers For Their Actions|roberto minelli,andrea mocci,michele lanza|gamification,IDE interactions,development session,visualization,vision|Developing software is a complex, intrinsically intellectual, and therefore ephemeral activity, also due to the intangible nature of the end product, the source code. There is a thin red line between a productive development session, where a developer actually {\em does} something useful and productive, and a session where the developer essentially produces ``fried air'', pieces of code whose quality and usefulness are doubtful at best. We believe that well-thought mechanisms of gamification built on fine-grained interaction information mined from the IDE can crystallize and reward good coding behavior. We present our preliminary experience with the design and implementation of a micro-gamification layer built into an object-oriented IDE, which at the end of each development session not only helps the developer to understand what he actually produced, but also praises him in case the development session was productive. Building on this, we envision an environment where the IDE reflects on the deeds of the developers and by providing a historical view also helps to track and reward long-term growth in terms of development skills, not dissimilar from the mechanics of role-playing games.|Visions (of the future):|accept|accept
ICSE|2015|Adaptability, Reliability, Causality, and Trust In Communication-Based Systems|ilaria castellani,mariangiola dezani-ciancaglini,rafael mayo garcia,thomas hildebrandt,robert jakob,ivan lanese,jorge andres perez,peter thiemann|behavioural types,business process models,adaptability,resilience,quantitative guarantee,qualitative guarantee|Dynamically evolving distributed systems arise in software construction, business process modeling, and biology. In this context, we propose a new, unified vision for modeling and analyzing self-adaptable systems of heterogeneous communicating processes.  Our vision puts forward qualified, self-adaptive, and probabilistic behavioral types as a new form of specification to express qualitative and quantitative guarantees for adaptive systems. We envision the resulting framework to serve as a new lingua franca for specifying systems from the above disciplines. Applying this framework should guarantee correctness, security, and resource usage bounds by construction. The new form of specification must be supported by a tool chain for modeling and simulating very large evolving computing infrastructures, highly dynamic business processes, and self-adapting biological systems as they occur in present-day practice. Furthermore, the framework must be evaluated on a range of applications from different domains.|Visions (of the future):|reject|reject
ICSE|2015|Leverage Mobile Crowdsourcing to Test Web Service Efficiently|minzhi yan,hailong sun,xudong liu,yili fang|Web service testing,mobile crowdsourcing,web service,set cover|In recent years, a lot of crowdsourcing systems have emerged and lead to many successful crowdsourcing systems like Wiki-pedia, Amazon Mechanical Turk and Waze. In the field of software engineering, crowdtesting has acquired increased interest and adoption, especially among personal developers and smaller companies. Meanwhile, improved mobile network techniques make crowdsourcing happen anywhere and anytime.  In this paper, we present iTest which combines mobile crowdsourcing and web service testing together to support the performance testing of web services. iTest is a framework for service developers to submit their web services and conveniently get the test results from the crowd testers. Firstly, we perform experiments to illustrate that both the way to access network and tester's location influence the performance of web service; secondly, the architecture of iTest framework is presented; then we introduce the workflow of testing web service in iTest and model the tester selection problem as Set Cover Problem, and propose an greedy algorithm for solving this problem. Finally, we conclude our work and provide the directions for future work.|Reflections (on the past):|reject|reject
ICSE|2015|Extending the H-index to Classify Faulty Modules|sandro morasca|Fault-proneness,Empirical study,H-index,Precision,Recall,F-measure|Background. Fault-proneness estimation models provide an estimate of the probability that a software module is faulty. These models can be used to classify modules as faulty or non-faulty, by using a fault-proneness threshold: modules whose fault-proneness exceeds the threshold are classified as faulty and the others as non-faulty. However, the selection of the threshold is to some extent subjective, and different threshold may lead to very different results in terms of classification accuracy. Objective. We introduce and empirically validate a new approach to setting thresholds, based on an extension of the H-index defined in Bibliometrics, called the Fault-proneness H-Index. We define and use this extension to identify the most fault-prone modules, which are candidates for intensive Verification & Validation activities. Method. We carried out the empirical validation on a data set from NASA hosted on the PROMISE repository, by using a technique similar to T-times K-fold cross validation. We computed  Precision, Recall, and the F-measure for the results obtained with our approach and compared them with the values obtained with other approaches. Results. In an initial empirical validation, our approach provides better Precision and F-measure results than most other thresholds, in a statistically significant way. Conclusions. Our approach seems to be able to contribute to accurately classifying modules as faulty or non-faulty.|Visions (of the future):|reject|reject
ICSE|2015|Putting Business Goals in Context for Measurement|luigi lavazza,sandro morasca,davide tosi|Software development process,Software process measurement,GQM,Domain representation|Effective software measurement in a business organization requires a deep understanding of the business context, i.e., the business world in which the organization operates. Thus, there is a need for describing the business world and placing business goals into their context, so sensible measurement plans can be defined and enacted. In this paper, based on Jackson’s ideas on domain representation and using concepts from the GQM+Strategies technique, we propose a method to precisely describe the business domain and its characteristics, the business goals, the strategies, their relationships with the software activities carried out to support the strategies, and how strategies are selected. Specifically, we propose a way to describe the business world first, including business and software processes, and then specify the measurements required.|Visions (of the future):|reject|reject
ICSE|2015|Applying Architecture Centrality in Software Product Line Development and Evolution|yongjie zheng|software architecture,architecture-centric development,software product line|A novel paradigm of architecture-centric software product line development and evolution is presented in this paper. It exploits the powers software architecture’s abstraction and software product line engineering’s planned reuse. The architecture-centric product line approach advocates that product line architecture be automatically processed to drive other development activities, including product line implementation and derivation of single products. A main challenge, automatic mapping of variability in PLA to code, is identified and specifically discussed in the paper. Promising solutions are also suggested.|Visions (of the future):|reject|reject
ICSE|2015|A New Vision for Debugging Production Systems using Live Cloning|nipun arora,gail e. kaiser|live cloning,debugging,linux containers,software monitoring|In this work we present a framework which allows users to debug a target production system (execution tracing, profile, breakpoint etc.) in a sandbox environment cloned from the live running system at any point in its execution.The paper leverages user-space containers (OpenVZ/LXC) to launch a container cloned and migrated from running instances of an application, thereby launching two containers: production (which provides the real output), and debug-container (for debugging).This debug-container provides a sandbox environment for safe application of instrumentation tools without any perturbation to the actual production environment. A customized-network proxy agent replicates inputs from clients to both the production and debug-container, as well as safely discards all outputs from the debug-container. We believe our tool provides a novel mechanism for practical live-debugging of large scale multi-tier and cloud applications, without requiring any application down-time, and minimal performance impact.|Visions (of the future):|reject|reject
ICSE|2015|LaunchPad: A Synthesis Framework for Feature-rich Applications|boris duedder,george t. heineman,jakob rehof|Software Engineering,Feature Systems,Product Line,Type Theory,Logic,Program Synthesis,Framework,Combinatory Logic Synthesis,Staged Composition|We present a new research program towards synthesis from prefabricated units of composition. Our method relies on a type-theoretical approach in which framework designers can formally document semantic rules of composition, and which can be exploited by framework extenders to generate extensions. We focus on applications to feature-rich applications, where rules of composition capture and automate high degrees of variability.|Visions (of the future):|reject|reject
ICSE|2015|Topological Configuration in Software Product Lines - The Case for Configuration References|holger eichelberger,klaus schmid|Variability modeling,Topological configuration,Domain-specific Languages|Variability modeling is at the heart of modern software product line engineering. While variability modeling traditionally focuses on hierarchical models, recent industrial case studies indicate that there is a need for complex variability organization models. In this paper, we suggest to use typed references to enable topologies as an extension to variability modeling. We discuss this in the context of the IVML language, an approach to variability modeling we developed over recent years, but the basic concepts could be integrated into different variability modeling approaches as well.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Algorithmic Diversity: A Novel Mechanism to Increase Robustness|vivek nallur,amal elgammal,siobhan clarke|Diversity,Robustness,Algorithmic diversity,Load Balancer|Robustness in software is a critical aspect of its quality. We introduce algorithmic diversity, a novel and yet simple mechanism to increase robustness in software, with very little modiﬁcation to the codebase. An initial evaluation on haproxy, an industrial-strength software, shows promising results.|Visions (of the future):|reject|reject
ICSE|2015|Summarizing Heterogeneous Information in Complex Development Artifacts|luca ponzanelli,andrea mocci,michele lanza|summarization,complex software artifacts,heterogeneous information|Summarization is hailed as a promising approach to reduce the amount of information that must be taken in by the person who wants to understand development artifacts, such as pieces of code, bug reports, emails, etc. However, existing approaches treat artifacts as pure textual entities, disregarding the heterogeneous and partially structured nature of most artifacts, which contain intertwined pieces of distinct type, such as source code, diffs, stack traces, human language, etc. We present a novel approach to augment existing summarization techniques (such as LexRank) to deal with the heterogeneous and multidimensional nature of complex artifacts. Our preliminary results on heterogeneous artifacts suggest that the reductionism of current approaches can be overtaken by our promising holistic approach.|Visions (of the future):|reject|reject
ICSE|2015|Commit Bubbles|titus barik,kevin lubick,emerson r. murphy-hill|version control,edit history,code bubbles,refactoring,integrated development environments|When working with version control systems, developers are expected to produce systematic commit histories that show well-defined steps with logical forward progress. Existing version control tools assume that developers also write code systematically and they can flawlessly produce commit histories that satisfy these expectations. In such a case, history revision would be rare. Yet, the process by which developers write source code is often evolutionary, or as-needed, rather than systematic.  Consequently, histories are rarely systematic without significant revision. Our insight is that by treating revision as a frequent and routine activity, developers can continue to work in their as-needed way, while simultaneously producing systematic histories. Our contribution is a fragment-oriented concept called Commit Bubbles that will allow developers to construct commit histories that adhere to version control best practices with less cognitive effort, and in a way that integrates with their as-needed coding workflows.|Visions (of the future):|accept|accept
ICSE|2015|Integrating Personalized Development History into Developers Recommendation|hui yang,xiaobing sun,yucong duan,bin li|Collaborative Topic Modeling,Change Request,Developer Recommendation,Source Code Recommendation,Software Maintenance|Given a change request, software development managers must find the right developers to implement the right developers to implement it in the right place in the source code. Current approaches to resolve this issue mainly focus on recommending developers who are the most suitable to implement the change requests. However, different developers have different development developing habits, programming skills and experiences. They can accomplish the change request in their own way to accomplish a change request. If some relevant source code files fit to developers’ common developing habit and programming experience, the efficiency of implementing the change request can be improved. In this paper, we propose a novel approach to assign an incoming change request to appropriate developers as well as recommending the source code relevant to the change request based on their historical development habit. Our approach uses the collaborative topic modeling (CTM) technique to help personalize the developers’ tasks.|Visions (of the future):|reject|reject
ICSE|2015|Constraint-based Multi-tenant Cloud Deployment Using Feature Modelling and XML Filtering Techniques|yang cao,chung-horng lung,samuel ajila|Feature Modelling,XML Filtering,Cloud Deployment|Abstract—The challenges of making cloud deployment decisions for software systems include a large configuration space, difficulty in managing the tradeoffs between non-functional requirements and multi-tenancy. In addition, making deployment decisions manually for multi-tenant cloud systems is very difficult and the computational complexity of existing approaches is high. In this work, we propose a new approach to multi-tenant cloud deployment using feature modelling to capture functional and non-functional requirements and other constraints. The features of a cloud system and tenant-specific systems are encoded with XPath feature representations and XML format. We adopt Yfilter, an established XML filtering and matching technique to match cloud configurations that satisfy tenant-specific requirements and constraints. The experimental results demonstrate that the proposed approach can automatically and correctly identify cloud system configurations that match a specific tenant’s requirements. In addition, the execution time for our approach is only a small fraction compared to the existing approach, e.g., FaMa, and the configuration space is smaller.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Towards a Software Ecosystem Evolution Maturity Model|simone da silva amorim,john mcgregor,eduardo almeida,christina von flach g. chavez|Software ecosystems,Software evolution,Maturity model|Many software ecosystems have achieved success in recent years. However, there is not an accepted model to categorize their evolution or even guide organizations to know how these ecosystems should be expected to evolve. This paper proposes a maturity model for the evolution of software ecosystems. We illustrate an initial representation summarizing the levels of maturity and their relations with three views of software ecosystems: Social, Business and Technical. We also describe a research approach to be used in the construction of our model.|Visions (of the future):|reject|reject
ICSE|2015|Towards Data-driven Software Engineering Skills Assessment|jun lin,han yu,zhiqi shen,chunyan miao|Data-driven Software Engineering,Agile Project Management,Soft Skills Assessment|Today’s software engineers often work in teams to develop complex software systems. Therefore, successful software engineering in practice not only require team members to possess sound programming skills such as analysis, design, coding, and testing, but also soft skills such as communication, collaboration, and self-management, etc. However, existing examination based assessments are often inadequate for quantifying students’ soft skill development. In this paper, we explore alternative ways for assessing software engineering students’ skills through a data- driven approach. Leveraging our proposed HASE online agile project management (APM) tool, we conducted a study involving 21 Scrum teams consisting of over 100 undergraduate software engineering students in multi-week coursework projects in 2014. During this study, students performed over 10,000 software engineering activities logged by HASE. By analyzing the collected activity trajectory dataset, we demonstrate the potential for this new research direction to enable software engineering educators to have a quantifiable way of understanding their students’ skill development, and take a proactive approach in helping them improve their programming and soft skills.|Visions (of the future):|reject|reject
ICSE|2015|The Software Engineering Field Repeats Decades-Old Mistakes - The Danger of Using Hard Solutions to Soft Problems and the Agile Software Development Context.|lucas gren|Agile Development Processes,Organizational Development,Organizational Culture,Problem Solving|The organization is like an iceberg and the formal processes are only the peak above the surface. In order to understand and predict what happens we need to first find a useful definition of organizational culture as a description of how it affects people in the organization. This paper presents another way of sorting different types of organizational problems. To use ``hard'' solutions (to only implement practices) for ``soft/messy'' problems (need for cultural change) often results in disaster. In conclusion, this paper argues that strategies for dealing with the kind of organizational development -- that an agile transition is -- already exist, and software engineering should make use of these theories instead of making the same mistakes again.|Reflections (on the past):|reject|reject
ICSE|2015|Distance-integrated Combinatorial Testing|eun-hye choi,cyrille valentin artho,takashi kitamura,akihisa yamada,yutaka oiwa|Combinatorial testing,N-wise testing,N-wise coverage,Hamming distance|We propose a novel approach to the construction of combinatorial N-wise tests that achieves an increase of not only the number of new combinations but also the distance between test cases. Experimental results show that our distance-integrated combinatorial test generation can provide better test effectiveness by improving interaction coverage with higher combinatorial strength.|Reflections (on the past):|reject|reject
ICSE|2015|Scalable Hybrid Variability for Distributed Evolving Software Systems|daniele costa,ferruccio damiani,maurizio griva,einar broch johnsen,ina schaefer,amund tveit,ingrid chieh yu|Cloud computing,Data intensive systems,Distributed software,Internet of things,Over-the-air updates,Software engineering,Software evolution,Software maintenance,Software product lines,Variability models|Software today is increasingly often upgraded after deployment by means of software patches. At the same time, the software is increasingly individualized, adapted to the preferences and needs of the specific customer. In the future, we may expect that also the upgrades themselves will be increasingly individualized; e.g., the software patches used to upgrade the software are selected and adapted depending on the configuration and external constraints of the host device. HyVar is a new European research project which proposes a development framework for continuous and individualized evolution of distributed software applications running on remote devices in heterogeneous environments. The framework will combine variability modeling from software product lines with formal methods and software upgrades, and be integrated in existing software development processes. HyVar goes beyond the state-of-the-art by proposing hybrid variability;  i.e., the automatic generation and deployment of software updates combines the variability model  describing  possible software configurations  with sensor data collected from the device. HyVar's scalable cloud infrastructure will elastically support monitoring and customization for numerous application instances. Software analysis will exploit the structure of the variability models. Upgrades will be seamless and sufficiently nonintrusive to enhance the user quality experience, without compromising the robustness, reliability and resilience of the distributed application instances.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Entity Identification Problem in Big and Open Data. Intelligent Reconciliation Platform and Virtual Graphs|vivian lee,jose gonzalez enriquez,masatomo goto,francisco jose dominguez mayo,maria jose escalona cuaresma|Software Engineering,Big Data,Open Data,Entity Identification,Intelligent Reconciliation,Virtual Graphs|Big and Open Data provide great opportunities to businesses to enhance their competitive advantages if utilized properly. However, during past many years of research in Big and Open Data processing, we have encountered big challenge in entity identification reconciliation, when we are trying to bridge accurate relationships between entities that are portrait by different data sources. In this paper, we present our innovative Intelligent Reconciliation Platform and Virtual Graphs solution that addresses this issue. With this solution, we are able to efficiently extract Big and Open Data from heterogeneous source, and integrate them into a common analyzable format. Further enhanced by the Virtual Graphs technology, entity identification reconciliation is processed dynamically to produce more accurate result at system runtime. We also conclude that this technology is applicable to a big diversity of entity identification problems in several domains, for example, e-Health, cultural heritage, and company identities in financial world.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|A topological approach to network coding|cristina martinez,alberto besana|Automorphism group,partition graph,network topology|We study graph representations of networks and how network topology determines porperties of networks and applications to data analysis and engineering.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Fast Feedback Cycles in Empirical Software Engineering Research|antonio vetro',saahil ognawala,daniel mendez fernandez,stefan wagner|Empirical methods,Research methods,Data mining,Knowledge transfer|Background/Context: Gathering empirical knowledge is a time consuming task and the results from empirical studies often are soon outdated by new technological solutions. As a result, the impact of empirical results on software engineering practice is often not guaranteed. Objective/Aim: In this paper, we summarise the ongoing discussion on ”Empirical Software Engineering 2.0” as a way to improve the impact of empirical results on indus- trial practices. We propose a way to combine data mining and analysis with domain knowledge to enable fast feedback cycles between researchers and practitioners. Method: We identify the key concepts on gathering fast feedback in empirical software engineering by following an experience-based line of reasoning by argument. Based on the identified key concepts, we design and execute a small proof of concept with a company, to demonstrate potential benefits of the approach. Results: In our example we observed that a simple double feedback mechanism notably increased the precision of the data analysis and improved the quality of the knowledge gathered. Conclusion: Our results serve as a basis to foster discus- sion and collaboration within the research community for a development of the idea.|Visions (of the future):|accept|accept
ICSE|2015|Configuring Latent Semantic Indexing for Requirements Tracing|sebastian eder,henning femmer,benedikt hauptmann,maximilian junker|Requirements Tracing,Traceability Link Recovery,Latent Semantic Indexing|Latent Semantic Indexing (LSI) is an accepted technique for information retrieval that is used in, e.g., requirements tracing, to recover links between artifacts. However, configuring LSI is difficult, because the number of possible configurations is huge. The configuration of LSI, which depends on the underlying dataset, greatly influences the accuracy of the results. Therefore, one of the key challenges for applying LSI-based methods is finding an appropriate configuration. Evaluating results for each configuration is time consuming, and therefore, automatically determining an appropriate configuration for LSI improves the applicability of LSI based methods. We propose a fully automated technique to determine appropriate configurations for LSI to recover links between requirements artifacts. We evaluate our technique on six sets of industrial requirements artifacts and show that the configurations selected by our approach yield nearly optimal results.|Reflections (on the past):|reject|reject
ICSE|2015|Systematic Mapping of Security Pattern Researches|yurina ito,hironori washizaki,yoshiaki fukazawa,takao okubo,haruhiko kaiya,atsuo hazeyama,nobukazu yoshioka|security patterns,systematic mapping,software patterns|Security patterns (SPs) are reusable solutions to security problems. Although the number of SPs has grown recently, two critical problems remain. First, whether SP researches are conducted actively remains unclear. Second, the tendencies in SP research are unclear (e.g., the tendencies such as SP research contents and modeling methods of SPs are unclear). To elucidate the features, herein SP research from 30 papers is classified using a technique called systematic mapping (SM). The results (e.g., as patterns that are less frequently studied (e.g., reference monitor) become more frequent, the number of practical research such as evaluation experiments should also increase) should guide future research on SPs as well as assist technicians in using SPs.|Reflections (on the past):|reject|reject
ICSE|2015|Predicting Field Reliability|pete rotella,sunita chulani,devesh goyal|software release reliability,prediction,modeling,testing,release quality,customer experience|The objective of the work described is to accurately predict, as early as possible in the software lifecycle, how reliably a new software release will behave in the field.  The initiative is based on a set of innovative mathematical models that have consistently shown a high correlation between key in-process metrics and our primary customer experience metric, SWDPMH (Software Defects per Million Hours [usage] per Month).  We have focused on the three primary dimensions of testing – incoming, fixed, and backlog bugs.  All of the key predictive metrics described here are empirically-derived, and in specific quantitative terms have not previously been documented in the software engineering/quality literature.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|SCRUMCASE - An Efficient Method for Effort Estimation for Mobile Applications Development|kwabena bennin,passakorn phannachitta,mohammed al-qadhi,jacky w. keung,akito monden,ken-ichi matsumoto|effort estimation,mobile applications development,scrum|In recent times, mobile devices have become a round-the- clock personal necessity. This necessity stems from the continuous usage of mobile applications in almost all daily life facets. In addition to the generic characteristics required by any software such as efficiency and maintainability, mobile applications development needs to take into account many unique challenges such as one imposed by smartphone platforms on which they are deployed. In the literature, Agile software development methodology was reported as the best fitting methodology for developing mobile applications. To the best of our knowledge, the literature does not contain a procedure to tailor a model-based nor a reasoning-based effort estimation techniques for mobile applications development. In this paper, we propose an ensemble effort estimation approach, namely SCRUMCASE which is based on the scrum methodology, use case points, and analogy-based effort estimation technique. A combination of these methods will not only enable an efficient way of estimating effort required for mobile development but will also obliterate and mitigate the minor issues and squabbles between developers and customers concerning satisfaction at the end of the project.|Visions (of the future):|reject|reject
ICSE|2015|Rapid Risk Assessment in Early Software Development Stage|takao okubo,haruhiko kaiya,hironori washizaki,nobukazu yoshioka|software development,architecture,non-functional requirements,security,performance,risk assessment,data flow,deployment diagram|We propose a novel method for assisting analysts in choosing the most appropriate architecture specification with assessing risks by preparing the risk values for typical architectures. We propose a new diagram called Risk Model with Architecture and Likelihood Level (RiMALL) which contains such deployment and risk likelihood level information. Analysts can calculate the risk values on non-functional requirements for architecture candidates by tracing data flows on RiMALL diagrams, and then can choose the most appropriate architecture.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Getting Stakeholders on the Same Indifference Curve - A Requirements Negotiation Model|manish motwani,abhishek sainani,smita ghaisas|Requirements negotiation,Indifference curve,Negotiation support system|Stakeholders’ experiential expectations from business processes (and software systems supporting them) are often negotiable when viewed from the angle of meeting conflicting goals. For a given set of conflicting goals that must be aligned with, stakeholders are seen to be willing to climb down from their most preferred choices in terms of meeting requirements and settle for the next best option(s). The process of requirements negotiation among stakeholders who ‘own’ conflicting goals should finally result in getting all the involved stakeholders on the same indifference curve:- a concept we borrow rather broadly in spirit, from Microeconomics. In this work we reimagine software systems as intelligent systems that take into account possible negotiations among stakeholders and recommend optimal decision based on negotiation parameters in a given scenario. We present a requirements negotiation model and early results of its illustration for stakeholders internal and external to an organization.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Constraint Transformation to Guide Software Design Evolution|andreas demuth,markus riedl-ehrenleitner,roberto erick lopez-herrejon,alexander egyed|model transformation,consistency checking,model-driven engineering,evolution|In Model-driven Engineering, models are first class development artifacts that evolve continuously throughout the entire engineering process. Typically, different engineers work on different areas of the system under construction. Thus, different parts of the system evolve at different times and at different speeds, which easily leads to contradictions and inconsistent knowledge being used. Indeed, there are various approaches presented in literature that avoid such issues by performing automatic transformations or bidirectional synchronization. However, these approaches do have serious drawbacks. For instance, they may automatically override decisions that were previously made by engineers. In this initiative paper, we present the outlines of a recently started, 3-year research project, funded by the Austrian Science Fund (FWF), that analyses how such contradictions and inconsistent knowledge can be detected immediately and how guidance can be generated to help engineers fix arising issues, without performing automatic, and potentially unintended, changes to the system under construction. The project specifically investigates an approach that dynamically generates and evolves constraints between individual parts of a system. In doing so, evolution of the system triggers only automatic evolution of constraints, which are used for i) finding contradictions and, if such are found, ii) generating recommendations on how to fix them.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Deriving Adaptation Plan of Self-adaptive System from Design Space using Repertory Grid.|sangeeta dey,seok-won lee|self adaptive systems,requirements elicitation,design space,repertory grid,adaptation plan|The growing complexity and dynamics of the execution environment have been major motivation for designing self-adaptive systems. Significant works can be found in the field of formalizing or modeling the adaptive requirements, but not enough attention has been paid towards the requirements elicitation techniques for the same. It is still an open challenge to introduce the required flexibility in the system right from the beginning of the requirements engineering phase. We explore the idea of using repertory grid to acquire knowledge from various stakeholders along the dimensions of the design space and induce an adaptation plan very naturally from the acquired knowledge. A stepwise methodology for deriving the adaptation plan has also been proposed in this work. To study the applicability of our idea we provide a preliminary example and corresponding result.|Visions (of the future):|reject|reject
ICSE|2015|Running Experiments in the Software Industry: Understanding the Progress from Laboratory to Field Studies|sira vegas,ayse tosun misirli,natalia juristo juzgado|Experimentation,Experimentation in industry,Replication of experiments,Professionals as experimental subjects|Software professionals currently find experimental findings to be of very little use. The application of the experimental research paradigm to software development is failing to realize the expected transition from the laboratory to the real world. Laboratory experiments are relatively common practice in software engineering, but experiments in industry are thin on the ground. Of the small number of existing cases, most are 1-1 (one company running one experiment), only some are 1-n (one company running n experiments) and still fewer are n-1 (n companies running one experiment). This paper describes a FiDiPro  project that aims to complete the experimental chain by conducting an m/l-n (m companies and l laboratories running n experiments) experience. This experience should serve to gain insight into the adoption of experimentalism in SE. The project aims to advance knowledge on: 1) the difficulties and challenges of experimentation in the software industry; 2) the identification of contextual variables; and 3) the representativeness of students as experimental subjects. Five companies have undertaken to provide professionals as subjects for the experiments to be run at their sites. The results of the experiments will be transferred to each company so that they can exploit the resulting evidence in their decision making process. The same experiment will be replicated with university students. Through the collection of such broad-based data, we will be able to achieve the above goals.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Secure Integration of Cryptograhic Software|sarah nadi,karim ali,sebastian thore erdweg,steven arzt,eric bodden,mira mezini|static analysis,software product lines,feature models,cryptography,API misuse,secure software|With the increase of software reliance on the Internet and the cloud, mobile apps and web applications are now increasingly gathering private user data that must be protected. While cryptography is often the method of choice, many application developers are not cryptography experts. Though high-quality cryptographic APIs are widely available, programmers often pick the wrong algorithms or misuse APIs due to lack of understanding. This opens up their applications to potential attacks that could leak personal user data. In this paper, we propose a long-term solution that helps application developers integrate cryptographic components correctly and securely by bridging the gap between cryptographers and application developers. Our solution consists of a software product line (represented by a feature model) that automatically identifies the correct algorithms to use based on the developer’s answers to high-level questions in non-expert terminology. We automatically synthesize a secure code blueprint that corresponds to the selected usage scenario. Since over time the developer may nevertheless change the application code, we propose to statically analyze the program to ensure that the correct use of the API is not violated in the future. The specifications for correctly using each API should be part of the algorithm’s definition in the feature model. Our proposal is a recently-funded project that is part of a bigger- scope, university-wide effort to develop and integrate secure code.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Simulation-Based Analysis of Safety-Critical Collaborative Systems|christoph dorn,thomas mehofer|system analysis,collective behavior,simulation,safety-critical systems,model mapping|The design of safety-critical systems requires dedicated analysis techniques for assuring high quality as well as safety integrity. Air Traffic Control or Command-and-Control systems call for techniques that provide a detailed understanding of effects emerging from collective user behavior. The current lack of appropriate approaches results in costly, over-provisioned systems which greatly constrain user activities. Our vision foresees a technic-operative mapping that tightly links user behavior models to technical models. This allows for analyzing system boundaries through simulation of interdependent user behaviors and subsequently raises confidence in the system's qualities.|Visions (of the future):|reject|reject
ICSE|2015|Measuring the Effect of Programming Languages on Code Quality in Application Domains: Towards a Software Engineering Materials Science|rob van der leek,magiel bruntink,joost visser|Software Technologies,Programming Languages,Application Domains,Code Quality,Materials Science,Technology Choice,Accidental Complexity|Software engineering as a discipline still lacks the objective means to evaluate and corroborate choices of software technology (e.g., programming language, frameworks, libraries) made in software projects. Consequently, many software technol- ogy choices in practice are guided by subjective preference (e.g., anecdotal- or historical arguments), potentially leading to a sub- optimal fit between the chosen technology and the application domain. As the supply side of new technologies grows in an increasing rate, technology choices becomes harder, yet more im- portant. This problem can be alleviated by developing a materials science that quantitatively studies the software quality delivered by various software technologies as observed in industrial and open source software projects. The proposed materials science in this paper will be able to provide practitioners with objective data that show to what extent a particular software technology is a fitting choice given a particular application domain. Where appropriate an analogy with established fields of materials science in other engineering disciplines is made. Furthermore we propose a research agenda and method that outlines how we plan to develop such a materials science for software projects.|Visions (of the future):|reject|reject
ICSE|2015|Strategic Analytics for the Location Management in Dynamic Distributed Software Development - DD-SCALE RESEARCH IN PROGRESS|pekka kamaja,mikko ruohonen|Distributed software development,Global software development,IT offshoring,Software Estimation,Intellectual Capital|The offshoring of software development (SD) to cost-competitive countries in South East Asia as well as to India has become increasingly popular among the US and Western Europe software companies. However, low costs play no longer that important role in the offshoring market. Future game changing factors are the quality of service, the building of dynamic competencies and other intangible factors required for managing dynamic distributed software development (DDSD) work. The increased importance of dynamism is seen in team-level competence requirements and in the shift from the one-way migration of RDI operations towards more complex manoeuvres by software companies. The current software estimation methods are capable to measure the costs of software development projects but the results may vary due the socio-cultural orientation of development teams. The contribution of this paper is introducing the building blocks in the framework of developing a more comprehensive analytics for the DDSD in the global setting. Explaining the variations in current estimation methods could be improved by consideration of the intangible factors involved in intellectual capital and knowledge management disciplines.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Using Qualitative Data Analysis in Requirements Engineering|andreas kaufmann,dirk riehle|requirements engineering,requirements elicitation,qualitative data analysis,QDAcity|This article discusses the application of qualitative research methods from the social sciences to the requirements engineering process. We propose a new approach pursued in our project QDAcity RE and highlight the research questions for our future research.|Visions (of the future):|reject|reject
ICSE|2015|Fathoming Software Evangelists With The D-Index|damien a. tamburri,elisabetta di nitto,ferran borreguero,dmitrii stebliuk,chengyu zheng|Open-Source,Reputation,Measurement|The increased importance represented by open-source and crowd-sourced software developers and software development in general, inspired us to consider the following dilemma: can we “compute" virtuous software developers? The D-Index is our preliminary attempt. Essentially, the D-Index meaningfully equates several indicators for the virtues of a developer, such as, contributed code, its quality, mentoring in online learning communities, community engagement. Our preliminary evaluation of the index suggests that establishing the virtues for certain developers eases the identification of software “evangelists", key success enablers for software communities.|Visions (of the future):|reject|reject
ICSE|2015|A Vision of Crowd Development|thomas d. latoza,andre van der hoek|Crowdsourcing,collaborative software development,open source software development|Crowdsourcing has had extraordinary success in solving a diverse set of problems, ranging from digitization of libraries and translation of the Internet, to scientific challenges such as classifying elements in the galaxy or determining the 3D shape of an enzyme. By leveraging the power of the masses, it is feasible to complete tasks in mere days and sometimes even hours, and to take on tasks that were previously impossible because of their sheer scale. Underlying the success of crowdsourcing is a common theme – the microtask. By breaking down the overall task at hand into microtasks providing short, self-contained pieces of work, work can be performed independently, quickly, and in parallel – enabling numerous and often untrained participants to chip in. This paper puts forth a research agenda, examining the question of whether the same kinds of successes that microtask crowdsourcing is having in revolutionizing other domains can be brought to software development. That is, we ask whether it is possible to push well beyond the open source paradigm, which still relies on traditional, coarse-grained tasks, to a model in which programming proceeds through microtasks performed by vast numbers of crowd developers.|Initiatives (recently funded research projects):|accept|accept
ICSE|2015|Rapid Multi-Purpose, Multi-Commit Code Analysis|carol alexandru,harald c. gall|Code analysis,MSR,large-scale,full history|Existing code- and software evolution studies typically operate on the scale of a few revisions of a small number of projects, mostly because existing tools are unsuited for performing large-scale studies. We present a novel approach, which can be used to analyze an arbitrary number of revisions of a software project simultaneously and which can be adapted for the analysis of mixed-language projects. It lays the foundation for building high-performance code analyzers for a variety of scenarios. We show that for one particular scenario, namely code metric computation, our prototype outperforms existing tools by multiple orders of magnitude when analyzing thousands of revisions.|Visions (of the future):|accept|accept
ICSE|2015|On Architectural Diversity of Dynamic Adaptive Systems|hui song,amal elgammal,vivek nallur,franck chauvel,franck fleurey,siobhan clarke|self-adaptive systems,software diversity,software architecture|We introduce a novel concept of "architecture diversity" for adaptive systems and posit that increased diversity has an inverse correlation with adaptation costs. We propose an index to quantify diversity and a static method to estimate the adaptation cost, and conduct an initial experiment on an exemplar cloud-based system which reveals the posited correlation.|Visions (of the future):|accept|accept
ICSE|2015|EnWare Engineering: Trustworthiness Assurance of Cyber-Physical Systems|roberto pietrantuono,stefano russo|Cyber-phyisical systems,Trustworthiness assurance,Environment,Safety|Cyber-physical system (CPS) refers to integrations of computing with physical processes. They are enabling the development of astonishing scenarios, where people, environment, and computers cooperate toward highly ambitious goals. On the other hand, these systems are often and increasingly involved in serious accidents, which undermine our confidence in them. Besides thinking about new and attractive scenarios for CPSs, a big issue is how to ensure their correct operation and improve their trustworthiness. Apart from contingent causes, a common underlying factor in CPS-related disasters is the unforeseen effect of the “physical” (i.e., environment) over the “cyber” (i.e., computing) part. CPSs actively engage the real world in real time: we can no longer expect to assure only the “cyber” part neglecting the “physical” one: environment must be considered an active subject of trustworthiness assurance (TA) processes. With this paper, we lay the foundation for the EnWare Engineering, a new conceptual framework to cope with TA in future’s CPSs. We conjecture that TA needs to be re-thought, in order to move from computer- to environment-centric assurance principles. The reported work is a first step toward that aim.|Visions (of the future):|reject|reject
ICSE|2015|Judging Asychronous Deferral in Interrupt Handlers for Concurrency Bugs|han liu,hehua zhang,xiaoyu song,ming gu,jiaguang sun|Interrupt handler,Asynchronous deferral,Static and run-time analysis|Interrupt handlers are prone to concurrency bugs, especially with posted deferrals which introduce complicated asynchronous semantics. Accurately capturing behavior of handlers is no straightforward task and uncovering bugs is also challenging due to concurrency complexity. In this paper, we propose a novel model to fit in interrupt handlers with asynchronous deferral. With an interrupt stack and deferral queue, we reserve semantics in interrupt masking, asynchronous posting, queue flushing and multiprocessing. Based on the model, we develop an analysis technique to collect static and run-time information, which are used to eliminate irrelevant operations and reduce interleaves. The proposed approach is evaluated on a benchmark standardized in IEC-61375. With the proposed model, we detect a real data race bug in the asynchronous deferral. Using the proposed analysis, the verification performance is improved by 30 times in speed and 20 times in memory. Initial results convince us of the possibility to cost-effectively detect concurrency bugs in large-scale interrupt handlers with asynchronous deferral.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Towards a Generic Framework for Supporting Extensive Analysis of Android Applications|li li,alexandre bartel,tegawende f. bissyande,jacques klein|Static Analysis,Generic Framework,Android Application,Apkpler|Despite much effort in the community, the momentum of Android research has not yet produced complete tools to perform thorough analysis on any application, leaving users vulnerable to malicious app developers. Because it is hard for a tool to efficiently address all of the various challenges of Android programming which make analysis difficult, we propose to instrument the app code for reducing the analysis complexity. We introduce in this paper Apkpler, a plugin-based framework for supporting such instrumentations.|Visions (of the future):|reject|reject
ICSE|2015|A Novel Design Property and Principle based Metrics Suite for Object-Oriented Design|yongrui xu,peng liang,muhammad ali babar|object-oriented metrics,design property,design principle|Software metrics are widely used to evaluate the quality of software processes and products. A large number of OO metrics have been proposed to evaluate the quality of design for OO systems. The OO metrics focus on structural properties of software systems (e.g., coupling & cohesion). Structural metrics may be 'cheating' software engineers as the design results do not correspond to developers’ expectations. We propose a novel OO design metrics suite based on design properties and design principles. To evaluate the proposed metrics, we use a genetic algorithm (GA) to automatically synthesize design solutions for a classical ATM OO design problem. The results show that the proposed metrics suite can generate solutions which are closer to a design by an expert.|Reflections (on the past):|reject|reject
ICSE|2015|When App Stores Listen to the Crowd to Fight Bugs in the Wild|maria gomez,matias martinez,martin monperrus,romain rouvoy|App store,Mobile apps,Crowdsourcing,Bug repair|App stores are digital distribution platforms that put available apps that run on mobile devices. Current stores are software repositories that deliver apps upon user requests. However, when an app has a bug, the store continues delivering defective apps until the developer uploads a fixed version. Thus, impacting on the reputation of both store and app developer. In this paper, we envision a new generation of app stores that: (a) reduce human intervention to maintain mobile apps; and (b) enhance store services with smart and autonomous functionalities to automatically increase the quality of the delivered apps. We sketch a prototype of our envisioned app store and we discuss the functionalities that current stores can enhance by incorporating automatic software repair techniques.|Visions (of the future):|accept|accept
ICSE|2015|MAIA: a Middleware for Adaptaptive wIreless sensor network Applications|luca berardinelli,antinisca di marco,francesco gallo,stefano pace|WSN,modeling,analysis,adaptation|Wireless Sensor Networks (WSN) can nowadays find application into a wide set of domains, ranging from safety to security, from health to environment. This versatility leads to a growing request for high quality applications running on WSNs. We are realizing the Agilla Modeling Framework (AMF), an executable UML model library and profile that allows to conveniently design adaptive WSN applications and to carry out their timing, performance and energy analyses upon the execution of the corresponding UML model. In addition, the models can be automatically transformed into executable code. Thanks to the AMF, we’ll be able to design WSN agents, to configure the WSN node to obtain the required performance within a modeldriven environment, to obtain the agents’ code and to evaluate the energy consumption. In this vision paper, we evolve AMF into MAIA, a Middleware for Adaptive wIreless sensor network Applications. We equip AMF with more advanced analysis techniques (extending the existing capabilities with support for performance and energy analysis of multiple agents running on WSN), and, in particular, we introduce the new capability of generating reconfiguration plans from which the most convenient one can be chosen in case of multiple reconfiguration strategies. This choice is driven by the results of the performance and energy analysis MAIA is able to carry out.|Visions (of the future):|reject|reject
ICSE|2015|Analysing Social Networks of Component Ecosystems|giordano tamburrelli,damien a. tamburri|Social Networks Analysis,Software Components Ecosystems,Recommendation|Modern software development increasingly relies on off-the-shelf components available on large and remotely accessible software repositories to speed up the development process and reduce its costs. As a consequence, the ability to accurately search, analyse and understand the existing intricate ecosystem of off-the-shelf software components is a crucial task of modern software development. This paper addresses this issue envisioning an innovative approach to the analysis of off-the-shelf software ecosystems based on the techniques traditionally used for Social Network Analysis. From this novel viewpoint, software components become nodes in a social network and their analysis results in viable recommendations for developers such as new useful components and clusters of correlated components. We illustrate the proposed approach with an example scenario.|Visions (of the future):|reject|reject
ICSE|2015|An X-Ray for Software|kecia ferreira|software networks,software topology,software health,software quality|Thousands of software systems have been developed. We can say that we know the main principles and techniques to construct a good software system, but we do not know how the systems we make and maintain are really structured. This may be a critical problem, especially in maintenance tasks of large projects, because the knowledge about the software structure is essential in such kind of activities. Some proposals have emerged in the last decades to help the analysis and the comprehension of software systems, such as software visualization techniques, software metrics, and approaches to detect code smells. The concepts of Complex Networks also have been used to explore the characterization of software systems. In this context, one of the results of my PHD dissertation is a macroscopic topology of the software systems network, named Little House. After that, me and my colleagues have investigated the characteristics and implications of such topology. We also have developed a platform to explore software systems by means of Little House. In this paper, I describe the ongoing research I have coordinating on Little House and summarize the main results this research have achieved so far. I also introduce the idea of using Little House as an X-Ray of software systems. Based in the previous and initial results that me and my colleagues have found, I describe an application of Little House to get insights on the health of software systems.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|An initiative to improve reproducibility and empirical evaluation of software testing techniques|francisco gomes oliveira neto,richard torkar,patricia machado|Reproducible Software Engineering,Software Testing,Empirical Software Engineering|The current concern regarding quality of evaluation performed in existing studies revealed the need for methods and tools to assist definition and execution of empirical studies and experiments. However, when trying to apply general methods from empirical software engineering in specific fields, such as evaluation of software testing techniques, new obstacles and threats to validity appear, hindering researchers’ encouragement to pursue empirical methods. This paper discusses those issues specific for evaluation of software testing techniques and propose an initiative for a collaborative effort to encourage reproducibility of experiments evaluating software testing techniques (STT). We also propose development of a tool that enables automatic execution and analysis of experiments producing reproducible research compendia as output that are, in turn, shared among researchers. There are many expected benefits from this endeavour, such as providing a foundation for evaluation of existing and upcoming STT, and maturing researchers to devise and publish better experiments.|Initiatives (recently funded research projects):|accept|accept
ICSE|2015|From Static Books to Dynamic Knowledge Bases: Moving Software Engineering Knowledge Dissemination into the 21st Century|ian gorton,john klein,soumya simanta,yiming yang,hanxiao liu,ruochen xu|knowledge capture,knowledge dissemination,tactics,knowledge base,semantic model,big data,NoSQL|The pace of software technology evolution creates incredible opportunities for software engineers to build ever more advanced, sophisticated applications. But as with all opportunities, there is downside, namely the difficulty of maintaining state-of-the-art expertise in a rapidly changing technological and engineering landscape. While the fundamental principles of computer science and software engineering are well established (and evolving relatively slowly), the state-of-the-art of technological and engineering approaches and tools change daily. This creates a knowledge capture and dissemination vacuum that sees software engineers constantly drawing on a diverse collection of unstructured, complex and often contradicting information sources – books, blog posts, products documentation, online presentations, discussion boards – to keep up with this pace of change. In this paper we describe a vision of the future in which software engineering knowledge is widely available from curated, semantically organized, dynamic knowledge bases. These knowledge bases link fundamental software design principles to supporting technologies and tools, and provide a basis for comparing competing approaches based on a detailed feature model. Such a knowledge base is curated by experts as well as populated by algorithms that are able to discover relevant information from the Internet using machine learning. As an example, we describe briefly the QuABase, a knowledge base for designing big data systems that is our first step to demonstrating the feasibility of this vision.|Visions (of the future):|reject|reject
ICSE|2015|Goal-Oriented Creativity and Creative Goal Modeling for Requirements Engineering|jennifer horkoff,neil a. m. maiden|requirements engineering,creativity,goal modeling,model analysis,conceptual analysis|Creativity techniques as applied to Requirements Engineering (RE) focus on finding novel and appropriate requirements, facilitating system and business innovation. However, ideas are generated in a free-form, manual fashion, with much guidance from human facilitators. Creative output is captured by analysts using text and informal diagrams, not amenable to (semi-) automated analysis, including decision support. Goal modeling and analysis captures stakeholder goals and requirements, with underlying formalisms allowing for (semi-) automated analysis over alternative requirements. However, it is not always easy for stakeholders to articulate their goals, or to come up with creative alternatives. In this paper we describe a new research effort which exploits the synergies between creativity techniques and goal modeling and analysis. Specifically, we use creativity techniques to populate goal models with creative content, and use the structure of goal models to facilitate exploratory, transformational, and combinatorial creativity. Methods are demonstrated by extrapolating over historical creativity projects.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|CodeAware: Sensor-Based Fine-Grained Monitoring and Management of Software Artifacts|rui abreu,m. hakan erdogmus,alexandre perez|Source code management,Continuous Integration,Continuous Monitoring,Team Productivity,Code Quality|Current continuous integration (CI) tools, although extensible, can be limiting in terms of flexibility. In particular, artifact analysis capabilities available through plugin mechanisms are both coarse-grained and centralized. To address this limitation, this paper introduces a new paradigm, CodeAware, for distributed and fine-grained artifact analysis. CodeAware is an ecosystem inspired by sensor networks, consisting of monitors and actuators, aimed at improving code quality and team productivity. CodeAware's vision entails (a) the ability to probe software artifacts of any granularity and localization, from variables to classes or files to entire systems; (b) the ability to perform both static and dynamic analyses on these artifacts; and (c) the ability to describe targeted remediation actions, for example to notify interested developers, through automated actuators. We provide motivational examples for the use of CodeAware that leverage current CI solutions, sketch the architecture of its underlying ecosystem,  and outline research challenges.|Visions (of the future):|accept|accept
ICSE|2015|Investigating Repertory Grids and Personal Constructs for Software Data Analytics|lucas layman,carolyn b. seaman,davide falessi,madeline diep|repertory grids,practitioners,transfer learning|The repertory grid technique is an interview-based procedure for eliciting “constructs” (e.g., adhering to coding standards) that individuals believe influence a worldly phenomenon (e.g., what makes a high-quality software project) by comparing example elements from their past (e.g., projects they have worked on).  We investigate the relationship between repertory grid constructs, which capture complex non-measurable concepts, elicited from eight software engineers and objective metrics of project quality and efficiency. We observed statistically significant correlations between objective project outcome measures and the subjective constructs. This suggests that repertory grids may be of benefit in developing models of project outcomes, particularly when project data is limited. We describe the limitations of the repertory grid approach and define several research challenges for integrating repertory grid constructs into software data analytics.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Virtual Reality in Software Engineering:  Affordances, Applications, and Challenges|anthony elliott,brian peiris,chris parnin|virtual reality,software engineering,code review,live coding|Software engineers primarily interact with source code using a keyboard and mouse, and typically view software on a small number of 2D monitors. This interaction paradigm does not take advantage of many affordances of natural human movement and perception. Virtual reality (VR) environments designed for software engineering can greatly increase productivity by taking advantage of innate affordances. This paper describes the affordances offered by VR; demonstrates the benefits of VR and software engineering in prototypes for live coding and code review; and discusses future work, open questions, and the challenges of VR.|Visions (of the future):|accept|accept
ICSE|2015|Towards Automated Explanations for Test-Suite Reduction Results|alex gyori,august shi,milos gligoric,darko marinov|Test-Suite Reduction,Software Testing,Test-Redundancy|Test-suite reduction speeds up regression testing by identifying and removing redundant tests from test suites; redundant tests are commonly defined as tests that cover only code elements that are also covered by the other tests in the test suite. Previous research proposed several test-suite reduction techniques aiming to minimize the size of reduced test suites while not substantially decreasing their fault-detection capability. While the techniques effectively reduced test suites automatically, none of the proposed techniques nor the following studies provided any explanation for why some tests are removed while others are kept. We envision a new approach where developers can easily find what makes a test redundant in the test-suite. Providing such explanations and understanding the results of test-suite reduction techniques in general is critical both for deciding whether or not to apply test-suite reduction in practice and for devising better test-suite reduction techniques in research. We propose several metrics to help understand redundancy causality, through distance and dominance relations among tests, which help in explaining test-suite reduction results. We implemented our metrics in an automated tool that we ran on the results of existing test-suite reduction techniques applied on 121 versions of 17 open-source projects. Inspecting the outliers from our metrics helped us to explain test-suite reduction results. Based on this, we identified cases in which test-suite reduction works as expected, cases in which it does not work as well, and cases that require human judgment from a test engineer to ensure that no valuable test is removed. Our insights based on these cases provide motivation for designing better, more actionable techniques, that provide clear causality information for redundancy enabling test-engineers to make informed decisions on removing tests.|Visions (of the future):|reject|reject
ICSE|2015|A Unified Framework for the Comprehension of Software's Time Dimension|omar benomar,houari a. sahraoui,pierre poulin|Program execution,Software evolution,Software comprehension,Software time dimension,Unified comprehension framwork|The dimension of time in software appears in both program execution and software evolution. Much research has been devoted to the understanding of either program execution or software evolution, but these two research communities have developed tools and solutions exclusively in their respective context.  In this paper, we claim that a common comprehension framework should apply to the time dimension of software. We formalize this as a meta-model that we instantiate and apply to the two different comprehension problems.|Visions (of the future):|accept|accept
ICSE|2015|Agile bottom-up development of domain-specific IDEs for model-driven development|steffen vaupel,felix rieger,daniel struber,gabriele taentzer|model-driven development,domain-specific IDEs,domain-specific modeling languages,bottom-up software development,agile software development|In modern software engineering, there is a trend towards shorter and more systematic software development. Two recent approaches are agile and model-driven development. To realize model-driven software development, a domain-specific integrated development environment (IDE) has to be developed. In this paper, we propose an agile process for bottom-up development of domain-specific modeling languages and their IDEs. We focus on a fine-grained co-evolution of concrete applications and domain-specific modeling languages (DSMLs) with their code generators. An important success factor of DSML development would be the support for automated deduction of migration scripts for all dependent artifacts of a DSML evolution step. We illustrate our approach by iteratively developing an IDE for model-driven development of mobile applications.|Visions (of the future):|reject|reject
ICSE|2015|Optimising Energy Consumption of Design Patterns|adel noureddine,ajitha rajan|design patterns,energy consumption,software transformation|Software design patterns are widely used in software engineering to enhance productivity and maintainability. However, recent empirical studies revealed the high energy overhead in these patterns. Our vision is to automatically detect and transform design patterns during compilation for better energy efficiency without impacting existing coding practices. In this paper, we propose compiler transformations for two design patterns, Observer and Decorator, and perform an initial evaluation of their energy efficiency.|Visions (of the future):|accept|accept
ICSE|2015|Identifying Architectural Technical Debt based on Architecture Decisions and Change Scenarios|zengyang li,peng liang,paris avgeriou|architectural technical debt,architectural technical debt identification,architecture decision,change scenario|Architectural technical debt (ATD) is incurred by design decisions that consciously or unconsciously compromise system-wide quality attributes, particularly maintainability and evolvability. ATD is harmful to the system’s long-term health but hard to identify due to its invisibility to external users and its high abstraction level. Existing ATD identification approaches use source code analysis to identify system implementation structure issues (e.g., modularity violations), leaving most other types of ATD in a software system undetected. We proposed an ATD identification approach based on architecture decisions and change scenarios. To validate this approach, we conducted a case study in an industrial environment. The case study results show that the proposed approach is useful and easy to use.|Visions (of the future):|reject|reject
ICSE|2015|Gamifying Software Engineering Tasks based on Cognitive Principles: The Case of Code Review|naomi unkelos-shpigel,irit hadar|Gamification,Code review,Boundary object,Boundary spanner|Code review is an important task in software development. However, performing code review is perceived, for the most part, as an undesired task, presenting several challenges to the required collaboration and knowledge transfer between programmers and reviewers. In order to overcome these challenges and improve the effectiveness of code review, we developed SCRUT: Social Code Review Unifying Tool. This tool motivates collaboration and knowledge sharing between programmers and reviewers, by recruiting relevant cognitive theories and implementing gamification elements to encourage and enhance the task of code review. This paper presents our vision for enhancing software engineering via gamification, and the theoretical cognitive foundation on which this vision is based, starting with the example of code review.|Visions (of the future):|reject|reject
ICSE|2015|Aragorn: A Live Notification System of  Inter-Cell Dependencies in Spreadsheets|sohon roy,felienne hermans|End-user computing,Spreadsheets,Dependence tracing|The spreadsheet programming paradigm is a major constituent of end-user computing and although not considered as part of conventional software engineering, we believe that the field can largely benefit from the application of software engineering best practices and methodologies. Modification of a large and complex spreadsheet, just like modification of a large and complex piece of code, is a challenging task without fore-warning of how the change might affect other parts of the spreadsheet. In this paper we propose a technique to address this challenge by taking cues from the type of support provided by modern IDEs (Integrated Development Environment) to programmers. However spreadsheet users are not programmers and therefore any new technique needs to be evaluated for its adoptability to end-users. Thus we validate our technique by developing a prototype and conducting an exploratory user-study with industrial spreadsheet users. The results reveal the pros and cons of our technique and pave a path for further improvements and future work.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|How (Much) Do Developers Test?|moritz beller,georgios gousios,andy zaidman|Developer Testing,Empirical Software Engineering,The Mythical Man-Month|What do we know about software testing in the real world? It seems we know from Fred Brooks’ seminal work “The Mythical Man-Month” that 50% of project effort is spent on testing. However, due to the enormous advances in software engineering in the past 40 years, the question stands: Is this observation still true? In fact, was it ever true? The vision for our research is to settle the discussion about Brooks’ estimation once and for all: How much do developers test? Does developers’ estimation on how much they test match reality? How frequently do they execute their tests, and is there a relationship between test runtime and execution frequency? What are the typical reactions to failing tests? Do developers solve actual defects in the production code, or do they merely relax their test assertions? Emerging results from 40 software engineering students show that students overestimate their testing time threefold, and 50% of them test as little as 4% of their time, or less. Having proven the scalability of our infrastructure, we are now extending our case study with professional software engineers from open-source and industrial organizations.|Reflections (on the past):|accept|accept
ICSE|2015|Generating Customized Real-Time Database Management Systems based on Flexible Transaction Models: the DAGGERS process|simin cai,barbara gallina,dag nystrom,cristina seceleanu|Real time database management systems,customized databases,embedded real-time systems,transaction models,ACID properties,timeliness,model-checking,product-line engineering|Real-time Database Management Systems (RTDBMSs) are a promising solution for data management in Embedded Real-Time Systems (ERTSs), as they provide structured data management with guaranteed timeliness. However, current RTDBMSs use rigid transaction models to manage data manipulation, which often do not match the needs of ERTSs. In this paper, we describe the DAGGERS project's process that aims at generating customized RTDBMSs (seen as a product line) from design specifications of transaction models, obtained by selecting and composing transaction-related behavioral properties. In this process, based on the characteristics of the desired data manipulations, transactions models are designed and then formally verified and refined iteratively, to achieve the desired trade-offs between timing and logical consistency. The outcome of DAGGERS are customized RTDBMSs with appropriate trade-offs, which will offer flexible solutions to real-time database management.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|The Gandhi-Washington Method for Extracting Patterns from Versioning Histories|maleknaz nayebi,guenther ruhe|Release cycle times,Versioning histories,Pattern recognition,Data analytics|Reducing release cycle time has proven to be a significant factor to boost adaptive software development. A closer look into release cycle durations showed a strong variation of the release cycle times in softwares. Relying on the proven value of design and process patterns, we explored release cycle time patterns as an impact factor on product success. We designed a pattern recognition approach called the Gandhi-Washington-Method (GWM). The initial evaluation of GWM on the 9703 mobile apps in the Android market showed the applicability of our approach. Triggered by the value of both discipline and variation, GWM is based on (i) mining software release cycle time, (ii) utilizing concepts of formal language theory, (iii) forming hierarchical dependency graphs, and (iv) the application of statistical analysis to perform merging operations. All this leads into the most essential patterns impacting software success factors. For the case study evaluation, we extracted patterns from mining of mobile app versioning histories. Among the 9703 mobile apps, GWM detected six essential patterns which affected the success rate of mobile apps.|Visions (of the future):|reject|reject
ICSE|2015|Correctness and Relative Correctness|nafi diallo,wided ghardallou,ali mili|Correctness,Relative Correctness,Software Fault,Program Repair|In the process of trying to define what is a software fault, we have found that to formally define software faults we need to introduce the concept of relative correctness, i.e. the property of a program to be more-correct than another with respect to a given specification. A feature of a program is a fault (for a given specification) only because there exists an alternative to it that would make the program more-correct with respect to the specification. In this paper, we explore applications of the concept of relative correctness in program testing, program repair, and (surprisingly) program design. Specifically, we argue that in many situations of software testing, fault removal and program repair, testing for relative correctness rather than absolute correctness leads to clearer conclusions and better outcomes. Also, we find that designing programs by stepwise correctness-enhancing transformations rather than by stepwise correctness-preserving refinements leads to simpler programs and is more tolerant of designer mistakes.|Reflections (on the past):|accept|accept
ICSE|2015|A Multi-Method Exploration of the OpenStack Free/Libre/Open Source Software Industrial Ecosystem. Lessons Learned and Future Visions.|jose teixeira|Mix-methods,Mining Software Repositories,Social Network Analysis,Netnography,Open-source,Free-software,Software-ecosystems,Open-Coopetition|Much software is neither developed in-house nor outsourced in a dyadic relationship. Instead, we are in a new age where software is developed by a networked community of actors and organizations, which base their relations dynamically to each other on a common interest. Advances arising from research in business-ecosystems, or even from natural ecosystems, can provide valuable knowledge for better understanding and explaining software development in an ecosystem setting. In this paper, we explore how rival firms collaborate in the open source arena by employing a multi-method approach that combines Netnography, Mining Software Repositories and Social Network Analysis. By turning to the OpenStack ecosystem, we found out that qualitative ethnographic material, combined with social network visualizations, provide a rich medium that enable a better understanding of competitive and collaborative issues that are simultaneously present and interconnected in the community. Besides addressing a novel, complex and unexplored open source case in a longitudinal way, this research stresses the power and feasibility of combining multiple methods in empirical Software Engineering research. We, therefore, call for more research leveraging the power of mix methods and for more research incorporating the established Software Engineering research methods with other methods established in other disciplines.|Visions (of the future):|reject|reject
ICSE|2015|Multi-Objective Service Similarity Metrics for more Effective Service Engineering Methods|dionysis athanasopoulos,apostolos zarras|service similarity,multi-objective metric,service engineering method,effectiveness|The usage of single-objective similarity functions in engineering tasks of service-oriented software may reduce their effectiveness, since a single similarity value can be misleading. A single value cannot be clearly interpreted, since it hides the values of its individual objectives. The state-of-the-art approaches, which propose service similarity functions, rely on single-objective functions exclusively. Going to a completely different direction, we propose the usage of multi-objective functions for calculating service similarity. We formally define such a function, and we provide preliminary results, which show that the effectiveness of a service-engineering task (esp., service organization) can be improved by using multi-objective functions.|Reflections (on the past):|reject|reject
ICSE|2015|Information Transformation: An Underpinning Theory for Software Engineering|david clark,robert feldt,simon poulding,shin yoo|information theory,software testing,software development process|Software engineering (SE) lacks underpinning scientific theories both for the software it produces and the processes by which it does so. We propose that an approach based on information theory can provide such a theory, or rather many theories. The primary research approach we envision to realise such a benefit would be based on the quantification of information involved and a mathematical approach to study the limiting laws that arise. However, we also point out that less formal more qualitative uses for information theory can be useful. For example, clearly stating the information a method or technique needs is essential in judging its industrial applicability. The main argument in support of our vision is based on the fact that both a program and an engineering process to develop such a program are fundamentally processes that transform information. To illustrate our argument we focus on software testing and develop an initial theory in which a test suite is input/output adequate if it achieves the channel capacity of the program as measured by the mutual information between its inputs and its outputs. We outline a number of metrics and concrete strategies for improving software testing, based on our information theoretical analysis. We find it likely that similar analysis and subsequent future research to detail them would be generally fruitful for software engineering.|Visions (of the future):|accept|accept
ICSE|2015|Metamodel and Constraints Co-evolution: A Context Aware Resolution of Broken OCL Constraints|djamel-eddine khelladi,regina hebig,reda bendraou,jacques robin,marie-pierre gervais|Constraints,OCL,Co-evolution,Metamodel,Evolution,Context,Aware|The Object Constraint language (OCL) can be used to define constraints on top of metamodels to be verified on the models. Metamodel evolution and the impact on its models is a hot topic among Model-Driven community. OCL constraints’ co-evolution is a non trivial process that has received little attention in contrast to models. The two existing approaches suffer from two main issues: 1) changes that impact the structure of a constraint are not always handled, which limits the use of the approach. 2) more importantly, a unique resolution strategy per metamodel change and for all OCL constraints is always performed, which can be a source of errors. In fact, for the same metamodel change, different resolutions depending on the contexts of the OCL constraints are often necessary. The context is the metamodel element on which an OCL constraint is defined. This paper first addresses metamodel changes that impact the structure of OCL constraints while being context-aware, i.e. on which metamodel elements the contexts are defined. We thus propose a context-aware co-evolution that applies different resolution strategies based on the used contexts. Our preliminary evaluation on Unified Modeling Language case study show promising results.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|What is User Experience Really: towards a UX Conceptual Framework|stefan hellweger,xiaofeng wang|User experience,Usability,Mobile devices,Phone applications|For more then a decade the term User Experience (UX) has been highly debated and defined in many ways. However, often UX remains as a vague concept and it may be hard to understand the very nature of it. In this paper we aimed at providing a better understanding of this concept. We explored the multi-faceted UX literature, reviewing the current state-of-the-art knowledge and emphasizing the multi-dimensional nature of the concept. Based on the literature review we built a conceptual framework of UX using the elements that are linked to it and reported in different studies. To show the potential use of the framework, we examined the UX delivered by different phone applications on different mobile devices using the elements in the framework. Several interesting insights have been obtained in terms of how the phone applications deliver different UX. Our study opens up a promising line of investigating the contemporary meaning of UX.|Reflections (on the past):|reject|reject
ICSE|2015|Model Transformations for End-Users|harald storrle,vlad acretoaie,johan davidsen,john gotze|Model Transformations,End-User Programming,Human Factors,Usability|Model transformations are the key element of the Model Driven Engineering paradigm. As a consequence of this, the current crop of model transformation languages assume their users are able to master multiple meta-model layers. This severely limits their scope and audience. We believe that model transformations can be applied in many more contexts than those traditionally considered in Model Driven Engineering. In particular, they can be used to support non-technical end-users in manipulating conceptual data from their respective application domain on a level that makes sense to them. We also believe that today's model transformation languages are not well suited for this use case. Instead, a new kind of model transformation language is needed: one that focuses on the user's concerns, hides technical complexity, and is applicable to a wide variety of transformation scenarios. To address these requirements, we propose the Visual Model Transformation Language, a transformation language for non-technical end-users.|Visions (of the future):|reject|reject
ICSE|2015|CONSIDER: A Novel Approach to Enabling Conflict-driven Cooperative Learning|neelam soundarajan,swaroop joshi,rajiv ramnath|Cooperative learning,Piaget's theory of conflict-driven learning,Software to enable effective cooperative learning|The potential of cooperative activities in promoting effective learning in a broad range of disciplines from CS and other STEM topics to business and economics and nearly every other field is widely recognized. Team-based activities have been introduced into courses at all levels, e.g., from freshmen programming courses through senior-level courses in software engineering. But the most common student reaction when they hear that a course includes such activities is one of horror! And faculty typically end up justifying the need for them by referring to the fact that nearly every project in industry is team-based. Indeed, much literature on the topic considers specific steps that instructors should take in order to mitigate the potential *negative* aspects, such as social loafing, of team-projects and their effect on individual learning. Part of our thesis, based on classic work by Piaget and others, is that a primary reason for the failure of many team-projects etc.\ to live up to the potential of cooperative learning to improve student learning is that a key component of cooperative learning is missing from them. Piaget's and others' work showed that *cognitive-conflict* in a learner, caused by exposure to conflicting ideas of *peers* and the resolution of this conflict is a potent source of progress in the student's learning; and this essential contributor to student learning is often missing in typical team-based activities.  The other part of our thesis, and the basis of the vision underlying our work, is that it is possible to develop a novel software-based approach to cooperative learning that is not only consistent with the principles of cooperative learning facilitated by cognitive conflict but, because of the natural affordances it offers, adds considerably to the power of the approach; indeed, can enable cognitive conflict to *drive* cooperative learning. We elaborate on our vision and present some details of a prototype implementation of the approach.|Visions (of the future):|reject|reject
ICSE|2015|Ensuring Conformance to Process Standards for Software Engineering Processes & Practices|fahad rafique golra,fabien dagnat,reda bendraou|Standardization,Conformance,Process|Establishing a common frame of reference and milestones for software life-cycle planning, measuring, monitoring, evaluation becomes difficult without conforming to software development standards. Subcontractors and other enterprises developing critical software are often expected to follow certain international and/or organizational standards.  Lack of conformance testing methodologies for the de-facto processes against the de-jure standards, makes it hard to exploit the potential of standardization.  We present here forth, a framework for modeling software development processes that ensures their conformance to adopted standards from design till execution.|Visions (of the future):|reject|reject
ICSE|2015|Using Persuasive Games to Motivate Software Engineers for Lifelong Learning|shifa haidry,quan z. sheng,muhammad ali babar|Software Engineering,Persuasive Technology,Digital Games,Knowledge and Learning|This study aims to assess potential of using persuasive technologies in human aspects in software engineering. Previous research suggests that reasons for software project failure include various human behavioral issues. One of such issues is inability of the team members to acquire necessary knowledge due to lack of motivation. Software Engineering being the knowledge-intensive field, it requires software engineers to continuously improve related learning. We believe that by applying persuasive technology we can accomplish behavior change in software engineers and motivate them to keep on seeking to acquire relevant new knowledge.|Visions (of the future):|reject|reject
ICSE|2015|On Selection of the Number of Topics for the Latent Dirichlet Allocation Model|andriy miranskyy,denise woit|LDA,Topic analysis,Topic selection,Text mining|Context: Extraction of topics from text corpuses helps improve Software Engineering processes. Latent Dirichlet Allocation (LDA) is often used to perform such analysis. However, calibration of the models is computationally expensive, especially if iterating over a large number of topics. Our goal is to create a simple formula allowing analysts to estimate the number of topics, so that the top X topics include the desired proportion of documents under study. Method: We derived the formula from the empirical analysis of two SE-related text corpuses. Results: We show that this simple power law based formula can be used to estimate the number of topics with high accuracy (R2 ranges from 0.901 to 0.998). Conclusions: We believe that practitioners can use our formula to expedite LDA analysis. The formula is also of interest to theoreticians, as it suggests that different SE text corpuses have similar underlying properties.|Reflections (on the past):|reject|reject
ICSE|2015|Humanizing Software Artifacts: Software Engineering with Intelligent and Social Software Artifacts as our Friends|mithun p. acharya|Software Artifact Choreographed Software Engineering (SACSE),Artificial Intelligence,Social Network Analysis,Cloud Services|We propose and explore a new paradigm that considers every software artifact such as a class as an artificially intelligent and a socially active entity. In this Software Artifact  Choreographed Software Engineering (SACSE) paradigm, the humanized artifacts themselves take the lead and choreograph (socially, in collaboration with other intelligent software artifacts, humans, and artifact/human organizations) software engineering solutions to the many software development and maintenance challenges such as (automatic) code reuse, documentation, testing, patching, and refactoring. In this paper, we discuss the implications of seeing software artifacts as our new intelligent friends.|Visions (of the future):|reject|reject
ICSE|2015|Turing-Completeness Considered Harmful|ramy shahin|Language Design,Termination,Software Analysis|This paper argues that Turing-Complete programing languages are too powerful, but that power comes at a high cost, often unjustified. In particular, non-termination comes as a feature in any Turing-Complete language, while in fact computationally it should be considered a bug rather than a feature. The non-terminating nature of reactive systems has nothing computational to it, and it can be safely specified by a coordination language, ridding computational languages from that very expensive yet unusable power. The paper outlines several software engineering problems due to the wide adoption of Turing-Complete languages, and also presents some potential research questions that need to be addressed in the design of usable strongly normalized languages.|Reflections (on the past):|reject|reject
ICSE|2015|Identifying Architecturally Significant Functional Requirements and their Impact on Architecture|preethu rose,balaji balasubramaniam,jane cleland-huang,roel wieringa,maya daneva,smita ghaisas|Architecturally Significant Functional Requirements (ASFRs,speech acts,linguistic,architectural decisions|Failure to identify and analyse Architecturally Significant Functional Requirements (ASFRs) early on in the lifecycle of a project can result in costly rework in later stages of software development. While non-functional requirements (NFRs) are known to have a tightly interconnected relationship with Software Architecture, the impact that functional requirements can have on architecture is not so obvious. The skills needed for capturing functional requirements are different to those needed for making architectural decisions. As a result, these two activities are often conducted by different teams in a project and the documentation is maintained separately. We present a study that establishes the existence of ASFRs in the functional requirements specification. Further, we automate the identification of ASFRS from requirements documents and their classification into categories based on the different kinds of architectural impact they can have. We believe this to be a crucial precursor to recommending specific design decisions. We envisage a tool ArcheR that provides a structure to organize ASFRs, infer their architectural impact and to provide specific design solutions. ArcheR has an ontological base derived from the speech-acts theory in Linguistics.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Model Transformation by Examples: from Wishful Thinking to a Real Possibility|islem baki,houari a. sahraoui|Model transformation,Search-based software engineering,Model-Driven Engineering|We discuss the fundamental issues that prevent the existing approaches from efficiently solving the problem of learning model transformations from examples. We show that when considering complex transformation problems, the search space is too large to be explored by straightforward and naive strategies. We propose two strategies to reduce the search space and to better explore it, namely multi-steps learning and adaptive search. The preliminary evaluation shows that these strategies are promising and and increase our confidence that fully automated learning of complex transformation is a true possibility rather than wishful thinking.|Visions (of the future):|reject|reject
ICSE|2015|Multi-version data and Epsilon-transaction in Distributed Feedback Control for QoS Enhancement|malek ben salem,emna bouazizi,rafik bouaziz|QoS,Multi-versions Real-time Data,Epsilon-serializability|Large amounts of real-time data is used by geographically distributed real-time applications. DRTDBMS1 are designed to manage the large amount of real-time data in these applications, but the workload in these systems is unpredictable. A lot of work dealing with Quality of Service (QoS) has been done to control the unpredictable workload and the load balancing for user transactions between different nodes. they are based on feedback control real-time scheduling theory. In this paper, we propose to apply both multi-version real-time data and distributed real-time epsilon-transactions in distributed feedback scheduling architecture using three data replication policies. The proposed architecture is called ESR-MVRTD-DFCS Architecture. We also show that our proposed architecture can significantly improve the QoS in DRTDBMS by increasing the number of transactions that meet their deadlines.|Reflections (on the past):|reject|reject
ICSE|2015|Determining Software Provenance to Enable License Compliance Analysis and Verification|christopher vendome,denys poshyvanyk|Software Licenses,Software Provenance,Mining Software Repositories|Software license compliance presents a difficult, yet serious problem for developers. While software licensing aims to facilitate the freedom to reuse, to distribute, and to modify a code base, the licenses impose different legal restrictions. These differences lead to incompatibility between license, which for a developer means the two systems or components cannot be used together. To further complicate compliance, both licensing of a system change during evolution and licenses themselves can be updated into a new version. We propose an approach to extract licensing information from both source code and byte-code, and to ensure license compliance of all licenses within a system. First, we establish provenance of the binary components of a system in order to identify their license(s) since they would be lost during compilation. Subsequently, we identify the licenses of both binaries linked to source code and the original source code of the system so that. The compliance engine analyzes not only the component-license pairs, but it also considers the way in which the code was integrated. If a license incompatibility is located, the approach aims to provide a feedback  by analyzing both the architecture of the system and functional clones so that the developer can more easily remedy the license incompatibility. Thus, the developer is at least aware why the incompatibility and between which components, and recommended solutions are provided when applicable.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Dynamic Safety Cases for Through-life Safety Assurance|ewen denney,ibrahim habli,ganesh pai|Dynamic safety case,Safety assurance,Lifecycle processes,Safety management system|We describe dynamic safety cases, a novel operationalization of the concept of through-life safety assurance whose goal is to enable proactive safety management. Using a simple example from the aviation domain, we motivate our approach, its underlying principles, and lifecycle. Then we identify the key elements required towards a formalization of the associated framework.|Visions (of the future):|accept|accept
ICSE|2015|Evolution-Aware Monitoring-Oriented Programming|owolabi legunsen,darko marinov,grigore rosu|Runtime Monitoring,Regression Test Selection,Regression Testing,Monitor-Oriented Programming,Runtime Verification,Software Testing|Monitoring-Oriented Programming (MOP) helps develop more reliable software by means of monitoring against formal specifications. While MOP showed promising results, all prior research has focused on checking a single version of a target software application. We propose to extend MOP to support multiple software versions and thus be more relevant in the context of rapid software evolution. Our approach, called eMOP, is inspired by regression test selection—a well studied, evolution- centered technique. The key idea in eMOP is to monitor only the parts of code that changed between versions. We illustrate eMOP by means of a running example, and show the results of preliminary experiments. eMOP opens up a new line of research on MOP—it can significantly improve usability and performance when applied across multiple versions of an application, and is complementary to algorithmic MOP advances on single versions.|Visions (of the future):|accept|accept
ICSE|2015|Enhancing Programming Interface to Better Meet Information Needs of Developers|haipeng cai|Programming interface,information need,context switching,visualization,interaction|In the past decades, integrated development environments (IDEs) have been largely advanced to facilitate common software engineering tasks. Yet, with growing information needs driven by increasing complexity in developing modern high-quality software, developers often need to switch among multiple user interfaces, even across different applications, in their development process, which breaks their mental workflow thus tends to adversely affect their working efficiency and productivity. Inspired by useful interface and visualization design features in visual programming environments, we propose several design enhancements in modern IDEs, aiming at reducing the overheads of context switching commonly seen by developers when seeking various information they need. In this paper, we highlight the problem with working context transitions in existing IDEs, and remark the primary blockades behind; we then briefly describe the high-level design considerations for overcoming those blockades in the next-generation IDEs.|Visions (of the future):|reject|reject
ICSE|2015|Dominoes: Supporting Interactive Project Explorations of Software Archives|jose ricado da silva junior,daniel campagna,esteban clua,leonardo murta,anita sarma|Software Analytics,project exploration tool,GPU computation|Exploratory data analysis is widely known to be essential, but in the context of software development most analytics have been restricted to be post hoc. Few interactive exploration tools exist, largely because the large amount of data that needs to be analyzed prohibits its exploration at interactive speeds. Moreover, such analysis usually requires the development of complex scripts. Here, we present our tool for interactive data exploration – Dominoes– that allows users to interact with different types and units of data to investigate project relationships. Dominoes allows the user to perform exploratory data analysis on their projects and view the results through different visualizations. Additionally, it allows users to save the derived data tiles as well as their exploration paths for later use or use in a different project. All data transformations are executed in the GPU, allowing the tool to be responsive and supporting a seamless exploration of different types of relationships among project elements.|Visions (of the future):|reject|reject
ICSE|2015|Inferring Behavioral Specifications from Large-scale Repositories by Leveraging Collective Intelligence|hridesh rajan,tien n. nguyen,gary t. leavens,robert dyer|Behavioral specification inference,formal methods,mining large-scale open source repositories|Despite their proven benefits, useful, comprehensible, and efficiently checkable specifications are not widely available. This is primarily because writing useful, non-trivial specifications from scratch is too hard, time consuming, and requires expertise that is not broadly available. Furthermore, the lack of specifications for widely-used libraries and frameworks, caused by the high cost of writing specifications, tends to have a snowball effect. Core libraries lack specifications, which makes specifying applications that use them expensive. To contain the skyrocketing development and maintenance costs of high assurance systems, this self-perpetuating cycle must be broken. The labor cost of specifying programs can be significantly decreased via advances in specification inference and synthesis, and this has been attempted several times, but with limited success. We believe that practical specification inference and synthesis is an idea whose time has come. Fundamental breakthroughs in this area can be achieved by leveraging the collective intelligence available in software artifacts from millions of open source projects. Fine-grained access to such data sets has been unprecedented, but is now easily available. We identify research directions and report our preliminary results on advances in specification inference that can be had by using such data sets to infer specifications.|Visions (of the future):|accept|accept
ICSE|2015|Open-Source Project Classification with Deep Belief Network|hoan anh nguyen,tien n. nguyen|Open-source project classification,Deep learning,Deep belief network,High-level concept|Automated software categorization is important in supporting users to quickly find the relevant applications in a specific category in open-source software repositories. Researchers have investigated several automated approaches to categorize software projects using various techniques from Information Retrieval (IR) (e.g., Latent Semantic Indexing), to Machine Learning (ML) (e.g., Latent Dirichlet Allocation and other traditional ML models). In this work, we perform an empirical study to apply Deep Belief Network (DBN) in automatic software categorization. We implemented a DBN model that consists of four layers that takes the source code project and classifies it into categories. Our preliminary experimental result shows that DBN is able to recognize important features such as API calls and identifiers and use them in software categorization. It is able to form high-level concepts in a project from low-level features. More importantly, DBN achieves higher classification accuracy from 5.9-26.4% than traditional ML models.|Visions (of the future):|reject|reject
ICSE|2015|Architectural Clones: Toward Tactical Code Reuse|mehdi mirakhorli,daniel krutz|Tactical Clones,Architectural Tactics,Code Recommender|Architectural tactics are building blocks of software architecture. They describe solutions for addressing specific quality concerns, and are prevalent across many software systems. Once a decision is made to utilize a tactic, the developer must generate a concrete plan for implementing the tactic in the code. Unfortunately, this a non-trivial task for many inexperienced developers. Developers often use code search engines, crowd-sourcing websites, or discussion forums to find sample code snippets. A robust tactic recommender system can replace this manual internet based search process and assist developers to reuse successful architectural knowledge, as well as implementation of tactics and patterns  from a wide range of open source systems. In this paper we study several implementations of architectural choices in the open source community and identify the foundation of building a practical tactic recommender system. As a result of this study we introduce the concept of tactical-clones and use that as a basic element to develop our recommender system. While this NIER paper does not present the details of our recommender engine instead it proposes the notion which we will base our architecture recommender system upon.|Visions (of the future):|reject|reject
ICSE|2015|SimDev: An Automatic Testing Framework for Virtual Devices|tingting yu|Software testing,Virtual machine,Full-system simulator,Virtual device|A full-system simulator (FSS) is a class of system virtual machine (VM) that provides a variety of virtual hardware devices that are complete enough to run the real target software stack. The development of virtual devices can be challenging due to their complexity and ambiguous/inaccurate documentation. As FSSes have been widely used in various kind of activities for modern computer systems (e.g., design, testing, debugging), the defects in the virtual devices may cause cascading implications. However, little research effort has been made to verify the correctness of virtual devices. In this work, we present SimDev, an automatic framework for testing virtual devices. SimDev employs a feedback driven random testing technique, focused on the real hardware states, to cover diverse behaviors of the virtual devices, and it employs a hardware oracle strategy by taking advantage of the physical devices to eliminate the need for manually generating test oracles from hardware specifications. Our preliminary results show that SimDev can be effective at detecting real faults on a commercial virtual platform.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Using the Learning Styles of Software Professionals  to Improve their Inspection Task Performance|anurag goswami,gursimran s. walia,abhinav singh|software inspection,learning style,requirements|Inspections   of   artifacts   (requirement   and   design document) at initial stages of software development aids software managers to  prevent early defects that  may be  hard to find and fix  later.  Evidence  suggests  that  individual  inspection  abilities vary  widely  which  affects  inspection  effectiveness.  Cognitive psychologists  have  used  Learning  Styles  (LS)  to  measure  an individual’s  characteristic  strength  and  ability  to  acquire  and process information. This concept of LS is utilized in the software engineering as a means to improve inspection performance. This paper presents the results from a study regarding the effect of LS of  inspectors  on  the  fault  detection  effectiveness  of  inspection teams.    Using    inspection    data    from    twenty    professional developers, we analyzed the effect of LS of individual inspectors on  their  team  performance.  The  results  from  our  studies  shows that  the  inspection  teams  composed  of  inspectors  of  diverse  LS preferences are more effective as compared to teams of inspectors with similar LS preference|Visions (of the future):|reject|reject
ICSE|2015|On the Evolution of Fine-grained Attack Surface Metrics|nuthan munaiah,andrew meneely,kevin campusano gonzalez|Risk assessment,Attack surface metric,Product metric|As the interconnectedness of a software system grows, so does the number of ways attackers can break in. To engineer secure software, developers must continually assess and understand the risk of attack so that they can properly allocate their fortification efforts. We propose three new metrics-entry point reachability, exit point reachability, and shallow risk-that leverage the attack surface metaphor to assess the security risk from changes to the source code of a software system. We studied the evolution of these metrics by computing their values for 14 versions of the open source project, FFmpeg. We observed that the entry point reachability, exit point reachability, and shallow risk varied, on an average, by 3.56 percentage points, 1.52 percentage points, and 0.01 percentage points, respectively, and the size of FFmpeg's attack surface, expressed as a sum of number of entry points and exit points, varied, on an average, by 5.62 percentage points. We also observed that the variation in the size of FFmpeg's attack surface reduced with time but the variation in the reachability metrics did not. The variability of reachability metrics shows promise of the applicability of these metrics in producing a fine-grained assessment of the security risk from source code changes.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|A Behavior Marker tool for measurement of the Non-Technical Skills of Software Professionals: An Empirical Investigation|lisa lacher,gursimran s. walia,fabian fagerholm,max pagels,kendall nygard,jurgen munch|Non-technical Skills,behavior marker,performance|Managers recognize that recognize that software development project teams need to be developed and guided. Although technical skills are necessary, non-technical (NT) skills are equally necessary for project success. There are no proven tools to measure the NT skills of software developers. Behavioral markers (observable behaviors that have positive or negative impacts on individual or team performance) are beginning to be successfully used by airline and medical industries. The purpose of this research is to develop and validate the behavior marker system tool that can be used by different managers to measurement the NT skills of software development individuals and teams. This paper presents an empirical study conducted at the Software Factory where users of the behavior marker tool rated video clips of software development teams. The results validate that the behavior marker tool can be reliably used with minimal training|Visions (of the future):|reject|reject
ICSE|2015|Capsule-oriented Programming|hridesh rajan|modularity,concurrency,modular reasoning|“Explicit concurrency should be abolished from all higher-level programming languages (i.e. everything except - perhaps- plain machine code.). Dijkstra (paraphrased). A promising class of concurrency abstractions replaces explicit concurrency mechanisms with a single linguistic mechanism that combines state and control and uses asynchronous messages for communications, e.g. active objects or actors, but that doesn’t remove the hurdle of understanding non-local control transfer. What if the programming model enabled programmers to simply do what they do best, that is, to describe a system in terms of its modular structure and write sequential code to implement the operations of those modules and handles details of concurrency? In a recently sponsored NSF project we are developing such a model that we call capsule-oriented programming and its realization in the Panini project. This model favors modularity over explicit concurrency, forbids use of explicit threads and locks, encourages concurrency correctness by construction, and exploits modular structure of programs to expose implicit concurrency.|Initiatives (recently funded research projects):|accept|accept
ICSE|2015|Detecting Task Switches of Software Developers with Psycho-Physiological Sensors|manuela zueger,thomas fritz|task switch detection,psycho-physiological sensors,experiment|Software developers switch between tasks every few minutes. An accurate and automatic detection of task switches provides multiple benefits, such as better support for task recovery or task tracking. While previous work to detect task switches mainly uses computer interaction, recent advances in psycho-physiological sensor technology provide the opportunity to develop approaches that are not restricted to a computer and have the potential to more accurately reflect a developer's task engagement. In this paper, we present a first feasibility study on task switch detection using psycho-physiological sensors. The results of our study provide initial evidence that psycho-physiological sensors can be used to detect task switches with high accuracy and using small time windows of data.|Visions (of the future):|reject|reject
ICSE|2015|Nobody Does Replication Studies and That's Okay|laura inozemtseva,reid holmes|replication,verification,methodology|Replication studies are considered vital for ensuring the validity of previously published results.  However, they are rarely done in practice due to the difficulty of funding, executing, and publishing such studies.  We argue that this apparent problem is an artifact of conflating replication studies, which adhere to the procedure used in the original study as closely as possible, with extension studies, which consider the same research problem using a different procedure or subject pool.  Replication studies are rightly disincentivized because the impossibility of conducting an exact replication means that they cannot actually verify previously published results. Extension studies, by contrast, eventually verify previous results while also adding new knowledge to the field.  As they are much easier to conduct and publish than replications, they are common in practice. Thus, extension studies eliminate the need for replications.|Reflections (on the past):|reject|reject
ICSE|2015|Leveraging Informal Documentation to Summarize Classes and Methods in Context|latifa guerrouj,david bourque,peter c. rigby|Summarization,Code Elements,Informal Documentation,Context|Critical information related to a software developer's current task is trapped in technical developer discussions, bug reports, code reviews, and other software artefacts. Much of this information pertains to the proper use of code elements (e.g., methods and classes) that capture vital problem domain knowledge. To understand the purpose of these code elements, software developers must either access documentation and online posts and understand the source code or peruse a substantial amount of text. In this paper, we use the context that surrounds code elements in StackOverflow posts to summarize the use and purpose of code elements. To provide focus to our investigation, we consider the generation of summaries for library identifiers discussed in StackOverflow. Our automatic summarization approach was evaluated on a sample of 100 randomly-selected library identifiers with respect to a benchmark of summaries provided by two annotators. The results show that the approach attains an R-precision of 54%, which is appropriate given the diverse ways in which code elements can be used.|Visions (of the future):|accept|accept
ICSE|2015|Smart Programming Playgrounds|rohan padhye,pankaj dhoolia,senthil mani,vibha sinha|code snippets,prototyping,playgrounds,cloud computing,dependency injection|Modern IDEs contain sophisticated components for inferring missing types, correcting bad syntax and completing partial expressions in code, but they are limited to the context that is explicitly defined in a project's configuration. Thus, these tools are ill-suited for quick prototyping of incomplete code snippets picked off the web or shared by fellow colleagues to demonstrate the use of a particular API that is not already configured in the workspace. Further, in many cases, the resolution of programming context itself is insufficient to make a snippet of code executable, since it may involve interaction with external services that may need to be set-up separately. We propose smart playgrounds for enabling rapid prototyping of code snippets through automatic resolution of programming context (such as required APIs), provisioning of external resources (such as back-end data and compute services) as well as the binding of handles to these resources in the original code. Such a system could be potentially useful in a range of different scenarios, from sharing code snippets on the Web such as in Q&A sites or blogs to experimenting with new ideas during the process of traditional software development.|Visions (of the future):|accept|accept
ICSE|2015|Initiative on Consistent Change Propagation  in Software Design Models|alexander egyed|change impact,change propagation,inconsistency,design models|Design models capture software systems from different points of views – separating functionality, from structure, behavior, or usage. While these models are meant to be separate, they are nonetheless related by manifold dependencies. Changes in one model thus often require changes in other models. This is known as the change propagation problem. This paper reports on a recently funded, large initiative that explores consistency-driven change propagation. The key observation is that incorrect or incomplete change propagation is detectable if it causes inconsistencies. In such cases, repair suggestions are generated based on the inconsistencies caused and these suggestions then serve as recommendations for further change propagations. A novelty of this initiative is that it does not see inconsistencies in isolation but understands their cumulative effects such that subsequent change propagations do not contradict earlier changes. This idea also helps us cope with an otherwise exploding space of possible change propagation alternatives. This initiative funds two researchers and a programmer over 3 years who will 1) research an approach for consistent change propagation, 2) develop a proof-of-concept tool to demonstrate feasibility, and 3) evaluate the tool on industrial models.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|StackOverflow Code Quality Model: An Exploratory Study|mohammad masudur rahman,chanchal kumar roy,iman keivanloo|Objective code quality,subjective evaluation,maintainability,StackOverflow|In StackOverflow, code examples are generally analyzed and subjectively evaluated by a large crowd of technical users. Given the growing interest of the community to those examples and their undeniable role in answers, we are motivated to study whether the subjective quality of those examples as perceived by StackOverflow actually matches with their metric-based quality. In this paper, we propose and evaluate a metric-based quality model for StackOverflow code examples by conducting an exploratory study where we analyze 160 carefully selected code examples from 80 programming problems. Our quality model agrees with StackOverflow for 87.03% of the test examples in relative quality prediction, which is promising, and the finding reveals effectiveness and reliability of the subjective evaluation by StackOverflow.|Visions (of the future):|reject|reject
ICSE|2015|Capturing Component Interaction Information in a Production Environment|atul kumar|Dynamic Analysis,Root cause analysis,network packet filtering and analysis|The root-cause analysis after a system failure/error is an important activity to determine the exact reasons for failure/error. Most of the time, these error conditions cannot be reproduced or it is not feasible to run the system again using the exact same scenario. Therefore, the execution trace log of various functions/components recorded during the event is essential for root cause analysis and debugging in a complex system. The source code level instrumentation for dynamic analysis provides accurate execution trace log. But it is difficult to use an instrumented system in the production environment because of the performance and system stability issues. In a distributed system, intercepted network messages can be analyzed to identify the interaction between various components of the system. However, the messages captured on the network alone do not provide complete information. That is because the messages between components on the same host would not appear on the network. We present a new idea to construct interaction information among the components of a distributed application using the messages captured on the network and an interaction model that is a set of rules and heuristics about component interaction. The interaction model is pre-built off line using the profile information and the static control flow graph of the system. The profiling is done with test data in a non-production environment such as a test environment using `close-to-real' test scenario. Messages corresponding to components interaction are captured on the network to create a partial execution trace log. Then the trace log is completed using the pre-built interaction model.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Context-Aware Reliability Prediction of Black-Box Services|jieming zhu,zibin zheng,michael r. lyu|Reliability prediction,Software services,Matrix factorization|Reliability prediction is an important research problem in software reliability engineering, which has been widely studied in the last decades. However, modelling and predicting user-perceived reliability of black-box services remain an open and challenging problem. Software services, such as Web services and Web APIs, generally provide black-box functionalities to users through the Internet. The new challenges imposed by such black-box services make traditional reliability models inapplicable for predicting user-perceived service reliability. To address these challenges, in this paper, we present a context-aware reliability prediction approach, CARP. Through context-aware model construction and prediction, CARP is able to alleviate the data sparsity problem that heavily limits the prediction accuracy of other existing approaches. The preliminary experimental results show that CARP can make significant improvement on prediction accuracy (up to 41% for MAE and 38% for RMSE), especially when the training data is sparse.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Crowdsourcing Requirements with E-Democracy?|timo johann,walid maalej|E-Democracy,Liquid Democracy,User Involvement,User Partizipation,Delegated Voting,Conflict Resolution|User involvement is a major success factor for software projects. For instance, interviews, surveys, or bug reporting are popular approaches for identifying requirements and planning the evolution of software. Pushed to the extreme, user involvement can be called crowdsourcing requirements. In this case the crowd of all users is systematically responsible for developing the requirements and deciding about future releases. While the idea sounds promising in the age of e-participation and social media, the main issues are the scalability to thousands or millions of initiatives, the subjectiveness (quality) of the involvement, as well as emerging conflicts. This paper discusses, how concepts from the E-Democracy can enable the crowdsourcing of requirements – in part.|Visions (of the future):|reject|reject
ICSE|2015|Human Software Interaction in Software Development|xin yang,kar-long chan,papon yongpisanpop,hideaki hata,hajimu iida,ken-ichi matsumoto|Human Software Interaction,Project Collaboration,Knowledge Sharing|Data-driven decision making is a promising direction in empirical software engineering. Since data mining on software data is challenging, Mining Software Repositories (MSR) has been an active research field, which analyzes various types of large-scale software data. In MSR, finding just interesting patterns is not enough, providing insightful and actionable information for practitioners is required recently, which leads to Software Analytics (SA), analytics on software data. From the viewpoint of practical software development scenario, it is required that software analytics need to be real time and actionable, and need to share information. This, in other words, we must think about the interaction with software data and the interaction between practitioners. However current text or some visualization data representation may not be suitable for interaction with data, and traditional keyboard-display environment may not be useful for collaboration. We've started tackling this interaction problems, and call this research field ``Human Software Interaction (HSI).'' Intuitively, HSI begins with the combination with MSR/SA and HCI, Human Computer Interaction, and not the part of HCI. This paper presents the challenges including data interaction, software data representation, collaboration, and human studies. We also show the visions of future software development and HSI research. We also show the visions of future software, in which we will apply gamification to construct a framework called guild. By establishing the concept of guild, we hope to construct efficient interaction with data and collaboration between practitioners.|Visions (of the future):|reject|reject
ICSE|2015|Property-Based Methods for Collaborative Model Development|marsha chechik,fabiano dalpiaz,csaba debreceni,jennifer horkoff,istvan rath,rick salay,daniel varro|Collaborative Modeling,MDE,Property Checking,Model Views,Model Locks|Large-scale software projects are often faced with the challenge of enabling the high degree of collaborative and concurrent development required to meet the aggressive delivery schedules while still maintaining a high standard of system correctness and safety. While Model Driven Engineering (MDE) can be effective in accelerating development, traditional approaches to addressing the above challenges are designed for code, and are not directly applicable to models. In this paper, we propose a novel approach to addressing the problem of model-based collaborative development using property-based techniques. We illustrate our proposals, outline the challenges to realizing them and present some preliminary results.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Don’t Cry Over Spilled Keys: Fixing Key Leaks in Shared Code Repositories|vibha sinha,diptikalyan saha,pankaj dhoolia,rohan padhye,senthil mani|privacy,program analysis,version management systems|In early 2014, there were quite a few articles that reported how developers had accidentally checked-in their Amazon Web Services account keys within code on GitHub. These keys were exploited by malicious users to run their own work-loads which drove up usage bills of legitimate users. As a preventive measure, the AWS documentation now urges developers to externalize keys into a separate file and add it to an ignore list to prevent it from being checked-in to repositories. However, considering the growing popularity of shared code management systems, in-spite of all best practice documentation, such accidental key leaks still happen. Moreover, the leaking of keys is not an AWS specific problem, but would plague any software service provider such as Google, CoinBase, Facebook etc., that allow developers to connect to their services using developer keys or client id/secrets. In this paper we propose extending version management systems with the capability of detecting such key leaks. When a developer attempts to commit their code, the version management system highlights the leak and also provides recommendations on how to fix it. We used two approaches for detecting key leaks. One of them searches committed code for typical key patterns. The other one uses static data flow analysis to identify arguments to functions that take as input key values, and traces these values to their initialization. We present an evaluation of these techniques on 84 GitHub projects.|Initiatives (recently funded research projects):|reject|reject
ICSE|2015|Characterizing Test Intent Using Symbolic Execution|xiangyu li,alessandro orso|Test intent,Test repair,Symbolic execution|The high cost of repairing broken (unit) tests in evolving software systems inspired a good deal of recent research on automated unit test case repair. An essential challenge for developing more effective test repair techniques is the preservation of the intent of the original tests. In this paper, we propose a technique for characterizing such intent using the path conditions generated during dynamic symbolic execution of the test cases. To validate our technique, we conducted a preliminary empirical study on a real-world software system. Our results provide initial evidence that (1) our approach can be effective in characterizing test intent and (2) achieves better results than two alternative, simpler techniques that we used as baselines.|Visions (of the future):|reject|reject
ICSE|2015|On Software Defect Prediction: Classification vs. Ranking|xuan huo,ming li|Mining Software Repository,Software Defect Prediction,Classification,Ranking,Software Quality Assurance|Software defect prediction aims to identify defectprone software modules in the purpose of allocating limited SQA resources in a cost-effective manner. To identify these defectprone modules, a classification model is usually built to classify software modules into defective or non-defective. However, a number of false alarms may be generated if the classifier is not so strong or training modules are insufficient, which may waste many SQA resources especially when the identified defectprone modules are more than the available SQA resources. A straight-forward way is to prioritize the defectprone modules to be checked based on the predictive confidence generated by the classifier. Nevertheless, due to the `point-wise' learning nature of the classifier, such predictive confidence may not be effective in generating the ordering of defectproneness. To address this problem, we argue that the defect prediction model should be generated from a `pairwise' perspective instead of a `point-wise' perspective, to explicitly modeling the pairwise relation between software modules to help improve the prediction. We formulate the software defect prediction task as a learning to rank problem and address it with Ranking SVM. Empirical studies on 12 open source software projects indicate that ranking model is able to provide better module prioritization than the classification model, leading to a more cost effective SQA.|Reflections (on the past):|reject|reject
ICSE|2015|A Empirical Study on the Status of Software Localization in Open Source Projects|zeyad alshaikh,xiaoying wang|Software Localization,Open Source Projects,Empirical Study|In modern software development, software localization is a key process to support distribution of software products to the global market. During software localization, developers typically convert all user-visible strings, resource files, and other culture-related elements to the local versions that are well accepted by local users. Despite the popularity of software localization, there have been little studies on the its current status in software practice, such as the proportion of localized projects, the most popular locales, and more importantly, the quality of software localization. In this paper, we present an empirical study on the status of software localization in top open source projects. We find from the study that, popularity of software localization varies a lot in different software domains, User Interface (UI) frameworks, and programming languages. Furthermore, we surprisingly find that only about 60\% of string keys are actually translated on average in localized projects and software localization often span a long period of time in software development history.|Reflections (on the past):|reject|reject
ICSE|2015|Measuring Behaviour Interactions between Product-Line Features|joanne m. atlee,ulrich fahrenberg,axel legay|feature-oriented software development,software product line,automatic analysis of feature interactions,bisimulation|We suggest a method for measuring the degree to which features interact in feature-oriented software development. We argue that our method is practically feasible, easily extendable and useful from a developer’s point of view.|Visions (of the future):|reject|reject
ICSE|2015|Incorporating Human Intention into  Self-Adaptive Systems|shihong huang,pedro miranda|Brain computer interface (BCI),human computer interface (HCI),neural input,self-adaptive systems,overt and covert behavior and attention,human in the loop|Self-adaptive systems are fed with contextual information from the environments in which the systems operate, from within themselves, and from the users. Traditional self-adaptive systems research has focused on inputs of systems performance, resources, and exception error recovery that drive systems’ reaction to their environments. The intelligent ability of these self-adaptive systems is impoverished without knowledge of a user’s covert attention (thoughts, emotions, feelings). As a result, it is difficult to build effective systems that anticipate and react to users’ needs as projected by covert behavior. This paper presents the preliminary research result on capturing users’ intention through neural input, and, in reaction, commanding actions from software systems—e.g., load an application—based on human intention. Further, systems can self-adapt and refine their behaviors driven by such human covert behavior. The long-term research goal is to incorporate and synergize human brain state and neural input to establish software systems with a self-adaptive capability to “feel” and “anticipate” users intentions, and put the human in the loop to respond to the response.|Visions (of the future):|accept|accept
ICSE|2015|Toward Reusing Code Changes|yoshiki higo,shinpei hayashi,hideaki hata,shinji kusumoto|Change reuse,Mining software repositories,Code clone|Existing techniques have succeeded to help developers implement new code. However, they are insufficient to help them change existing code. Some tools such as GenProg and PAR support bug fixes but other kinds of code changes such as function enhancement and refactoring are not supported by them. In this paper, we propose a novel framework that helps developers change existing code. Unlike existing techniques, our framework can support any kinds of code changes if similar code changes occurred in the past. Our research is still on very early stage and we have not have any implementation or any prototype yet. This paper introduces our research purpose, an outline of our framework, and how our framework is different from existing techniques.|Visions (of the future):|reject|reject
ICSE|2015|Managing Self-Adaptive Architecture Trade-offs: A Game Theoretical Vision|maria salama,rami bahsoon|Software Architecture,Self-Adaptive Architecture,Trade-offs Management,Sustainability,Game Theory|This paper discusses the vision of engineering managing architecture trade-offs during run-time for self-adaptive software using game theory. It first examines the current research state of managing architecture trade-offs. It then envisions the use of game theoretic approach for  quantitatively managing the trade-offs arising from multiple emergent requirements during run-time, system and environmental goals, and reasoning about architectural self-adaptation process to achieve get long-lived of software, for a empowered by a proactive design.. Next, it sketches the research impact, and then identify the research challenges imposed.|Visions (of the future):|reject|reject
ESEM|2016|Identifying stakeholders of a mature hybrid OSS ecosystem:  The community management viewpoint|hanna maenpaa,terhi kilamo,myriam munezero,fabian fagerholm,mikko nurminen,tomi mannisto|hybrid open source,community management,software development tools,social network analysis|Background: Hybrid Open Source Software projects establish a particular working context where a versatile set of commercial and non-aligned stakeholders collaborate using openly available software development tools. The role of the community's manager is to advocate, organize and manage the collaboration of the stakeholders to ensure the productivity of the community's work. Aim: Forming a clear picture of a software development community is complicated, which we aim to alleviate by presenting a simple analysis and visualization approach. Method: A single case study, using a snapshot of logs of socio-technical systems of the complete software development workflow. We use Social Network Analysis to present an overview of active stakeholders and the roles they act in. The relevance of findings is validated through interviews with a community manager. Results: A variety of roles that are shared by both commercial and independent stakeholders at different stages of the development process are found. Based on this, the depth of their involvement in the process can be identified. The community manager identifies stakeholder groups that are potential for increasing contributions of developers and finds the visualization approach to support his understanding of the entry path of new developers. Stakeholder groups where fluctuation of members might indicate problems in the organization of the software development process are identified. Conclusions: For a community manager to understand the ecosystem context, observing a single source of information on who the stakeholders are and what is the frequency of their contributions is not enough. Our approach helps community managers in identifying which individuals and teams could be addressed for support or rewarding and what could be indicators of problems in an organization's software development process.|None|reject|reject
ESEM|2016|Prioritizing Random Combinatorial Test Suites: An Empirical Study|rubing huang,weiwen zong,jinfu chen,dave towey,yunan zhou|Software testing,combinatorial testing,random testing,random combinatorial test suite,test case prioritization|The behaviour of the system under test has been influenced by some factors, such as system configurations, user inputs, and so on, and many failures have been caused by a small number of factors. Combinatorial testing aims at generating a small-sized test suite to identify these failures; while random testing selects test cases in a random manner, which has been demonstrated that it sometimes has similar performance to combinatorial testing. Due to limited testing resources, test case prioritization has been employed, in order to identify failures as earlier as possible. However, many prioritization studies have focused on test suites constructed by combinatorial testing rather than random testing. In this paper, we attempt to investigate test case prioritization upon random combinatorial test suites, and further check whether there exist some different observations. We conducted a series of empirical studies involving two real-world programs, each of which uses twelve random combinatorial test suites, and adopting a well-known test case prioritization technique, i.e., fixed-strength interaction coverage based prioritization (FICBP). Different to the results of previous studies, our experimental results present some interesting findings. On the one hand, FICBP with high prioritization strength achieves better interaction coverage rate than that with low prioritization strength. On the other hand, when the size of random test suite is small, it would be better to assign the prioritization strength as two; otherwise, strength three would be more suitable.|None|reject|reject
ESEM|2016|A Distributed Cognition Perspective on Remote Working in Agile teams|advait deshpande,helen c. sharp,leonor barroca,peggy gregory|Remote working,Distributed Cognition,DiCoT,Agile|Background: Distributed Agile teams are now an accepted aspect of software practice, and have been extensively researched. However, remote working, when only one or two individuals are not co-located with the rest of the team (a hybrid team), remains largely unexplored. Aims: The aim of this paper is to present a comparative analysis of collaboration in a hybrid team by investigating information flows, and the use of artefacts and physical space for a remote worker and his co-located office team. Method: We use distributed cognition, a framework for analysing collaborative work, to analyse the information flows around and the use of artefacts and physical space by a remote worker and his corresponding co-located teammates. Results: There are substantial differences between the remote worker and his co-located team in three aspects: virtual artefacts have more significance for the remote worker’s engagement in the project than for his co-located teammates; information flow around the remote worker is more formalised and deliberate than in the co-located team; and hence, collaborative platforms are central to the remote workers’ activities. Conclusions: Collaborative platforms that create parity between the co-located members of the team and the remote worker, in information access, in understanding and in engagement with the project as it evolves, are crucial to integrating capability in an otherwise co-located Agile team.|None|reject|reject
ESEM|2016|Developing Processes to Increase Technical Debt Visibility and Manageability – An Action Research Study in Industry|jesse yli-huumo,andrey maglyas,kari smolander,johan haller,hannu tornroos|Technical debt,Technical debt management,Software process improvement,Action research|Background: The knowledge on technical debt and its management has increased in recent years. The interest of academia and industry has generated many viewpoints on technical debt. Technical debt management consists of technical and organizational aspects, which make it a challenge in software development. Aim: To increase technical debt visibility and manageability, new processes must be developed and thoroughly empirically tested for their applicability. Method: In this paper, we use the action research methodology to design processes for identification, documentation, and prioritization of technical debt. Our partner in this research is a large Nordic IT company Tieto, currently in a need for new ways to improve their technical debt management. Results: The results include a set of processes and templates that were successfully used to identify and document technical debt. The identified technical debt items were later prioritized based on evaluation by Tieto employees. Conclusions: Tieto was able to create a prioritized technical debt backlog, which is now used for reduction activities to create more healthy and sustainable product for future. This was considered as a success and therefore Tieto is planning to use the same process to other product lines in near future.|None|reject|reject
ESEM|2016|Towards Effectively Test Report Classification to Assist Crowdsourced Testing|junjie wang,qiang cui,qing wang,song wang|Crowdsourced testing,Report classication,Cluster|Background: Automatic classification of crowdsourced test reports is important due to their tremendous sizes and large proportion of noises. Most existing approaches towards this problem focus on examining the performance of different machine learning or information retrieval techniques, and most are evaluated on open source dataset. However, our observation reveals that these approaches generate poor and unstable performances on real industrial crowdsourced testing data. We further analyze the deep reason and find that industrial data have significant local bias, which degrades existing approaches. Aims: We aim at designing an effective approach to overcome the local bias in real industrial data and automatically classifying true fault from the large amounts of crowdsourced reports. Method: We propose a cluster-based classification approach, which first clusters similar reports together and then builds classifiers based on most similar clusters with ensemble method. Results: Evaluation is conducted on 15,095 test reports of 35 industrial projects from Chinese largest crowdsourced testing platform and results are promising, with 0.89 precision and 0.97 recall on average. In addition, our approach improves the existing baselines by 17% - 63% in precision and 15% - 61% in recall. Conclusions: Results imply that our approach can effectively discriminate true fault from large amounts of crowdsourced reports, which can reduce the effort required for manually inspecting the reports and facilitate project management in crowdsourced testing. To the best of our knowledge, this this the first work to address the test report classication problem in real industrial crowdsourced testing practice.|None|accept|accept
ESEM|2016|Elements of a Programmer Performance Model|dror g. feitelson|programmer performance,individual differences,skill,knowledge,motivation,conceptual model|Background: there is evidence that programmers may differ significantly in their capabilities, but what exactly the differences are and what causes them is unclear. Understanding this is important both for industry and for software engineering research. Aims: make progress toward a comprehensive model of factors affecting programmer performance and their interactions. Method: consider factors that have been suggested and possible interactions. Results: the emerging model revolves around two main inputs. One is the combined effect of education, experience, and practice, via their effect on knowledge and skills. The other is motivation, including the effect of interest and feedback from performance. Both are modulated by the project and the environment. Conclusions: A lot of additional empirical work is needed to better understand factors and their interactions.|None|reject|reject
ESEM|2016|Non-random Motifs in Software Systems|barbara russo|Complex Networks,Design patterns,Call Graphs,Network Motifs|Background. Complex networks such as  protein interaction networks or  electronic circuits exhibit specific common global traits. Such characteristics distinguish them from random networks and have been proved to indicate enhanced internal information flow. Software systems can be modelled as complex networks with the same global characteristics by means of their dependency structure. Software systems can be very different in their basic sub-structures though.  Their local organisation reflects their evolutionary nature: some of their recurring sub-structures are intentionally coded to make them evolvable (i.e., design patterns),  others may arise because of  human interventions during their evolution (i.e., emergent sub-structures). Aims.  In this work, we aim at profiling software systems as complex networks by their non-random sub-structures (motifs),  analyse them across consecutive releases, and search for those that include specific design patterns. Our overall goal is to explore a new means to compare software systems through their micro-organisation. Method. We build the dependency structure  of subsequent versions of five open source software systems and extract their non-random sub-structures with three nodes, size-3 motifs. We  profile each network using such substructures. We compare the resulting profiles across the versions of the same project and across those of different  projects. We then discuss  the probability such motifs include design patterns. Results. We found that the software systems under study have common  motifs that profile their global evolution pinpointing peculiar evolutionary phases. We found that depending on the project some of the motifs constantly include design patterns over their versions, whereas other motifs are good candidates for emergent sub-structures. Discussion. Dependency sub-structures emerging non randomly can be used to profile the micro-architecture of software systems and characterise their evolution.|None|reject|reject
ESEM|2016|Identification of Barriers and Solutions in the SLR Search Phase: A Systematic Literature Review|ahmed al-zubidy,jeffrey carver|Systematic Literature Review,Searching,Tool,Barriers,User Experience Research|Background: In order to make Systematic Literature Reviews (SLR) a useful tool for a larger portion of the software engineering community, there is a need to identify and remove barriers. These barriers reduce the likelihood that researchers will undertake an SLR. Based on our previous work, we have identified the Searching phase as the source of may such barriers.Aims: In this work, we seek to identify the specific barriers in the SLR searching process and document the solutions that researchers have used to address those barriers. As a result, we identify which barriers are still problematic and in need of further study. Method: We conducted a systematic literature review to identify SLR searching barriers and solutions reported in published SLRs. Results: We identified 86 papers that reported SLR searching problems. These problems result from issues with the digital libraries, missing tool support, or missing guidelines. Conclusions: There are a number of barriers in the SLR searching process that still remain. Many of the solutions employed are manual and ad hoc. Furthermore, current tool support is still very limited and does not solve the problem completely. Therefore, there is a need for additional work to address the identified barriers.|None|reject|reject
ESEM|2016|What Developers Think about Feature Prioritisation in Open Source Software Development|barbara russo,gabriele bavota|Feature prioritisation,Open Source Development,Survey|Background. When a new release of a software product is planned, a number of important decisions must be taken. Among those, it is of paramount importance the prioritisation of new features to implement in the next release given the available resources (time, developers, etc.). This activity is known as feature prioritisation. Aims. Previous work studied strategies adopted in software companies while performing feature prioritisation. However, there is still a lack of knowledge of how feature prioritisation is performed in Open Source Software (OSS) projects. This paper aims at investigating (i) how features are prioritised in OSS, (ii) the difficulties they experience during feature prioritisation, and (iii) the kind of tool they would like to have as support when performing feature prioritisation. The ultimate goal of our study is to identify possible research areas related to the development of techniques/tools supporting OSS developers in feature prioritisation activities. Method. This paper presents the results of a survey conducted with 244 OSS developers. To introduce the survey, the respondents were presented with a scenario of feature prioritisation. The on-line questionnaire provided respondents with closed and open questions. Open questions were used to gather further information of feature prioritisation in OSS development and features of a tool supporting such process. Results. OSS developers experience difficulties during feature prioritisation activities, mainly because feature prioritisation in OSS requires time, consensus, and user involvement. For them, User needs is the most important factor to take into account for prioritising features. They also have a clear interest in a tool for feature prioritisation and a clear idea of which features this tools should have. Conclusions. Besides giving an overview of how feature prioritisation activities are run in OSS projects, our study underlines the need for a tool supporting such activities. The results of our survey also provides indications on what features developers expect from such tools.|None|reject|reject
ESEM|2016|Assessment of Source Code Obfuscation Techniques|alessio viticchie,leonardo regano,marco torchiano,cataldo basile,mariano ceccato,paolo tonella,roberto tiella|Code obfuscation,Tamper-resistant,Comprehension|Context: code obfuscation is a portmanteau approach widely adopted to prevent malicious tampering of the code. Individual techniques have been developed to make the code more difficult to understand and thus harder to modify. Goal: this paper aims at assessing the effectiveness and efficiency in preventing an attack task of a specific data obfuscation technique – VarMerge. Method: we conducted an experiment with student partic- ipants performing two attack tasks on clear and obfuscated versions of two applications written in C. Results: the experiment showed a significant effect of code obfuscation on both the time required to complete and the successful attack efficiency. Conclusions: programs obfuscated with VarMerge are able to reduce by six times the number of successful attack per unit of time. This outcome provides a practical clue that can be used when applying software protections based on code obfuscation.|None|reject|reject
ESEM|2016|Rigor, Relevance, and their Relationship to Scientific Impact in Software Engineering Studies|jefferson molleri,kai petersen,emilia mendes|empirical software engineering,quality assessment,scientific impact,exploratory study,conditional inference tree|Background: The importance of achieving high quality in research studies has been highlighted, in particular Ivarsson and Gorschek proposed an evaluation rubric. At the same time, citations are utilized to measure the impact of academic researchers and institutions. An open question is whether study quality represented by rigor and relevance is related to impact, which would be desired. Objective: In this exploratory study we aim to: 1) investigate how the scoring rubric by Ivarsson and Gorschek for rigor and relevance has been used to assess study quality; 2) analyze the level of rigor and relevance scores achieved for different research areas in Software Engineering (SE); and 3) explore the relationship between rigor, relevance and academic impact defined as normalized citations. Methods: We identified 10 secondary studies using the proposed rubrics to assess the quality of 682 included papers. We utilized cluster analysis and conditional inference trees to explore the relationship between quality (here represented by rigor and relevance) and scientific impact (here represented by normalized citations). Results: Results showed that only rigor is related to studies’ normalized citations. These results suggest that the two dimensions are not applied equally by the academic community. We also show that other confounding factors could influence the number of citations. Conclusion: Our findings could be used as basis to further understand the relation between study quality and research impact, and foster new discussions on how to fairly acknowledge research studies for performing well with respect to the emphasized quality.|None|reject|reject
ESEM|2016|An Improved Approach to Self-adaptive Task-Worker Matching in Software Crowdsourcing|lihang gong,haopeng chen,ying fu,feiya song|Crowdsourcing,Meta-model,Task-worker matching,Team formation,Sliding window analysis|Crowdsourcing engages a workforce to accomplish complex tasks regardless of geographical limitation, which is now growing rapidly in a variety of areas. Despite the fact that the selection of a wide array of workers has created a competitive and flexible market that suits well the needs of different kinds of employers, a major challenge is how to discover cost-effective workers that satisfy the requirements of the employers best among a large number of workers while guaranteeing high quality of deliveries. In this paper, based on precedents of our lab, an improved approach and advanced meta-model are proposed for customising tasks and workers description, which allows self-adaptive matching of the task requirements against workers' skills. What is more, a worker skills assessment is introduced to estimate workers' skills dynamically on the basis of the feedback evaluation. Furthermore, several workers will be chosen to form a team once a single individual doesn't meet the requirements of the task. A full experimental validation with four tasks and thousands of workers has been done showing the validation of our approach.|None|reject|reject
ESEM|2016|Hidden Markov Models for a Fine-Grained Study of Developer Roles and Involvement Dynamics|verena honsel,jens grabowski|Hidden Markov Models,Developer Roles,Software Development|The evolution of software projects is driven by developers who are in control of the developed artifacts. When analyzing the behavior of developers, the observable behavior, e.g., commits, messages, or bug assignments, can be easily assessed. For defining dynamic activities and workload of developers, we are interested in underlying characteristics, which means the level of involvement according to their role in the project. In this paper, we propose to employ Hidden Markov Models (HMMs) to model this underlying behavior given the observable behavior as input. For this, we observe monthly commits, bugfixes, mailing list activity, and bug comments for each developer over the project duration. As output we get a model for each developer describing how likely it is to be in a low, medium, or high contribution state to every point in time. As a result, on the one hand we discovered that same developer types exhibit similar models in terms of state patterns and transition matrices, which represent their involvement dynamics. On the other hand, the workload of the different developer roles related to this is more complex to model.|None|reject|reject
ESEM|2016|Experience-based guidelines for effective and efficient data extraction in systematic reviews in software engineering|vahid garousi,michael felderer|Systematic mapping studies,systematic literature reviews,research methodology,data extraction,empirical software engineering|To systematically collect evidence and to structure a given area in software engineering (SE), Systematic Literature Reviews (SLR) and Systematic Mapping (SM) studies have become common. Data extraction is one of the main phases (activities) when conducting a SM or a SLR, whose objective is to extract required data from the primary studies and to accurately record the information researchers need to answer the questions of the SM/SLR study. Based on experience in a large number of SM/SLR studies, we and many other researchers have found the data extraction in SLRs to be time consuming and error-prone, thus raising the real need for heuristics and guidelines for effective and efficient data extraction in these studies, especially to be learnt by junior and young researchers. As a ‘guideline’ paper, this paper contributes a synthesized list of challenges usually faced during SLRs’ data extraction phase and the corresponding solutions (guidelines). For our synthesis, we consider two data sources: (1) the pool of 16 SLR studies in which the authors have been involved in, as well as (2) a mini-SLR of challenges and guidelines in the existing literature. Our experience in utilizing the presented guidelines in the near past have helped our junior colleagues to conduct data extractions more effectively and efficiently.|None|reject|reject
ESEM|2016|Documentation in Agile Software Development|stefan voigt,jorg von garrel,julia muller,dominic wirth|information behavior,agile documentation,agile software development|Although agile methods have become established in software engineering, documentation in projects is rare. Employing a theoretical model of information and documentation, our paper analyzes documentation practices in agile software projects in their entirety. We apply methodological triangulation to present an overall picture of the demand for information and supply of documentation. As an example, we demonstrate the correlation between information search satisfaction and the amount of documentation that exists for some types of information. Searches demand nearly twice as much time as documentation. Our paper explores the reasons for insufficient documentation in agile projects. In the conclusion, we provide recommendations on the use of supporting methods or tools to shape agile documentation.|None|reject|reject
ESEM|2016|Moving to Stack Overflow: Best-Answer Prediction in Legacy Developer Forums|fabio calefato,filippo lanubile,nicole novielli|Developer forums,Stack Overflow,Q&A sites,Best-answer prediction|Context: Recently, more and more developer communities are abandoning their legacy support forums, moving onto Stack Overflow. The motivations are diverse, yet they typically include achieving faster response time and larger visibility through the access to a modern and very successful infrastructure. One downside of migration, however, is that the history and the crowdsourced knowledge hosted at previous sites remain separated or even get lost if a community decides to abandon completely the legacy developer forum. Goal: Adding to the body of evidence of existing research on best-answer prediction, here we show that, from a technical perspective, the content from existing developer forums might be automatically migrated to the Stack Overflow, although most of forums do not allow to mark a question as resolved, a distinctive feature of modern Q&A sites. Method: We trained a binary classifier with data from Stack Overflow and then tested it with data scraped from Docusign, a developer forum that has recently completed the move. Results: Our findings show that best answers can be predicted with a good accuracy, only relying on shallow linguistic (text) features, such as answer length and the number of sentences, combined with other features like answer upvotes and age, which can be easily computed in near real-time. Conclusions: Results provide an initial yet positive evidence towards the automatic migration of crowdsourced knowledge from legacy forums to modern Q&A sites.|None|accept|accept
ESEM|2016|Context Aware Requirements Elicitation and Modeling in Empirical Studies:A Systematic Review|li yuanbang,peng rong,ji jingjing,wang bangchao|systemtic literatual review,empirical study,requirements elicitaiton,requirements modeling,technology transfer|Background: Context-aware mobile applications are more and more appealing as they can adapt their behavior to the current situation. But eliciting and modeling the context-aware requirements of these context-aware system become big challenges as the dimension of contexts are countless. Therefore, learning from the existing studies can help both academia and industry meet the challenges. Aims: This main objectives of this study are: (1) finding out which dimensions of context have been attached great importance in context-aware requirements elicitation and modeling processes, (2)finding out the methods used in the processes, and (3) evaluating the technology transfer maturity of these methods. Method: We adopt the methodology of systematic literature review to search relevant empirical studies, extract available data aimed at the objectives and assess the quality of these studies. Results: 61 studies are finally selected, in which 5 first-class context dimensions,11 requirements elicitation methods,10 requirements modeling methods, 5 evaluation methods and their technology transfer status are identified. Conclusions: After synthesizing the data by statistical methods, the results show that (1) individualization and action are most popular dimensions considered, followed by location and relations, and then time dimension; (2)user case and scenario are the most popular means to elicit context-aware requirements, and various goal-oriented methods are used to model context-aware requirements; (3)the evidences have proven that technology transfer is not well done in majority of these empirical studies. Based on the above findings, this paper summarizes the status-of-the-art of the domain, and points out the future research directions, which aim to help researchers get a better understanding on this domain.|None|reject|reject
ESEM|2016|Empirical Evaluation of Energy Efficiency in ORM Approaches|giuseppe procaccianti,patricia lago,wouter diesveld|Energy Efficiency,Object-Relational Mapping,Empirical Experiment|Background. Object-Relational Mapping (ORM) frameworks are widely used in business software applications to interact with database systems. Even if ORMs introduce several benefits when compared to a plain SQL approach, these techniques also have known disadvantages. Aims. In this paper, we present an empirical study that evaluates the energy efficiency of three different approaches to programmatically access SQL databases in PHP applications. The selected approaches are: plain SQL queries in the source code, and two specialized frameworks, Propel and TinyQueries. Method. We performed an experiment in a controlled environment. We selected three factors for our experimentation: the different ORM approaches, the type of query (Create, Read, Update, Delete) and the size of database tables. Our response variables were execution time and energy consumption. Results. As expected, pure SQL yielded the best performance and energy efficiency in all test cases. Propel exhibited a much higher energy consumption and longer execution times. The TinyQueries tool performed slightly worse than SQL, but significantly better than Propel, offering a convenient trade--off between ORM benefits and energy efficiency. Conclusions. Our experiment shows that ORM approaches have a significant impact on both energy consumption and performance. Hence, developers and architects need to consider the trade-off between their benefits (e.g. in terms of code maintainability and readability) and drawbacks.|None|accept|accept
ESEM|2016|A Systematic Review of Open Data Quality|marco torchiano,mohammad rashid|Open Data,Quality,Systematic Literature Review,Metrics|Context: The main objective of open data is to make information freely available through easily accessible mechanisms and facilitate exploitation. However usefulness of information depends strongly on the data quality. Due to the diver- sity and complexity of open data applications many research has recently focused on methodologies to assess open data quality. Objective: We aim to identify and synthesize various features of open data quality approaches, in particular application domain, quality dimensions, strategies and techniques. Method: We undertook a systematic literature review to summarize the state of the art on open data quality. We applied thematic synthesis to identify the most relevant research problems and quality assessment methodologies. Studies are classified according to open data application domains. Results: There are many quality issues concerning open data hindering their actual usage for real-world applications. The main ones are unstructured meta data, heterogeneity of formats, lack of accuracy, incompleteness and lack of validation techniques. Furthermore we collected the existing quality methodologies from selected papers and synthesize under a unifying classification schema. Also a list of quality dimensions and metrics from selected paper is reported. Conclusion: Open data quality methodologies vary depending on the application domain. Moreover the majority of studies focus on satisfying specific quality criteria. With metrics based on generalized data attributes a platform can be created to evaluate all possible open dataset.Also lack of methodology validation remains a major problem. Studies should focus on validation techniques.|None|reject|reject
ESEM|2016|Tracking Misconfiguration Errors in Dynamically-typed Multi-layer Systems - Empirical Study on Wordpress|mohammed sayagh,bram adams|Multi-layer systems,Dynamically-typed systems,Software configuration,Dynamic slicing,Wordpress,Empirical study|Administrators can adapt and customize a software system by changing its configuration option values. However, assigning a wrong value to one configuration option could lead the system to behave incorrectly or even to crash. A large number of studies provide solutions to such mis-configuration problems using a variety of approaches, yet none of them treats mis-configuration problems in multi-layer systems, in which the configuration of each layer can affect the higher layers, which typically are implemented using different programming languages. To find the mis-configured options in multi-layer systems such as Wordpress, this paper proposes an approach based on dynamic slicing. We evaluated this approach on 22 real mis-configuration errors of seven Wordpress plugins, two versions of the Wordpress core system, and the PHP interpreter Quercus. In most of the cases, our tool accurately reports the mis-configured option as first suggestion, within few minutes, which makes our solution practically useful.|None|reject|reject
ESEM|2016|Using Forward Snowballing to update Systematic Reviews in Software Engineering|katia r felizardo,emilia mendes,marcos kalinowski,erica f souza,nandamudi l. vijaykumar|Systematic Literature Review,SLR,Forward snowballing,Updating SLRs|Background: A Systematic Literature Review (SLR) is a methodology used to aggregate all the relevant evidence related to one or more research questions. Whenever new evidence is published after the completion of a SLR, this SLR should be updated in order to preserve its value. The update of an SLR is a time consuming, manual task and therefore many SLRs end up not being updated; this in consequence has an effect upon the existing combined body of knowledge on the state of the art in the research areas covered by those outdated SLRs. Objective: The goal of this paper is to investigate the application of forward snowballing to support the update of SLRs. Method: We compare outcomes of an update achieved using the forward snowballing versus a published update using the search-based approach, i.e., searching for studies in electronic databases using a search string. Results: In our study, forward snowballing showed a much higher precision and a slightly lower recall. It allowed analyzing more than five times less papers and missed only one relevant study. Conclusions: Due to its high precision, we conclude that the use of forward snowballing could significantly reduce the effort the updating SLRs in Software Engineering, but the risk of missing relevant papers should not be underrated.|None|reject|reject
ESEM|2016|Measuring Inconsistency of Programming Style using Hierarchical Agglomerative Clustering|qing mi,jacky w. keung,yang yu|programming style,hierarchical agglomerative clustering,empirical software engineering|Background: Although software engineering methodologies and guidelines are provided, it is common that the developers will apply their very own programming styles due to different experiences and personalities. These individually preferred programming styles are more comprehensive for themselves, but many violate those good programming principles and guidelines which should have been consistently applied to the entire code base. The lack of consistency in programming style is an inevitable problem during the entire software development process involving multiple developers, the result is undesirable and that will significantly degrade program readability and maintainability. Aims: Given limited understanding in this regard, we perform an empirical analysis for the purpose of quantitatively measuring programming style inconsistency within a software project team. Method: We first propose stylistic fingerprints, which are represented as a set of attribute-counting-metrics, in an attempt to characterize different programming styles, then we identify the corresponding taxonomy for those stylistic metrics. Subsequently, we adopt the hierarchical agglomerative clustering (HAC) technique to quantitatively measuring the proximity of programming style based on four open source projects chosen from different application domains. Results: The empirical result shows that we are able to demonstrate the feasibility and validity of our fingerprinting methodology for programming style. Moreover, the proposed clustering procedure utilizing HAC algorithm with dendrograms is capable of effectively illustrating the inconsistency degree of programming style among source files, which is significant for future analysis of programming style research. Conclusions: This study proposed an effective and efficient approach for analyzing programming style inconsistency, supported by a sound theoretical basis for dealing with such a problem. Ultimately improving program readability and therefore reduce the maintenance overhead for large software projects.|None|reject|reject
ESEM|2016|On the Analysis of Co-occurrence of Anti-patterns and Clones|fehmi jaafar,angela lozano,yann-gael gueheneuc,kim mens|Anti-patterns,Clones,Faults proneness|In software engineering, a smell is any symptom in the source code of a software system that possibly indicates a deeper problem. Although many kinds of smells have been studied to analyze their causes, their behavior and their impact on software quality, those smells typically are studied independently from each other. However, if two smells coincide inside a class, this could increases their negative effect (e.g., spaghetti code that is being cloned across the system). This paper presents results from an empirical study aimed at understanding the relationship between two specific kind of smells: code clones and anti-patterns. We conducted our study on five open-source software systems: Azureus, Eclipse, JHotDraw, Lucene, and XalanJ. Results show that (1) between 8% and 63% of classes in the analysed systems present co-occurrence of smells, (2) such classes could be five times more risky in term of fault-proneness, (3) and those classes have a negative impact on software reliability and maintainability.|None|reject|reject
ESEM|2016|Application of Mutual Information-Based Sequential Feature Selection to ISBSG Mixed Data|marta fernandez-diego,fernando gonzalez-ladron-de-guevara|Feature Selection,Subset Feature Selection,Mutual Information,ISBSG,Effort estimation,K-nearest neighbor|Background: There is still few research work focused on Data Preprocessing and, specially, Feature Selection (FS) techniques including both categorical and continuous features in Software Cost Estimation (SCE) literature. Aim: A Mutual Information (MI) based algorithm is proposed to obtain a ranked list of continuous and categorical most relevant and non-redundant ISBSG features for SCE. This mixed features list is split in two and recombined later according to the accuracy of a Case Based Reasoning model. Method: Four FS algorithms, involving filter and wrapper approaches, are compared using a complete dataset with 621 projects and 12 features from ISBSG Release 12. MI_1L and MI_2L only consider relevance, while mRMR_1L and mRMR_2L algorithms minimize redundancy too. MI_2L and mRMR_2L use two lists for continuous and categorical features. Results: MI_2L and mRMR_2L present quite similar behavior and better performance than those algorithms that do not discriminate. Thus, it is meaningful to consider two different lists of mixed features so that the categorical features may be selected more frequently. Conclusions: FS methods that distinguish between categorical and continuous features are a promising research area to select the most relevant and non-redundant ISBSG features for SCE.|None|reject|reject
ESEM|2016|Gap between Academia and Industry: A Case of Empirical Evaluation of Software Testing Techniques|sheikh umar farooq|Aggregation,Evaluation,Experimentation,External Validity,Replication,Testing Methods Evaluation.|Doing the right kind of testing has always been one of main challenging and a decisive task for industry. To choose right software testing technique(s), industry need to have an exact objective knowledge of their effectiveness, efficiency and applicability conditions. The most common way to evaluate testing techniques, for such knowledge, is with empirical studies - which help us to investigate testing techniques in order to compare and improve software testing methods and process. Despite significant efforts put in by academia to acquire such knowledge, there still exist a significant gap between academia and industry. Although the empirical studies carried so far to evaluate testing techniques contain many important and interesting results; however, we still lack solid objective and factual knowledge about performance and applicability conditions of testing technique(s), making it unfeasible to be adopted by the industry. Besides examining the current state of the art of knowledge base on testing techniques, this paper tries to identify the major factors responsible for limiting academia from producing results, which are significantly reliable and have an industrial impact. We conclude that there is a need for more systematic, quantifiable and comprehensive empirical studies; mainly replications using a standard framework, which should evaluate every dimension of testing techniques, so as to create an effective and applicable knowledge base about testing techniques which potentially can fill the void between testing research in academia and industry.|None|reject|reject
ESEM|2016|Toward a Measurement Program for the Agile Transition – An Industrial Case Study on Requirements Management|krzysztof wnuk,markus borg,sardar sulaman,hussan munir|requirements scoping,agile requirements management,decision making,requirements metrics|Many software-intensive development organizations are currently adapting agile-inspired practices and techniques. The trend toward agile is visible for both small companies and large globally distributed organizations. As an agile transition has a profound impact on the requirements management process, companies are struggling to measure both the benefits and the drawbacks. Still, few studies address measuring the agile transition from a requirements management process point of view, but rather take an artifact-level perspective. We present the results from an exploratory survey among 22 participants working with requirements management processes at a large company that develops embedded systems to a global market. Our respondents shared their opinions about the current set of requirements management process metrics as well as what additional metrics they envisioned as useful. We present a set of process metrics organized into four categories: quality, time, organization, and volume. Also, we show that metrics regarding quality is particularly requested in the company. The findings provide practical insights that can be used as input when introducing new measurement programs, representing a process-oriented perspective.|None|reject|reject
ESEM|2016|The detection of code smell on software development: a mapping study|xinghua liu,cheng zhang|code smell,experiment study,metric,detection,detection tool|Context: Although Code Smell can’t cause problems with execution of project code, Code Smell can cause some potential problems of maintainability and understandability on the software projects. Meanwhile, as for the research of code smell, current research results pay attention to only several specific Code Smells, and then don’t have a systematic detection on Code Smell. Objective: To investigate how many research results concentrated on Code Smell detection and the detection tools could detect what kinds of code smells. Methods: According to the Guidelines of Kithenham, we carry out a mapping study about 22 code smells, searching the relevant papers till 2015. Results: Through the process of mapping study, 257 papers are finally included and then classified into our data records. Conclusion: Referring to detection tools, firstly they only take notice of several specific Code Smells, because these code smells can be easily measured in term of quantification. Secondly, experiment systems of these papers are almost lab projects and industrial open source not the industrial closed source projects. Thirdly, code lines of most detected lab projects are under 30 KLOC. In the future, we will focus efforts on detection of Code Smells that can’t be easily detected, what’s more, we will put our studies under the industrial environment.|None|reject|reject
ESEM|2016|Can we trust Capture-recapture estimates? An evaluation approach by measuring dependence|bohan liu,guoping rong,he zhang|defect estimation,capture-recapture,software inspection,coefficient of covariation|Background: The Capture-ReCapture method (CRC) has  studied in Software Engineering (SE) community as  promising technique to estimate post-inspection defects. While most existing studies focused on the performance of  CRC models and estimators, few have been done on  assessment of the credibility of the estimation results, rendering the difficulty to apply the estimation results for  decision-making. Objective: This research aims to explore  and investigate a reliable approach to assess the credibility of the estimates generated by the CRC method. Method: We introduce the `Coecient of CoVariation' (CCV) as an  to measure the dependence between the defect sets  by different inspectors in the two-person inspection teams. The utility of the CCV towards the assessment of the  results is evaluated by examining its relationship with the Relative Error (R.E.) of CRC estimates using all the available raw data retrieved from the existing studies on CRC-related research. Results: The results indicate that most two-person inspection teams have positive CCVs and the R2 (Coecient of Determination) of non-linear curve-fitting for the CCVs and R.E.s is higher than 0:8. Conclusion: Our study suggests that the dependence among inspectors (measured by CCV ) does exist. Besides, there also exists a strong correlation between CCV and R.E., which may provide clues to assess the credibility of CRC-based estimates on defects.|None|reject|reject
ESEM|2016|Benchmarking Software Effort Prediction Models: An Automated Framework and Experimental Results|christian quesada-lopez,juan de dios murillo,carlos castro-herrera,marcelo jenkins|Learning schemes,large scale experiment,empirical study|Background: The development of software effort prediction models has been an area of considerable research in software engineering. Improving the prediction models available to project managers would facilitate more effective control of time in software development. Several prediction models have been evaluated in the literature and inconsistent findings have been reported regarding which technique is the best. Unbiased and comprehensive comparison between prediction models is needed. Objective: This paper evaluates an automated framework for software effort prediction. Our study reports a large scale benchmarking experiment evaluating 600 learning schemes (8 data preprocessing X 5 attribute selectors X 15 learning algorithms) for given datasets. Method: Twenty eight private and public software effort datasets were analyzed executing a N-PASS (10 times) and a MXN-fold cross-validation (10X10) for each learning scheme. The framework builds models according to learning schemes and predicts software development effort with new data according to the constructed models. Results: In total, 16,800 runs were conducted (Learning schemes X Datasets). The best learning schemes based on Spearman performance were: BoxCox + (ForwardSelection, BackwardElimination, BestFirst) + SMOreg, and BoxCox + BackwardElimination + LeastMedSq. Conclusions: Different learning schemes should be selected for different datasets. Our findings support previous ones stating that the setup applied in evaluations can completely change model results.|None|reject|reject
ESEM|2016|Evaluating Human-Computer Interaction techniques for requirements elicitation: A family of experiments on developers’ and users’ perceptions|jose pablo hernandez,christian quesada-lopez,melissa jensen,marta calderon,marcelo jenkins|Human-computer interaction,User-centered design,IEEE 830,requirements elicitation,mobile app,experiment|Background: Raising software requirements that capture information in an unambiguous language that can be easily understood by users is challenging. The human-computer interaction (HCI) field has a set of techniques for software requirements elicitation, which claim to improve end users and software engineers’ communication. Objectives: This paper presents an empirical comparison of the traditional IEEE 830 standard and HCI requirements elicitation techniques based on users and developers perceptions. The goal is to evaluate effectiveness, understanding, empathy, and intention of adoption properties of requirements specification documents based on these two approaches and determine the preference for one of them. Methods: The evaluation was carried out through a family of experiments conducted with five groups. Replication of experiments supports the analysis of the generalization and practical applicability of the findings. In total, we evaluated the responses of 104 participants. Results: The HCI requirements elicitation document effectiveness was significantly better than the IEEE 830 approach (p-value < 0.05). All groups of participants found the HCI techniques more efficient, understandable and adoptable. Most of the participants consider the HCI techniques generate more empathy. Based on the results, the HCI specification defines how the application will look like, uses an easier to understand written and visual language, and gives a clearer idea of how users interact with the application. Conclusions: Our experiment confirms literature claims about how HCI techniques might improve the requirements elicitation process. Participants perceived they might adopt HCI techniques in software development projects to create specifications with a language easy to understand and help developers and users to be aligned with the type of application to be developed.|None|reject|reject
ESEM|2016|The Intersection of Continuous Deployment and Architecting Process: Practitioners’ Perspectives|mojtaba shahin,muhammad ali babar,liming zhu|Software architecture,DevOps,continuous deployment,empirical study.|Context: Development and Operations (DevOps) is an emerging software industry movement to bridge the gap between software development and operations teams. DevOps supports frequently and reliably releasing new features and products– thus subsuming Continuous Deployment (CD) practice. Goal: This research aims at empirically exploring the potential impact of CD practice on architecting process. Method: We carried out a case study involving interviews with 16 software practitioners. Results: We have identified (1) a range of recurring architectural challenges (i.e., highly coupled monolithic architecture, team dependencies, and ever-changing operational environments and tools) and (2) five main architectural principles (i.e., small and independent deployment units, not too much focus on reusability, aggregating logs, isolating changes, and testability inside the architecture) that should be considered when an application is (re-) architected for CD practice. This study also supports that software architecture can better support operations if an operations team is engaged at an early stage of software development for taking operational aspects into considerations. Conclusion: These findings provide evidence that software architecture plays a significant role in successfully and efficiently adopting continuous deployment. The findings contribute to establish an evidential body of knowledge about the state of the art of architecting for CD practice.|None|accept|accept
ESEM|2016|Mining Technology Landscape from Stack Overflow|chunyang chen,zhenchang xing|Knowledge graph,Technology landscape,Stack Overflow,Graph measurement|The sheer number of available technologies and the complex relationships among them make it challenging to choose the right technologies for software projects. Developers often turn to online resources (e.g., expert articles and community answers) to get a good understanding of the technology landscape. Such online resources are primarily opinionbased and are often out of date. Furthermore, information is often scattered in many online resources, which has to be aggregated to have a big picture of the technology landscape. In this paper, we exploit the fact that Stack Over ow users tag their questions with the main technologies that the questions revolve around, and develop association rule mining and community detection techniques to mine technology landscape from Stack Over ow question tags. The mined technology landscape is represented in a graphical Technology Associative Network (TAN). Our empirical study using Stack Over ow data dump shows that the mined TAN captures a wide range of technologies, the complex relationships among the technologies, and the trend of the technologies in the developers' discussions on Stack Over ow. We develop a web site (https://graphofknowledge.appspot.com/) for the community to access and evaluate the mined technology landscape. We report the Google Analytics results of the web site usage from Sep 4, 2015 to Jan 13, 2016.|None|accept|accept
ESEM|2016|An Empirical Study on the Equivalence and Stability of Feature Selection for Noisy Software Measurement Data|zhou xu,jin liu,liang hong,he zhang|Feature selection,Equivalence,Stability,Principal component analysis(PCA)|Background: Software measurement data (SMD) are usually used to build defect prediction models for software quality assurance. However, the irrelevant features in the dataset will reduce the per-formance of the models. Existing work employs feature selection to alleviate this issue. Previous studies show that different feature selection methods do not always yield similar performance on de-fect prediction models for SMD, which indicates that these methods are not equivalent. Also, previous studies show that SMD usually contain noise which may threaten the process of feature selection. Aims: We conduct an empirical study to investigate the equivalence of different feature selection methods and measure the equivalence of these methods for SMD (i.e. an equivalence analysis). Further, we analyze the stability of the methods for noisy SMD (i.e. a stabil-ity analysis). Method: We perform experiments on 10 projects of NASA dataset with six feature selection methods. For the equivalence analysis, we introduce the Principal Component Analysis (PCA) technique to qualitatively analyze the equivalence of these six methods and use the overlap index to quantitatively analyze the extent of their equiva-lence. For the stability analysis, we apply Jaccard index to measure the stability of these methods. Results: Experimental results indicate that different feature selec-tion methods are indeed not equivalent to each other, and that the average percentages of overlap are usually low. The results also show that the Correlation (Cor) method is the most stable one among these six methods for noisy SMD. Conclusions: We conclude that PCA is an effective technique to analyze the equivalence of different feature selection methods. Our findings also suggest that more stable methods should be proposed for noisy SMD.|None|reject|reject
ESEM|2016|Meta-analyzing families of experiments: Aggregated vs Participant Data|adrian santos parrilla,burak turhan,natalia juristo juzgado|SE Replication,Family of Experiments,Systematic Reviews,IPD Meta-Analysis,AD Meta-Analysis|Context: Many experiments have been run in the area of SE in order to assess the benefits of new technologies, processes or artifacts. Families of experiments are becoming common nowadays as they allow researchers to increase the reliability and validity of their findings. As experiments accumulate over the years, finding out ways to accurately synthesize them into a comprehensive result becomes of paramount importance. Aggregation of results might suffice for proposing the need for further experiments, open new lines of research or even prove inefficacy of treatments. Objectives: We aim to understand the benefits, drawbacks and commonalities of the different aggregation techniques that can be used in order to aggregate experimental results when researchers have full access to the raw data of the experiments. As well, we want to set the ground for applying those techniques to families of experiments within the area of SE. Method: A selective literature search on techniques for aggregating experimental results in medicine and social sciences was performed in order to come up with an overview of the most used techniques. We study the categorization of those techniques in medicine and sociology. This categorization is subsequently imported into SE. Results: Synthesis of quantitative results can be carried out with individual level or with experiment level data. We have identified several techniques: narrative synthesis, aggregated data (AD) meta-analysis (MA) and individual participant data (IPD) meta-analysis (MA). We have realized that in our community meta-analysis is commonly identified as AD-MA. IPD meta-analysis seems to be superior to AD meta-analysis. Conclusion: IPD has been widely recognized as the “gold-standard” in medicine and sociology for aggregation of experimental results. Should not this be the same for SE? Researchers should strive for performing IPD meta-analysis at least when aggregating their own experiments since they have full access to the raw data.|None|reject|reject
ESEM|2016|How the Development and Management of Apps for Mobile Devices are Perceived in the Software Industry|giuseppe scanniello,rita francese,carmine gravino,michele risi,genoveffa tortora|Mobile development and management,Qualitative study,Software industry,Survey,Interview|Background: Applications for mobile devices (or simply apps) make up an important part of the software industry, yet the software engineering community only recently is approaching issues related to their development and management. Aims: We aim at understanding the practice of the main aspects related to development and management of apps in the software industry. Method: Our investigation is qualitative and it is composed of two main steps. In the first step, we interviewed IT (In- formation Technology) managers with experience in devel- opment and management of apps. This part of our investigation can be intended as an explorative study because we used results to plan and execute our second step, namely a survey with software professionals. Results: From gathered data, we obtained a number of find- ings. For example, we can summarize the following ones: (i) development is mostly done by junior developers; (ii) agile methodologies are largely adopted even if there is not the best approach and technology for cross-platform development; (iii) support for testing is considered inadequate; (iv) fragmentation of software and hardware is perceived an important concern; and (v) for their nature apps are different from web- and desktops-applications and then they need to be specifically treated (e.g., user’s experience). Conclusions: Based on our findings, we highlight areas that require more attention from the research and the industry.|None|reject|reject
ESEM|2016|Innovative Behaviour of Software Engineers: Findings from a Pilot Case Study|cleviton monteiro,fabio q. b. da silva,luiz fernando capretz|Innovative behavior,Innovation,Software engineering,Pilot Case Study|Context: In the workplace, certain individuals tend to engage in the voluntary and intentional generation, promotion, and realization of new ideas to benefit individual performance, group effectiveness, or the organization. The literature classifies this phenomenon as innovative behaviour. Despite its importance to the development of innovations, innovative behaviour has not been fully investigated in software engineering. Objective: To understand the factors that support or inhibit innovative behaviour in software engineering practice. Method: We conducted a pilot case study in a Canadian software company using interviews and observations as the data collection techniques. Using qualitative analysis techniques, we identified relevant factors and relationships not addressed by studies from other areas. Results: We found that the individual innovative behaviour is influenced by the individual attitude and also by situational factors such as the relationships in the workplace, organizational characteristics, and the project type. We also built a model to express the interacting effects of these factors. Conclusions: The innovative behaviour is dependent on individual and contextual factors. Our results point to relevant impacts to research and practice, and to topics that deserve further studies.|None|accept|accept
ESEM|2016|Survey on Agile Implementation in Industrial Contexts|samireh jalali,tony gorschek|Agile methods,Agile practices,Agile implementation,Scrum,Survey,Context analysis|Context: Agile methods deliver working software in sprints and consider change as a natural part of development by utilizing continuous re-planning. Thus, they offer shorter lead-time to change in response to customer or market demands. However, although the Agile values and principles were formulated over a decade ago, there is a lack in understanding how Agile is actually realized in various industrial settings. Objective: This research aims at providing an overview of contexts in which Agile has been applied as well as to investigate the potential associations between Agile methods/practices and the contextual factors. Method: An online questionnaire-based survey was used to reach out to as many Agile practitioners as possible. The survey design was based on information gathered through exploring the research literature. Results: Data was collected from 120 sufficiently experienced Agile practitioners. A majority of respondents work in small teams and collaborate with distributed teams. Scrum was found to be the most popular Agile method whilst traditional methods e.g. Waterfall were also widely used. Despite the fact that Agile is applied in a variety of contexts, its implementation might depend on the contextual factors in particular team’s size, location, as well as number of collaborating distributed teams, software area, and organization size. Conclusions: Scrum represents Agile methods in practice as it is widely used in various contexts. However, Agile practitioners tend to employ a combination of methods and practices based on their contextual needs, and thus Agile does not have a common definition in terms of its implementation. It is observed that Agile practitioners are actively involved in distributed software development (DSD) as well as in large-scale Agile software development (ASD).|None|reject|reject
ESEM|2016|API Failures in Cloud Environments - An Empirical Study on OpenStack|pooya musavi,bram adams,foutse khomh|cloud,empirical study,api failure,OpenStack|Stories about service outages in cloud environments have been making the headlines recently. In many cases, the reliability of cloud infrastructure Application Programming Interfaces (APIs) were at fault. Hence, understanding the factors affecting the reliability of these APIs is important to improve the availability of cloud services. In this study, we mined bugs of 25 modules within the 5 most important OpenStack APIs to understand API errors and characteristics. Results show that in OpenStack, one third of all API-related changes are due to bug fixes with 7\% of all fixes even changing the API interface, potentially breaking clients. Through qualitative analysis of a sample of API failures we observed that the majority of API related bugs are due to small programming errors. Fortunately, the subject, message and stack trace as well as reply lag between comments included in these failures' bug reports provide a good indication of the cause of the failure.|None|reject|reject
ESEM|2016|Sustainable Software Development through Overlapping Pair Rotation|todd sedano,paul ralph,cecile peraire|Extreme Programming,Grounded Theory,Code ownership,Sustainable software development|Context: Conventional wisdom says that team disruptions (like team churn) should be avoided. However, we have observed software development projects that succeed despite high disruption. Objective: The purpose of this paper is to understand how to develop software effectively, even in the face of team disruption. Method: We followed Constructivist Grounded Theory. The primary researcher conducted participant-observation of several projects at Pivotal (a software development company), and interviewed 21 software engineers, interaction designers, and product managers. The researcher iteratively sampled and analyzed the collected data until achieving theoretical saturation. Results: This paper introduces a descriptive theory of Sustainable Software Development. The theory encompasses principles, policies, and practices aiming at removing knowledge silos and improving code quality (including discoverability and readability), hence leading to development sustainability. Limitations: While the results are highly relevant to the observed projects at Pivotal, the outcomes may not be transferable to other software development organizations with different software development cultures. Conclusion: The theory refines and extends the understanding of Extreme Programming by adding a few principles, policies, and practices (like the unique Overlapping Pair Rotation practice) and aligning these principles, policies, and practices towards the business goal of sustainability.|None|accept|accept
ESEM|2016|Evaluating the DRank Method: A Requirements Prioritization Method based on Dependencies|rong peng,fei shao,dong sun|Requirements prioritization,prioritization evaluating,dependencies,link analysis|Current requirements prioritization approaches seldom take dependencies into consideration, such as contribution, conflict and business logic, for it is difficult for stakeholders to prioritize requirements not only from the perspective of their subjective preferences, but also from the perspective of the objective dependencies existed between requirements. And the lack of acknowledged assessment framework for prioritization methods influences the development of these studies, to deal with the issue, a prioritization method named DRank and a reasonable evaluating system are proposed, DRank is inspired by CBRank and PageRank, and it is composed of 3 parts: 1) it introduces the Software Attributes Tree (SAT) to guide the ranking criteria selection, and adopts RankBoost to calculate the subjective requirements prioritization according to stakeholders’ preferences; 2) it proposes a requirements influence analysis method based on weighted PageRank by considering the direction and strength of dependencies; 3) it generates the integrated requirements prioritization by integrating subjective requirements prioritization and requirements influences. A controlled experiment based on the reasonable evaluating system is performed to validate the efficiency and effectiveness of DRank by comparing with CBRank and AHP. The results show that DRank is less time-consuming and more effective. And according to a quantitative analysis it can significantly increase the accuracy of the prioritization by introducing the dependencies.|None|reject|reject
ESEM|2016|How Are Discussions Associated with Bug Reworking? An Empirical Study on Open Source Projects|yu zhao,feng zhang,emad shihab,ying zou,ahmed e. hassan|bug reworking,re-patch,re-open,discussions,Latent Dirichlet Allocation|Bug fixing is one major activity in software maintenance to solve unexpected errors or crashes of software systems. However, a bug fix can also be incomplete and even introduce new bugs. In such cases, extra effort is needed to rework the bug fix. The reworking requires to inspect the problem again, and perform the code change and verification when necessary. We focus on two types of “reworked bug fixes”: 1) the initial bug fix made in a re-opened bug report; and 2) the initially submitted patch if multiple patches are submitted for a single bug report. Discussions throughout the bug fixing process are important to clarify the reported problem and reach a solution. In this paper, we explore how discussions during the initial bug fix period (i.e., before the bug reworking occurs) associate with future bug reworking. We perform a case study using five open source projects (i.e., Linux, Firefox, PDE, Ant and HTTP). We find that the occurrence of bug reworking is associated with the duration, the number of comments, the frequency, the number of involved developers and the experience of developers. Furthermore, we extract topics of discussions using Latent Dirichlet Allocation (LDA). We find that discussions on some topics (e.g., code inspection and code testing) can decrease the frequency of bug reworking. In summary, the discussions during the initial bug fix period may serve as an early indicator of what bug fixes are more likely to be reworked.|None|accept|accept
ESEM|2016|Distribution Patterns of Software Effort in Big Dataset|yong wang,xiaoyan zhu,guangtao wang,yinlong liu|software effort estimation,stage effort,stage effort distribution|In software engineering domain one of the most challenging tasks is to estimate the project effort accurately. It’s critical to understand the project’s law of effort distribution for predicating efforts precisely. However with the passage of time the software project effort fluctuations are very sharp without any clear law. In this paper the project per month effort was normalized to its effort ratio. The statistical features were concluded based on analysis of data taken from a large scale real world software engineering repository. The statistical features included: 1) consistent projects month effort curves which are all up-convex among various project length. 2) The effects of project features e.g. project type, industry etc. on the project month effort ratio curve are not that much considerable as generally regarded. 3) The single month effort ratio resembles normal distribution. The normal distribution character in the middle months is more obvious than that at the two ends. All of these findings are helpful to deeply understand the distribution and change law for software project’s stage effort. It also makes a solid foundation for building precise and accurate software effort estimation models.|None|reject|reject
ESEM|2016|Using Cognitive Psychology Perspective on Errors to Improve Requirements Quality: An Empirical Investigation|vaibhav k. anu,gursimran s. walia,wenhua hu,jeffrey carver,gary bradshaw|Human error,Error abstraction,Taxonomy,Empirical Study,Inspections|Background: Software inspections are an effective technique for early detection of faults present in software development artifacts (e.g., requirements and design documents). However, a lot of faults are left undetected due to the lack of focus on the underlying source of faults (i.e., what caused the injection of the fault?). Aim: To address this problem, research work done by Psychologists on analyzing the failures of human cognition (i.e., human errors) is being used in this research to help inspectors in finding errors and corresponding faults (manifestations of errors) in requirements documents. We hypothesize that a formal taxonomy of human errors, focused on the underlying source of faults (i.e., errors) is a significant improvement over traditional fault-based inspection. Method: This paper describes a newly developed Human Error Taxonomy (HET) and a formal Error-Abstraction and Inspection (EAI) process to improve fault detection performance of inspectors during the requirements inspection. A controlled empirical study evaluated the usefulness of HET and EAI compared to fault based inspection. Results and Conclusion: The results of the empirical study verify our hypothesis and provide useful insights into commonly occurring human errors that contributed to requirement faults along with areas to further enhance HET and EAI process.|None|reject|reject
ESEM|2016|Personal User Experiences Using GitHub and Open Academy for Learning and Education Purposes in Computer Science|nada alasbali,boualem benatallah|GitHub,Education,Open Academy,Open Source,Personal User Experience|This paper reports insights from the qualitative analysis of personal user experiences reported by the users of GitHub and/or Facebooks Open Academy for learning purposes in an educational setting or outside it. Reports of personal user experience are found through a set of systematic online searches and then investigated using an integrated approach. Key findings include several unexpected learning uses reported by the GitHub users outside a formal educational setting. Furthermore, insights regarding previously unreported aspects of contributing to an Open Source (OS) project are found. The major challenges experienced by both mentors and students when using OS for educational purposes are highlighted. A number of useful recommendations are given to the researchers in the field and also to those aiming to interact with OS communities including students and education providers.|None|reject|reject
ESEM|2016|Where is the road for issue report classification based on text mining?|qiang fan|issue tracker system,machine learning algorithms,mining software repositories|Many software issues classification model have been build using historical issue data obtained by mining software repositories (MST). To improve the performance of the model, the common approach is adding kinds of data used for building model. But most of time, these data are produced after the managers and developers participation. Too much manpower makes it contradictory to the motivation of building classification model and make it less helpful for managers and developers. In this paper, we use many different machine learning algorithms (MLA) to build classification model to discuss the performance of these MLA. At the same time, we use case study to explore why there are some issues hard to classify for MLA, and use the regulation we found to feedback the classification model and improve the performance of MLA. Finally, we use case study to discuss the imbalanced distribution for bug and feature with the topic of these issues.|None|reject|reject
ESEM|2016|An External Replication on the Effects of Test-driven Development Using Blind Analysis|davide fucci,giuseppe scanniello,simone romano,martin j. shepperd,boyce sigweni,fernando uyaguari,burak turhan,natalia juristo juzgado,markku oivo|test-driven development,external experiment replication,blind analysis|Background: Test-driven development (TDD) is an agile practice claimed to improve the quality of a software product, as well as the productivity of its developers. A previous study (i.e., baseline experiment) at the University of Oulu (Finland) compared TDD to a test-last development (TLD) approach through a randomized controlled trial. The results failed to support the claims. Aims: We want to validate the original study results by replicating it at the University of Basilicata (Italy), using a di erent design. Method: We replicated the baseline experiment, using a crossover design, with 21 graduate students. We kept the settings and context as close as possible to the baseline experiment. In order to limit researchers bias, we involved two other sites (UPM, Spain, and Brunel, UK) to conduct blind analysis of the data. Results: The Kruskal-Wallis tests did not show any signi cant di erence between TDD and TLD in terms of testing e ort (p-value = .27 ), external code quality (p-value = .82 ), and developers' productivity (p-value = .83 ). Nevertheless, our data revealed a di erence based on the order in which TDD and TLD were applied, though no carry over e ect. Conclusion: We verify the baseline study results, yet our results raises concerns regarding the selection of experimental objects in general, i.e., the ordering e ect of treatment application. We recommend future studies to survey the tasks used in experiments evaluating TDD. Finally, to lower the cost of replication studies and reduce researchers' bias, we encourage other research groups to adopt similar multi-site blind analysis approach described in this paper.|None|reject|reject
ESEM|2016|An empirical Approach to quantify the Impact of object-relational Mapping Strategies|martin lorenz,gunter hesse,jan-peer rudolph|object-relational mapping,relational database,ORM|Object-oriented applications often achieve persistence by using relational database systems. In such setup, object-relational mapping is used to link objects to tables. Due to the fundamental differences between object-orientation and relational algebra, the definition of a mapping is a considerably difficult task. Today, the only method available to engineers, to decide what mapping strategy to choose, are informal guidelines. However, guidelines do not provide a quantification of the actual impact and trade-off between different mapping strategies. Thus, the decision what mapping strategy to implement contains a large portion gut feeling and is not based on facts. In this paper, we propose a framework, which provides an empirical analysis of the impact of object-relational mapping strategies on selected non-functional system characteristics. Our framework allows to quantify the impact of a mapping strategy in the areas efficiency and maintainability. This provides significant improvements, because it allows engineers to make distinctive and informed decisions, based on empirical data, rather than gut feeling.|None|reject|reject
ESEM|2016|Requirements Engineering Practice and Problems in Agile Projects: Results from an International Survey|stefan wagner,daniel mendez fernandez,michael felderer,marcos kalinowski|requirements engineering,agile,state of the practice,survey|Requirements engineering (RE) is considerably different in agile development than in traditional processes. Yet, there is little empirical knowledge on the state of the practice and contemporary problems in agile RE. As part of a bigger sur- vey, we build an empirical basis on such aspects of agile RE. Based on the responses from 92 people representing 92 organisations, we found that agile RE concentrates on free text documentation of requirements elicited with a variety of techniques. Many manage explicit traces between require- ments and code. Furthermore, the continuous improvement of RE is done because of intrinsic motivation. Important ex- perienced problems include unclear requirements and com- munication flaws. Hence, agile RE is in several aspects not so different from RE in other development processes. We plan to investigate specific techniques, such as acceptance- test-driven development, in a future survey to better capture what is special in agile RE.|None|reject|reject
ESEM|2016|Software Security Professionals: Expertise Indicators|mortada al-banna,boualem benatallah,moshe chai barukh|Crowd-sourcing,Vulnerability Discovery,Security professional,Expertise|In crowd-sourcing, selecting the person with suitable expertise is very important; especially since the task requester is not always in direct contact with the worker. Recently, this has become increasingly important particularly when the crowd-sourced tasks are complex and require skillful workers (e.g. software development, software testing, vulnerability discovery, and open innovation). In this paper, we aim to identify indicators to determine the expertise of security professionals in a crowd-sourcing vulnerability discovery platform. We review literature and online content, conduct interviews with domain experts in industry, and survey security professionals involved in the task of vulnerability discovery.  We discuss the indicators we have found, and we provide some rec-ommendations to help improve the process of selecting security professionals to perform crowd-sourced tasks related to vulnerability discovery.|None|reject|reject
ESEM|2016|Software Development Methods and Practices Adoption and Suitability: Results from a Multi-National Industry Practitioner Survey|sherlock a. licorish,johannes holvitie,sami hyrynsalm,ville leppanen,rodrigo o. spinola,thiago s. mendes,stephen g. macdonell,jim buchan|Software development methods,Agile practices adoption and suitability,Empirical studies,Survey|Context: Our understandings of the adoption and use of software development methods and practices have been largely influenced by consultants and tool vendors’ reports on the state of agile. Some believe that these surveys and reports are driven by self-interest and they lack scientific rigour, threatening their reliability. Objective: There have been calls for academic-led studies to offer validation for these outcomes. This study has answered these calls. Method: We surveyed 184 practitioners working in Brazil, Finland, and New Zealand in a transnational study. Results: Results revealed that most practitioners in our sample focused on a small portfolio of projects that were of short duration. These members were very experienced and operated in a transparent, efficient, agile, and sustainable manner in small teams to deliver both complete and partial solutions. We observed that while Scrum and Kanban were used most, some practitioners also used conventional methods. In addition, practices such as Coding Standards, Simple Design, Refactoring, Continuous Integration and 40-hour Week were used most by practitioners. Furthermore, practitioners felt that these practices were largely suitable for project and process management. Conclusion: Our evidence points to the need to properly understand and support the software methods that are applied (including conventional methods).|None|reject|reject
ESEM|2016|Forecast of Team Communication Behaviors based on Previous Software Project Experiences – An Exploratory Study on Student Projects|fabian kortum,jil kluender|Software Project,Team Communication,Lessons Learned,Predictive Analytics,Project Success Impacts|Effective team communication is a prerequisite for software project success. Customer requirements and change requests must be correctly elicited and communicated within the software developing team. It would be highly desirable to recognize dysfunctional communication habits in early developing phases, before they can negatively influence the project success. However, it is difficult to estimate or forecast accurate future communication conditions in a newly formed team. A team may not have reached its steady state during the first few weeks, and communication intensity may vary over time. Although communication intensity and effectiveness are obviously essential, they seem to be difficult to grasp. This publication is introducing an approach for forecasting team communication intensities over several weeks by identifying nearest neighbors in a set of previous projects. We measured and analyzed team communication indicators in a field study with 34 student projects. All projects were mostly comparable in size and complexity. Furthermore, they fulfilled the demands to a real-world environmental software project. We identified strong correlations between communication intensities and event-factors, like deliverable submissions, deadlines and even holidays. We use several first weeks of projects with team communication activities and extrapolated them by identifying the closest matches to previous projects. We validated the forecasts long-term accuracy by using cross project validations on all 34 software projects. The forecast can preview team communication tendencies, especially for deadlines or other significant events. These events probably cause conflicts because of higher required communication intensities. This study is an encouraging first step towards predicting team communication intensities and points-out critical situations in a variety of early phases of a project. It is our aim to diagnose team communications problems early enough to adjust them, before they harm the project. We plan to extend our approach towards industry projects that are less similar and possibly provide additional significant effects.|None|reject|reject
ESEM|2016|Supporting the Evolution of Event-driven Architectures with Change Operations at Different Abstraction Levels: A Controlled Experiment|srdjan stevanetic,simon tragatschnig,uwe zdun|event-driven architectures,change patterns/primitives,evolution|The components of an event-driven architecture are composed in a highly decoupled way, facilitating high exibility, scalability and concurrency of distributed systems. Evolving an event-driven architecture is challenging because the absence of explicit dependencies among constituent components makes understanding and analysing the overall system composition dicult. The evolution of event-driven architectures typically happens by performing a series of primitive changes (which can be described formally as change primitives). On a higher level of abstraction, change patterns that encompass a number of change primitives have been suggested to evolve event-driven architectures. In this paper, we present the results of a controlled experiment on understanding and performing changes in event-based architectures on dierent levels of abstraction. The experiment is conducted with 90 students of the Software Architecture course at the University of Vienna. Particularly, we compare the eciency of 3 sets of change operations for modifying a given system architecture to obtain a desired architecture: a minimal set of 3 change patterns, an extended set of 5 change patterns, and a minimal set of 4 change primitives. Our results show that change patterns based evolution requires signicantly less time to capture a similar level of correctness compared to the evolution based on change primitives, presuming that a certain level of transformation complexity is required. Furthermore, we did not observe a signicant dierence in the correctness level nor in the time required to perform the changes using an extended pattern set compared to a minimal set of patterns.|None|reject|reject
ESEM|2016|Automation of Self-Admitted Technical Debt for Rework Effort Estimation|solomon mensah,kwabena ebo bennin,jacky w. keung,michael bosu|Self-admitted Technical Debt,Rework Effort,Text Mining,Source Code Comments|Background: The concept of technical debt refers to the problem of releasing uncompleted and temporary workaround software products in order to meet short-term business objectives subsequently incurring long-term business cost. This debt metaphor is recently attracting the attention of software engineering researchers to provide an optimal solution in order to improve the software quality.  In this paper, we studied a variant of technical debt which is introduced by software developers referred to as self-admitted technical debt (SATD) and usually found in developers’ source code comments. Aim: To automatically extract SATD from developers’ source code comments and to compute the rework effort needed to resolve the SATD. Method: We propose a text mining algorithm based on key indicators of SATD. We used this algorithm in extracting and analyzing SATD from developers’ source code comments existing in four large open source projects. We also developed a method for computing rework effort needed to fix SATD in each of these projects. Results: Our analysis indicate that design debt is the most predominant technical debt (consisting of more than 50% of all technical debts) in each of the systems studied which confirms the result of prior studies. This study also found out that a significant amount of 11 to 38 person-months of rework effort is required to resolve SATD across all projects considered prior to release. Lastly, this study found a significant correlation in the range of 0.64-0.85 which signifies high software bugs traceability from source code comments. Conclusion: We believe that our proposed approach will speed up the extraction and analysis of SATD and also aid project managers in their decisions of whether to handle SATD as part of on-going project development or defer it to the maintenance phase.|None|reject|reject
ESEM|2016|A glimpse inside the mind of a Programmer: Brainwave patterns during Code Comprehension|makrina viola kosti,kostas georgiadis,dimitrios adamos,nikos laskaris,lefteris angelis|Brainwaves,Wearable EEG,Neural Synchrony,Human Factor,Software Engineering|This paper provides a proof of concept for the use of wearable technology, and specifically wearable Electroencephalography (EEG) in aid of Software Engineers (SEng) in the field of software development. Particularly, we investigated the brain activity of a SEng while performing two distinct but related mental tasks: understanding and inspecting code for syntax errors. By comparing the emerging EEG patterns of activity and neural synchrony, we identified brain signatures that are specific to code comprehension. Moreover, using the programmer's rating about the difficulty of each code snippet shown, we identified neural correlates of the induced cognitive workload. The reported results show premise towards novel alternatives to programmers’ training and education. Findings of this kind may eventually lead to various technical and methodological improvements in related fields like programming languages, building platforms for teams, and team working schemes.|None|reject|reject
ESEM|2016|Early Evaluation of the ISO 25010 Security Characteristic  Using the Structural and Functional Size Measure to the Authentication Sub Characteristic|hela hakim,asma sellami,hanene ben-abdallah|ISO 25023 Quality Measurement,Security,Authenticity,Structural Size,Functional Size,Design phase|With the increased demand of software security, software development organizations have established robust safeguards to prevent information access and viruses due to less secure provider systems. Inevitably, this has resulted in a rapid increase in the advent of quantitative measurement of the security quality characteristic as early as possible because paying attention to security in the early phase of the software life cycle can saves more cost and effect. Although ISO 25023 quality measurement proposed measures of the security sub-characteristic as internal and external metrics, they does not yet accommodate a clearly set of measures for estimating authenticity at the design phase. This paper present an early specific measurement based in the structural and functional size to quantify the ISO 25023 quality sub-characteristic “authenticity” internally at the design phase. The main idea of using this specific measure is to evaluate/asses the security quality characteristic early in the SDLC because the security quality must be involved throughout all the SDLC particularly at the design phase and not to be provided only at the end of the development cycle. Measuring the authenticity early in the software development life cycle (design phase) may help not only designer’s software and testers/QA but also managers to performing risk analysis and to make initiate corrective action as early as possible in the development life cycle so that they can be efficiently used to find preventive solutions to enhance security. We illustrate our measurement proposition from an example.|None|reject|reject
ESEM|2016|Crowdsourcing Initiatives: A Systematic Review of Factors that Influence Users Participation|hazleen aris,izyana ariffin,badariah solemon|Crowdsourcing participation,Influencing factor,Sustained participation,Crowdsourcing initiatives,Systematic review|It can be assumed that volume of users participation is important towards the success of a crowdsourcing initiative.  Literally, it means that the larger the volume, the higher the chance of completing the advertised tasks.  Recognising the factors that influence users participation, both positive and negative, enables appropriate elements to be incorporated during the design of the crowdsourcing initiative to ensure that volume of users participation can be obtained and sustained.  Realising their importance, studies had been performed that identified these factors, notably in the recent years.  In this paper, a systematic literature review performed on these studies is presented.  The objective of this review is to identify factors that influence users participation in crowdsourcing initiatives derived and presented in these studies.  Thirteen evidences were eventually included in the review from the initial list of 1758 unique results returned by eleven online data sources searched.  The thirteen evidences belonged to three previously defined types of crowdsourcing; eight to crowd creation type, three to crowd wisdom type and two to crowd funding type.  A total of 102 influencing factors was initially extracted from the evidences, which was derived from direct interaction with users and/or data extracted from the crowdsourcing platforms.   Analysis performed on the extracted factors found that some of the influencing factors are recurring and common across different types of crowdsourcing initiatives and some others are specific to a particular type of crowdsourcing initiatives.   Frequency of occurrence of each common influencing factor is also presented.  Finding from this study can be a beneficial guide in the design of crowdsourcing initiatives.|None|reject|reject
ESEM|2016|Evaluating PRED Criterion in Software Development Effort Estimation|ali idri,ibtissam abnane,alain abran|Software development effort estimation,accuracy measure,MMRE,Pred,Standarised accuracy|Software Development Effort Estimation (SDEE) plays a primary role in software project management. Being able to choose the appropriate SDEE technique remains elusive for many project managers and researchers. Moreover, the choice of a reliable estimation accuracy measure is primordial since SDEE techniques behave differently given different accuracy measures. The most widely used accuracy measures in SDEE are those based on Magnitude of Relative Error (MRE) such as Mean/Median Magnitude of Relative Error (MMRE/MedMRE) and Prediction at level p (Pred(p)). While MMRE has proven to be an unreliable accuracy estimator, favoring SDEE techniques that underestimate, Pred(p) performance as an accuracy measure in SDEE is still poorly investigated. This paper investigates the consistency of Pred(p) as accuracy measure and SDEE techniques selector. Firstly, we investigate whether Pred(p) is biased toward underestimates; and secondly, the Pred(p) criterion as SDEE techniques selector is assessed using seven SDEE techniques over seven datasets. The relationship between Pred(p) and Standardized Accuracy (SA) is also investigated in this study.|None|reject|reject
ESEM|2016|Usability in Agile Development: A Systematic Mapping Study|daniel a. magues,john w. castro,silvia t. acuna|Agile software development,user-centred design,systematic mapping study,usability|Background: Interest in the integration of the agile software development process (ASDP) and user-centred design (UCD) has grown over the last decade. However, there are not many papers that study this issue holistically and uncover the current state of this integration. Aim: This study sets out to answer the following research question: What is the current state of integration between agile processes and usability? Method: We conduct a systematic mapping study (SMS). A SMS is a form of systematic literature review that aims to identify and classify the research papers published about a specified issue. Results: We retrieved a total of 161 primary studies, which we categorized according to four criteria: process, practice, team and technology integration. The largest group refers to process integration with a total of 76 primary studies. Judging by the increase in the number of publications over the last decade, the integration of human-computer interaction (HCI) activities and techniques into the ASDP is an issue of notable interest. Conclusions: There are very few papers that study the literature holistically and report the current state of usability in the ASDP. Additionally, there are no formalized proposals for adopting usability techniques in ASDP.|None|reject|reject
ESEM|2016|Effectiveness of project nominal knowledge for identifying Design Smells in Open Source Software: an exploratory study.|khalid alkharabsheh,yania crespo gonzalez-carvajal,jose ramon rios viqueira,jose angel taboada gonzalez|Design Smell detection,Machine learning,God Class|Background: Previous studies applying machine learning techniques for Design Smell detection focused on using numerical knowledge (metrics). Experiments reported were conducted without involving any nominal knowledge on the projects, which are the context of the classes analysed. Aim: The main goal of this paper is to explore the influence of nominal project information on the prediction of God Class Design Smell in order to evidence that taking this information into account lead to obtain better predictions that can be more useful for developers. In particular, we use project size category, status and domain. Method: A dataset formed by the 12,588 classes of 24 systems with different nominal values for project size categories, status and domain was built. The classes on this dataset were checked against five Design Smell detection tools, which are considered as experts. Due to the nature of the problem, unbalanced training and tests sets can arise easily. Therefore, in the intention of circumvent the unbalancing problem, a set of experiments were conducted with eight different machine learning classifiers. They were trained and tested with crossed sets and their results were evaluated using Accuracy, Kappa and ROC area tests. Results: The classifiers change their behaviour when they are used in sets that differ in the value of the nominal information of their classes. Conclusions: This study concludes the importance and influence of certain nominal project knowledge in the God Class Design Smell prediction through machine learning classifiers and should be taken into account in the future works.|None|reject|reject
ESEM|2016|On the Use of Fault Abstractions for Assessing System Test Case Prioritization Techniques|joao felipe ouriques,emanuela gadelha cartaxo,patricia machado,francisco gomes oliveira neto,ana emilia coutinho|Software testing,System testing,Test case prioritization,Fault abstractions,Empirical software engineering|Background: Empirical studies often evaluate Test Case Prioritization (TCP) techniques by measuring their ability to uncover faults as early as possible. In this context, techniques can be exercised with actual faults detected by failure occurrences during testing execution. While this accounts for validity, results can be biased. For instance, conclusions regarding a TCP technique’s behavior may be affected by the number of failures, faults and, in turn, the failure-fault relationship. Aims: We propose the use of fault abstractions to exercise techniques on empirical studies of system level TCP techniques. Method: Rather than modeling specific classes of faults, our strategy is based on identifying cliques of test cases that are related according to a distance function as a fault abstraction assuming that the test cases can fail due to a number of faults in common. Based on a set of maximal cliques obtained from test case distance graphs along with a set of essential test cases, different profiles of failure-fault relationship can be selected to apply on empirical studies. Results: In our empirical studies, even though our technique is not predictive, a significant amount of fault abstractions calculated by the proposed technique corresponded to actual reported faults. Conclusions: Fault abstractions may represent the relationship between faults and failures in order to assess TCP techniques when none or a small amount of artifacts is available to perform an empirical evaluation.|None|reject|reject
ESEM|2016|Using the Score Feature to Refine Search Strings and Support the Selection of Studies in Systematic Reviews|fabio octaviano,cleiton silva,andre di thommazo,anderson belgamo,elis hernandes,sandra fabbri|systematic review,systematic literature review,search string,studies selection,score feature,StArt tool|Background: Systematic review (SR) is used to find relevant evidence about a research topic. Some SR activities are subjective, like the search string definition and studies selection. Aim: To present how the score feature supports the search string refining and the selection activity. Method: Based on the score feature, the studies are ranked and clustered. Keywords of the highest ranked papers are used to refine the search string, and the clusters are used to support the selection activity. A case study containing six SRs is presented in order to analyze the support provided by the score feature. Results: Our approach detected new keywords to refine the search string. Besides, regarding the selection activity, on average, our approach obtained a precision of 62.2% on a recall of 83.2%. Conclusions: The score feature can improve the quality of a search string by detecting missing keywords and by prioritizing the reading of studies in the selection activity.|None|reject|reject
ESEM|2016|The Use of Ontologies in Embedded Systems: a Systematic Mapping Study.|aeda sousa,celso agra,tarcisio couto,rosa candida,fernanda alencar|systematic mapping,ontology,embedded systems|Many domains of embedded systems (e.g., automotive, avionics, telecommunications, consumer electronics, industrial automation, and medical) are rapidly evolving toward solutions that integrate hardware and software or incorporate complete systems on a single chip. The specificities of the concepts often complicate data analysis and identifying relationships between them. The proposition of an ontology is necessary. Ontology, an important engineering artifact in several domains, provides uniformity to the concepts in terms of syntax and semantics and facilitates communication in various fields. In this work, a systematic literature mapping was conducted to investigate using ontologies to develop embedded systems in order to identify the languages that have been used, for which domains of embedded systems, and the benefits of using ontologies in developing these systems. After applying selection criteria in the mapping driving phases, 19 papers were selected and analyzed. This mapping provides evidence of benefits in using ontologies for embedded systems.|None|reject|reject
ESEM|2016|Requirements Engineering for Embedded Systems: A Systematic Literature Review|tarcisio pereira,deivson albuquerque,aeda sousa,fernanda alencar,jaelson castro|Embedded Systems,Requirements Engineering,Systematic Literature Review|[Background] In the embedded systems area, more than 50% of problems occur at system delivery and are related to misconceptions in capturing requirements. A requirements engineering process is crucial to meeting time, cost, and quality goals. Despite advances in embedded development, we argue that few requirements engineering initiatives meet these systems particularities. [Aims] We examine available studies that propose approaches, methods, techniques, and processes to support requirements engineering for embedded systems. [Method] We perform a systematic literature review to identify, evaluate, synthesize, and present the available studies. [Results] We analyzed 67 studies and found (1) evidence of benefits to using the studies in requirements engineering activities, especially for reducing ambiguity, inconsistency, and incompleteness of requirements; (2) the majority of studies only partially address the requirements engineering process; and (3) evidence suggesting topics for future developments. [Conclusion] This research presents an advance for researchers knowledge through summarizing existing research efforts.|None|reject|reject
ESEM|2016|Process Monitoring Using Network Analysis Based on Source Code Repository Data|alma orucevic-alagic,martin host|Open Source,Social Network,Change Evaluation|The emergence of new development practices, e.g. agile development, has prompted many companies to implement extensive changes to the way software development effort is organized and managed. Understanding if and how the implemented changes affect the way the teams collaborate to produce software products is important in assessing the effects of the implemented changes. One way to analyze teams' collaborations is by studying collaboration networks. The goal of the research is to understand if and how the developers' network metrics can be used to assess and monitor the effectiveness of the newly introduced changes. In this work, network analysis of data extracted from a source code repository of a large unit within Ericsson, a Swedish multinational company with focus on communication technology, is performed. The unit has undergone major organization and development process changes in the recent years. For this purpose the source code repository data was mined in order to calculate relevant network metrics. The resulting network metrics were validated through three focus group meetings with participants from the company. The presented network analysis based approach has shown to be effective and useful in the assessment of software development dynamics. In addition, based on the feedback received in the focus group meetings, the underlying metrics can be useful to correctly assess software development structures and monitor how they evolve over the time. Developers' network metrics can be used to identify and track changes in specialized development subgroups and degree of collaboration.  More studies are needed to gain a better understanding on the best usage of the metrics.|None|reject|reject
ESEM|2016|A Study on the Influence of Software and Hardware Features on Program Energy|ajitha rajan,adel noureddine,panagiotis stratis|Energy,Software Performance,Feature Selection,Compiler Optimisation|Software energy consumption has emerged as a growing concern in recent years. Managing the energy consumed by a software is, however, a difficult challenge due to the large number of factors affecting it -- namely,  features of the processor, memory, cache, and other hardware components, characteristics of the program and the workload running, OS routines, compiler optimisations, among others. In this paper we study the relevance of numerous architectural and program features (static and dynamic) to the energy consumed by software. The motivation behind the study is to gain an understanding of the features affecting software energy and to provide recommendations on features to optimise for energy efficiency. In our study we used 58 subject programs, each with their own workload, from different application domains. We collected over 100 hardware and software metrics, statically and dynamically, using existing tools for program analysis, instrumentation and run time monitoring. We then performed statistical feature selection to extract the features relevant to energy consumption. We discuss potential optimisations for the selected features. We also examine whether the energy-relevant features are different from those known to affect software performance.|None|accept|accept
ESEM|2016|Model-Based Security Engineering for Cyber-Physical Systems: A Systematic Mapping Study|phu h. nguyen,shaukat ali khan,tao yue|Systematic Mapping,Cyber-Physical Systems,Security,Model-Based Engineering,Security Engineering,Survey,Snowballing|Cyber-physical systems (CPSs) have emerged to be the next generation of engineered systems that would have enormous impacts for human beings. The security of CPSs must be taken into account more early, seriously, and systematically than the security of information systems, which often came as an afterthought. Model-Based (Security) Engineering methodology could be one of the key solutions to the handling of complex systems, including (the security of) CPSs. In this paper, we present a systematic mapping study (SMS) of Model-Based Security Engineering for CPSs (MBSE4CPS) to shed some light into an emerging, interdisciplinary, but important research area. From thousands of relevant publications, we have thoroughly identified a set of primary MBSE4CPS studies for answering our research questions. The analysis results show that the last two years (2014-2015) saw a significant increase in the number of MBSE4CPS publications. Within the MBSE4CPS approaches, the popularity of using Domain-Specific Languages is comparable with the use of the standardized UML modeling notation. Another key finding is that most approaches did not address specific security objectives explicitly but rather focused on security analyses in general on threats, attacks, or vulnerabilities. We also found that none of the studies explicitly considered handling uncertainty, which is one of the key challenges in CPSs. Based on the results, we discussed the open issues in MBSE4CPS research so far as well as proposed some directions for future work.|None|reject|reject
ESEM|2016|Who Should Take This Task? Dynamic Decision Support for Crowd Workers|ye yang,mohammad rezaul karim,razieh saremi,guenther ruhe|crowdsourced software development,worker behaviors,dynamic decision making,submission rate,submission score,task-quitting,competition pressure|The success of crowdsourced software development (CSD) depends on a large crowd of trustworthy software workers who are registering and submitting for their interested tasks in exchange of financial gains. Preliminary analysis on software worker behaviors reveals an alarming task-quitting rate of 82.9%, which consequently results in a low task completion rate. While the reasons for different workers to quit a competition may be complex, there is a need to better understand and support software workers’ decisions to accept and complete a task. In this paper, we propose a novel problem formulation and an analytics-based decision support methodology to guide workers in acceptance of offered development tasks. We report the analysis results for more than one year's real-world data including 4907 development tasks and 8108 workers from the leading CSD TopCoder platform. Applying Random Forrest based machine learning with dynamic updates, we can predict a worker as being a likely quitter with 96% precision and 98%-99% recall accuracy. Similarly, we achieved 80-82% precision and 79% recall for the worker winner class. In total, the results imply an increased total success rate and reduced failure rate of tasks performed.|None|reject|reject
ESEM|2016|Investigating the Feasibility of Using Synthetic Data For Cross-Project Software Defect Prediction|kwabena ebo bennin,jacky w. keung,nachai limsetho,solomon mensah|Cross-Project,Defect Prediction,Sampling Technique,Data transformation|Background: Defect prediction aims at finding defective classes or modules that helps in the prioritization and allocation of scarce testing resources. The prediction models proposed in recent studies work well when historical data exist for training the models. A major problem thus arises when historical data is not available for the prediction of a new project. Cross project defect prediction (CPDP) where data is obtained from other projects has been proposed and has shown promising results in recent studies to help address this concern. Recently proposed approaches such as Burak Filter and application of clustering, aid in selecting appropriate and similar project data, which can be used for constructing a predictive model for a new project. However, characteristics or homogeneity of data sets selected by these approaches impedes the success of CPDP. Data transformation approaches could help the predictive model adapt notwithstanding the characteristics of the filtered data set. Aim: In this study, we investigate the feasibility of using synthetic data generated from a filtered dataset for CPDP and empirically validate its effectiveness in the domain of CPDP. Method: Employing the Burak Filter to obtain new datasets for CPDP, synthetic data generated by applying two oversampling techniques SMOTE and ADASYN are investigated. The results are compared to examine the feasibility of using only the synthetic data for training the defect prediction model for CPDP. Five defect prediction models and fifty three datasets extracted from the PROMISE repository were considered in this comprehensive empirical experiment and evaluated using the Precision, PD, PF, F-measure and G-mean. Results: Predictive models trained on synthetic datasets had better performance in relation to PF values but worse in all the other performance measures. The original filtered data performed better and was statistically significant than the synthetic data. Conclusion: Prediction results provided by these synthetic data sets are comparable those provided by the original filtered data set in terms of prediction performance. It is recommended to use synthetic data generated with SMOTE only when lower PF values are required.|None|reject|reject
ESEM|2016|Empirical Investigation of Motivators in Green IT-Outsourcing|siffat ullah khan,rahmat ullah khan,rafiq ahmad khan|Green IT-outsourcing,Questionnaire Survey,Vendors|From the last decade the idea of green computing has commenced to widen, enhancing reputation. In addition the widespread understanding to environmental concerns, such attention too stalks from financial wants, while both power expenses and electrical necessities of IT production around the world show a constantly rising tendency. Green computing is the study and practice of efficiently using computer resources. Initially we conducted and published systematic literature review for the identification of motivators in Green IT-outsourcing from vendors’ perspectives. In this paper we present findings of a research survey being conducted in software development outsourcing industry to find out various motivators that can positively affect Green IT-outsourcing. We have performed questionnaire survey that was conducted using both online resources and face-to-face meetings and finally we got responses from 47 experts from different Green IT-outsourcing companies. In the survey we asked from the participants to grade each motivator on a seven point Likert scale to determine the perceived consequence of each motivator. We identified 9 motivators for vendors including ‘Promoting reusability and sustainability both in terms of hardware and software’ and ‘Improving quality of services’.|None|reject|reject
ESEM|2016|On the Evolution of Developers Expertise and Roles on GitHub|eleni constantinou,georgia kapitsaki|Role,Expertise,Contributor,Evolution,GitHub,Stack Overflow|Background: Contributions to open source software provide evidence about developers' expertise and roles. Acquiring this information can assist in identifying their competencies in specific software technologies. Existing expertise and role identification metrics focus on identifying experts within the contributors of a software project. Following a project-centric approach benefits the needs of expert identification within the project, but provides a limited view of developers' expertise that lie beyond a single project. Aims: We aim to identify contributors' expertise and roles by considering their contribution history across multiple projects and study their evolution over time. Method: Firstly, we extract information from Stack Overflow in order to identify terms related to software technologies and next, we employ information from GitHub in order to extract contributors' activity on specific technologies. Secondly, we present four contributor roles, namely developer, technical leader, bug fixer and bug contributor, which are defined according to different activity types in social coding platforms. Finally, we define the Depth and Breadth evolution models in order to draw useful conclusions about contributors' expertise in different software technologies. Results: Our findings show that the majority of contributors are committed to their activities as developers and bug contributors in technologies they have previously gained expertise. The majority of contributors scarcely contribute in activities as technical leaders and bug fixers. Finally, developers tend to focus on technologies they have already worked with, rather than acquire new expertise by contributing to projects of new technologies. Conclusions: The results of our study reveal trends in developers' contributions in open source software and tendencies in their evolution over time. Our results can be exploited by employers for identifying experts or by practitioners for showcasing their expertise in various technologies with different roles.|None|reject|reject
ESEM|2016|Using Metrics for Modeling the Impact of Design Patterns on Quality along Software Evolution|sofia charalampidou,apostolos ampatzoglou,paris avgeriou,seren sencer,elvira-maria arvanitou,ioannis stamelos|Design patterns,Quality,Metrics,Evolution|Design patterns have been widely recognized as reusable solutions that can be applied for improving design quality. However, the effect of a pattern is not uniform in all of its instances, for all quality attributes and along software evolution, since it is influ-enced by some pattern-related parameters (e.g., number of classes and methods that participate in the instance). Therefore, there is a need for guidance on deciding when a pattern is beneficial and when not. In this paper we use an existing analytical method to simulate the effect of patterns on software design quality, along software evolution. The application of the method can guide deci-sion making about when to insert, maintain or remove a pattern from a system. As an illustrative example, we present and discuss the results of the simulation on the Decorator design pattern and explore possible evolution scenarios. For example, the results suggest that Decorator instances that are not expected to evolve through the addition of components in composite objects decrease system cohesion; consequently, modularity and maintainability are weakened.|None|reject|reject
ESEM|2016|Evaluation of a Maturity Model for Integrating Agile Processes and User Centred Design: Lessons Learnt and Conclusions|dina salah|Evaluation,Maturity Model,Integrating Agile and User Centred Design,Agile,User Centred Design|This paper discusses the evaluation process of a maturity model for integrating Agile processes and user centred design. The paper provides details on three phases of evaluation including author evaluation, expert reviewers evaluation and case study evaluation. The paper discusses the results of those various evaluation phases and how it impacted the model design and development.|None|reject|reject
ESEM|2016|Toward Technical Debt Aware Software Modeling|gonzalo rojas,clemente izurieta,isaac griffith|Technical Debt,Model Driven Engineering,Model Refactoring|Over the last decade, the technical debt metaphor has gained in popularity, and many tools exist today that can calculate the debt associated with a miscellany of source code. However, no corpus of studies has investigated the effects that creation and refactoring of conceptual models have on technical debt of corresponding source code. Our work addresses this fundamental gap by first providing a map of correspondences between recognized model smells of UML Class Diagrams and Java source code issues. We then describe a set of formal experiments we perform to calculate the technical debt of generated source code as a result of refactorings performed on their corresponding models. Our results reveal a significant disconnect between model smells and technical debt values of resultant generated source code, and little effect of model refactorings on reducing these values. However, once correspondences between model smells and code issues are defined, model refactoring proves helpful in preventing technical debt from a high abstraction level. We further exemplify this scenario by providing an in-depth causal analysis example.|None|reject|reject
ESEM|2016|Clean Code: An Industry Survey and Open Source Study|vidmantas blazevicius,jamie stevenson,murray wood|Clean Code,Questionnaire,Survey,Open Source Software|Background: Clean Code practices are strongly advocated but there is a lack of empirical evidence about their use and value. Aims: To determine the contribution of Clean Code practices to software design quality and to investigate their use in open source systems. Method: A questionnaire survey investigating software design quality practices was completed by 102 industrial software developers from around the world. An automated analysis of 20 open source systems was performed investigating seven properties related to Clean Code development practices. Results: Analysis of the survey results found a recurring theme regarding the positive contribution that Clean Code practices make to software design quality including: meaningful names, small size, single responsibility, judicious commenting and test driven development. Analysis of the open source systems found general trends that could suggest use of Clean Code practices: long domain-influenced names, generally small functions with limited complexity and few function arguments. There were mixed results on class size and complexity, and a lack of strong evidence on comment density and code duplication. Conclusions: The contribution made by this paper is evidence that developers working in industry are using Clean Code practices to achieve design quality. Evidence is also presented that suggests many Clean Code practices are being adopted by open source software developers. There is a need for more empirical investigation of Clean Code practices, gathering evidence of their use and exploring their potential design quality benefits. There is also scope for tool support which encourages adoption of these practices and which also highlights non-adherence.|None|reject|reject
ESEM|2016|Identifying Thresholds for Software Faultiness via Optimistic and Pessimistic Estimations|luigi lavazza,sandro morasca|Fault-proneness,Faultiness,Thresholds,Grey Zones,Software measures|Background. When estimating whether a software module is faulty based on the value of a measure X for a software internal attribute (e.g., size, structural complexity, cohesion, coupling), it is sensible to set a threshold on fault-proneness first and then induce a threshold on X by using a fault-proneness model where X plays the role of independent variable. However, some modules cannot be estimated as either faulty or non-faulty with confidence: they belong to a "grey zone" and estimating them as either would be quite aleatory and may result in several erroneous decisions. Objective. We propose and evaluate an approach to setting thresholds on X to identify which modules can be confidently estimated faulty or non-faulty, and which ones cannot be estimated either way. Method. Suppose that we do not know if the modules belonging to a subset of a set of modules are faulty or not, as happens in practical cases with the modules whose faultiness needs to be estimated. We build two fault-proneness models by using the set of modules as the training set. The "pessimistic" model is built by assuming that all modules whose faultiness is unknown are actually faulty and the "optimistic" model by assuming that they are actually non-faulty. The optimistic and pessimistic models can be used to set two thresholds: we estimate faulty those modules whose fault-proneness is above the optimistic threshold and non-faulty those whose fault proneness is below the pessimistic threshold. Modules that are not estimated either faulty or non-faulty or that are estimated both faulty and non-faulty are in the "grey zone," i.e., no reliable faultiness estimation can be made for them. Results. We applied out approach to datasets from the PROMISE repository, we carried out cross-validations, and we assessed accuracy via commonly used indicators. We also compared our results with those obtained with the conventional approach that uses one Binary Logistic Regression model. Our results show that our approach is effective in identifying the "grey zone" of values of X in which modules cannot be reliably estimated as either faulty or non-faulty and, conversely, the intervals in which modules can be estimated faulty or non-faulty. Our approach turns out to be more accurate, in terms of F-measure, than the conventional one in the majority of cases. In addition, it provides F-measure values that are very concentrated, i.e., it consistently identifies the intervals in which modules can be estimated faulty or non-faulty. Conclusions. Our method can be practically used for identifying "grey zones" in which it does not make much sense to estimate modules' faultiness based on measure X and, therefore, the zones in which modules' faultiness can be estimated with confidence.|None|accept|accept
ESEM|2016|Detection of Requirement Errors and Faults via a Human Error Taxonomy: A Feasibility Study|wenhua hu,jeffrey carver,vaibhav anu,gursimran s. walia,gary bradshaw|Human Errors,Software Requirements,Software Quality Improvement|Background: Developing correct software requirements is important for overall software quality. Most existing quality improvement approaches focus on detection and removal of faults (i.e. problems recorded in a document) as opposed identifying the underlying errors that produced those faults. Accordingly, developers are likely to make the same errors in he future and not recognize other existing faults with the same origins. Therefore, we have created a Human Error Taxonomy (HET) to help software engineers improve their software requirement specification (SRS) documents. Aims: The goal of this paper is to analyze whether the HET is useful for classifying errors and for guiding developers to find additional faults. Methods: We conducted a empirical study in a classroom setting to evaluate the usefulness and feasibility of the HET. Results: First, software developers were able to employ error categories in the HET to identify and classify the underlying sources of faults identified during the inspection of SRS documents. Second, developers were able to use that information to detect additional faults that had gone unnoticed during the initial inspection. Finally, the participants had a positive impression about the usefulness of the HET. Conclusions: The HET is effective for identifying and classifying requirements errors and faults, thereby helping to improve the overall quality of the SRS and the software.|None|accept|accept
ESEM|2016|Replication Management Tools and Materials for Use in Replication in Experimental Software Engineering: A Systematic Mapping Study|edison espinosa,juan marcelo ferreira aranda|Experimental Software Engineering,Experimental Replication,Experimental Material,Experimental Material Management|Experimental Software Engineering (ESE) applies experimentation to get knowledge about products, methods and techniques used in the software development process. The original experiment should be repeated as many times as necessary to validate that knowledge. Lots of information and materials about experiment is required for an experimenter to perform a replication. Before the execution of the experimental replication, all or part of the materials may require changes producing new or modified versions of these. After experimental replication is executed, it is expected that all or part of the materials are incorporated in the material of the original experiment. The increase of the number of replications of the original experiment is directly related to the increase of the versions of the experimental material,  generating commonly confusion and disorder in the organization. In this work, we conduct a mapping study to locate articles about Replication Management Tools and Materials for use in experiment replication in ESE. The results show the lack of articles in this subject. Furthermore, most of them suffers from problems in the replication version management and experimental materials to carry out a replication process. These data provide interesting information to start a research of the adoption of the software configuration management paradigm in the experimental material management.|None|reject|reject
ESEM|2016|Can Reproducible Research Improve Empirical Software Engineering Practice?|lech madeyski,barbara a. kitchenham|Empirical Software Engineering,Reproducible Research,Scientific Practice|Context: Researchers have identified problems with the validity of software engineering research findings. In particular, it is often impossible to reproduce data analyses, due to lack of raw data, or sufficient summary statistics, or undefined analysis procedures. Objective: The aim of this paper is to raise awareness of the problems caused by unreproducible research in software engineering and to discuss the concept of reproducible research (RR) as a mechanism to address these problems. RR is the idea that the outcome of research is both a paper and its computational environment. Method: We report some recent studies that have cast doubts on the reliability of research outcomes in software engineering. Then we discuss the use of RR as a means of addressing these problems. Results: We discuss the use of RR in software engineering research and present the methodology we have used to adopt RR principles. We report a small working example of how to create reproducible research. We summarise advantages of and problems with adopting RR methods. Conclusion: We conclude that RR supports good scientific practice and would help to address some of the problems found in empirical software engineering research.|None|reject|reject
ESEM|2016|How Practitioners Perceive the Relevance of ESEM Research|jeffrey carver,oscar dieste tubio,nicholas a. kraft,david lo,thomas zimmerman|Survey,Industrial Relevance,ESEM Conference|Background: The relevance of ESEM research to industry practitioners is key to the long-term health of the conference. Aims: The goal of this work is to understand how ESEM research is perceived within the practitioner community and provide feedback to the ESEM community ensure our research remains relevant. Method: To understand how practitioners perceive ESEM research, we replicated previous work by sending a survey to several hundred  industry practitioners at a number of companies around the world. We asked the survey participants to rate the relevance of the research described in 161 ESEM papers published between 2011 and 2015. Results: We received 10,230 ratings by 437 practitioners who labeled ideas as Essential, Worthwhile,  Unimportant, or Unwise. The results showed that overall, industrial practitioners find the work published in ESEM to be valuable: 58\% of all ratings were essential or worthwhile. We found no correlation between citation count and perceived relevance of the papers. Through a qualitative analysis, we also identified a number of research themes on which practitioners would like to see an increased research focus. Conclusions: The work published in ESEM is generally relevant to industrial practitioners. There are a number of topics for which those practitioners would like to see additional research undertaken.|None|reject|reject
ESEM|2016|Improving Inspection Team Performance Using Cognitive Learning Styles: An Empirical Analysis|anurag goswami,gursimran s. walia,urvashi rathod|Requirements,software inspection,learning style,virtual team|Background - Software industries focus on improving the quality of software products by utilizing software inspections, to detect and eliminate faults in early software artifacts. However, evidence suggests that overall performance of an inspection team is highly dependent on an individual inspectors’ ability to detect faults. This paper leverages research on cognitive Learning Styles (LS) (i.e., characteristic strengths of inspectors as they acquire and process information recorded in software artifacts) to improve their team performance. Aim - The goal of this research is to improve inspection team performance by staffing inspectors based on the disparity of their LS’s to reduce the overlap and increase fault coverage. Method - Using inspection data from forty participating subjects in India and meta-analysis of our previous LS studies, we analyzed the impact the LS’s of individual inspectors had, on inspection team performance by creating and evaluating virtual inspection teams of varying size. Results - The results in this study showed that the larger inspection teams (size 6 and more) formed with inspectors of diverse LS’s outperform teams with team of inspectors with similar LS’s. These results also provide insights into the nature of faults found by inspectors belonging to different LS’s. Conclusion - These results can benefit software managers in managing inspection process, enabling higher cost savings, and improving software quality.|None|reject|reject
ESEM|2016|Using Eye Tracking to Investigate Reading Patterns and Learning Styles of Software Requirement Inspectors to Enhance Inspection Team Outcomes|anurag goswami,gursimran s. walia,mark mccourt,ganesh padmanabhan|Requirements,Software Inspection,Learning Styles,Eye Tracking|Inspections helps software industries in saving rework effort and cost by detecting faults in the early software artifacts. Results from previous research shows that inspection performance of a team rely on each inspectors’ ability to detect and report faults. To find factors of individuals’ ability in inspection, our past research takes in individual’s ability to perceive and process information known as Learning Styles (LS) in cognitive psychology research. Results from our research showed that LSs could be used as an effective technique to employ inspections. Software engineering researchers also used the concept of recording eye movements of individuals on screen with some information (known as eye tracking) to enhance various software usability problems. Cognitive psychologists have also used eye tracking concept to validate individuals’ LS preference. To extend results of our research, the concept of eye tracking is utilized along with LS in software requirements inspections to detect reading patterns of inspectors during inspections. The current research uses data (LS, eye tracking, and inspection) from thirteen graduate and twenty six undergraduate students as inspectors to find its impact on inspection effectiveness and efficiency. This research analyzed the reading trend of inspectors in general on inspection performance (i.e. effectiveness and efficiency) and investigates the effect of eye movement factors of individual inspector with various LSs as well as of virtual inspection teams (of varying sizes) on inspection effectiveness and efficiency. Results from this study show that, the more individual inspectors focus at the region where fault exists, the more significant is the inspection effectiveness. For individual LS categories and team based inspections, results showed that inspectors with sequential LS are preferred for requirements inspection.|None|accept|accept
ESEM|2016|The Effect of Gang-of-Four Design Patterns Usage on Design Quality Attributes|shahid hussain,jacky w. keung|Design Patterns,Quality,Spearman Correlation,Reuability,Understandability,Flexibility,Extendebility,Effectiveness|In the plethora of studies, it has been empirically investigated that theincidence of design pattern instances can be considered as an indicator to elaborate the software design. The agile developers, who have more concern with design quality, are interested to know the effect of use intensity of design patterns on the system level design quality attributes. The objective of our study is to empirically investigate the effect of frequent use of the Gang-of-Four (GoF)design patterns on the design quality attributes. We perform a case study which includes three analyses in order to investigate, 1) the existence ofa correlation between design pattern usage and design quality attributes, 2) the confounding effect of system size (number of classes) on the correlation and 3) how the change in number of employed design pattern instances affects the design quality in the subsequent releases of a system. The result of this study suggests that reusability, flexibility and understandability have a significant relationship with the employed instances of Template,Adapter-Command, Singleton and State-Strategy  design patterns, however it is affected by the confounding effect  of system size.Moreover,in the subsequent releases of an open source project named velocity, we observed the use intensity of Singleton, Adapter-Command and State-Strategy design patterns can improve the design quality in term of reusability and flexibility attributes.|None|reject|reject
ESEM|2016|Assessment of the SEMCO Model-based Repository Approach for Software System Engineering|brahim hamid|Modeling Artifact,Model-based Repository,Model Repository,Metamodel,Model-Driven Engineering,Software System Engineering|We have developed a methodological tool support for software development based on the reuse of dedicated subsystems, that have been pre-engineered to adapt to a specific domain. We use Model-Driven Engineering (MDE) to develop a repository of models and a methodological tool support for developing software systems based on this repository. This paper proposes an empirical evaluation of the proposed approach through its practical application to a use case in the railway domain with strong security and dependability requirements, followed by a description of a survey performed among domain experts to better understand their perceptions regarding our approach. The case study enables us to determine that our approach centered around a model-based repository of patterns leads to a reduction in the number of steps in the engineering process or to their simplification. The survey assists in assessing whether domain experts agree on the benefits of adopting the model-based repository approach in a real industrial context. Since patterns were provided as models for the application developers, these results may be generalized to other model-based development.|None|reject|reject
ESEM|2016|Measuring Developers’ Contribution in Open Source Projects using Software Quality Metrics|patricia de bassi,pedro banali,emerson paraiso|Software Quality Metrics,Open Source Projects,Developer Contribution|Background: Software development has become an essential activity, as organizations increasingly rely on it to manage their business. People involved in software development seek better ways of developing quality software. As software development can be seen as a collaborative activity dependent on technology and performed by group of people, developers’ habits have an impact on software quality. The quality can be directly linked to the degree of collaboration and commitment of the development team. Aims: This research aims to identify and analyze software quality metrics that are able to measure software development team members’ participation regarded to contribution in the project source code. Method: We selected and categorized a set of quality metrics, applied them to open source code and analyzed them in other to evaluate members’ contribution. Results: Results show that it is possible to determine whether the team member contribution increases, decreases or had no influence on source code quality. From a total of 20 software quality metrics already known in the literature, only a few have significant influence on source code maintainability, testability, reusability and understandability. Conclusions: A practical effect of this research is to contribute with software engineering project managers to conduct and better organize project development teams, taking into account developer’s quality code profile. Also developers can realize how they are interfering on code quality project and correct themselves. As a consequence, incrementing project team qualification and collaboration may increase team development productivity and code quality.|None|reject|reject
ESEM|2016|Replicating and Comparing Vulnerability Prediction Models on the Linux Kernel|matthieu jimenez,mike papadakis,yves le traon|Vulnerabilities,Prediction Models,Linux Kernel,Machine Learning,Software Security|Background: To assist the vulnerability identification process, researchers proposed vulnerability prediction models that highlight (for inspection) the parts of the system that are likely to be vulnerable. Aims: Our aim is to make a reliable comparison of the main vulnerability prediction models. Thus, we seek for determining their effectiveness, i.e., their ability to distinguish between vulnerable and non- vulnerable components, in the context of the Linux Kernel, under “experimental” (random data), “realistic” (simulate actual cases) and “practical” (using historical data to predict future ones) settings. Method: To achieve the above aims, we mined vulnerabilities reported in the National Vulnerability Database and created a large dataset with all vulnerable components of Linux from 2005 to 2016. Based on this, we then implement and evaluate the prediction models under the examined settings. Results: We demonstrate that an approach based on the header files included and on function calls performs best when aiming at future vulnerabilities, while text mining is the best one when aiming at random instances. We also found that models based on code metrics perform poorly. Conclusion: We show that in the context of the Linux kernel, vulnerability prediction models can retrieve more than 75% of all future vulnerabilities. Thus, practitioners have a valuable tool for prioritizing their security inspection efforts.|None|reject|reject
ESEM|2016|Is GUI Testing Identical to System Testing? A Comparative Study|abdulaziz alkhalid,yvan labiche|System testing,GUI testing,Entity-Control-Boundary design principle|The practitioner interested in reducing software verification effort may found herself lost in the many alternative definitions of Graphical User Interface (GUI) testing that exist and their relation to the notion of system testing. One result of these many definitions is that one may end up testing the same parts of the Software Under Test (SUT) twice. To clarify two important testing activities for the avoidance of duplicate testing effort, this paper studies possible differences between GUI testing and system testing experimentally. Specifically, we selected a SUT that was designed using the Entity-Control-Boundary principle, equipped with system tests directly interacting with the control classes; We used GUITAR, a well known GUI testing software to GUI test this SUT; We compare structural coverage of the two test suites. Experimental results show that coverage of system testing is better than that of GUI testing.|None|reject|reject
ESEM|2016|DIGS – A Framework for Discovering Goals for Security Requirements Engineering|maria riaz,jonathan stallings,munindar singh,john slankas,laurie a. williams|Security goals,Security requirements,Patterns,Controlled experiment,User study,Framework|Context: The security goals of a software system provide a foundation for security requirements engineering. Identifying security goals is a process of iteration and refinement, leveraging the knowledge and expertise of the analyst to secure not only the core functionality but the security mechanisms as well. Moreover, a comprehensive security plan should include goals for not only preventing a breach, but also for detecting and appropriately responding in case a breach does occur. Goal: The objective of this research is to support analysts in security requirements engineering by providing a framework that supports a systematic and comprehensive discovery of security goals for a software system. Method: We develop a framework, Discovering Goals for Security (DIGS), that models the key entities in information security, including assets and security goals. We systematically develop a set of security goal patterns that capture multiple dimensions of security for assets. DIGS explicitly captures the relations and assumptions that underlie security goals to elicit implied goals. We map the goal patterns to NIST controls to help in operationalizing the goals. We evaluate DIGS via a controlled experiment where 28 participants analyzed systems from mobile banking and human resource management domains. Results: Participants considered security goals commensurate to the knowledge available to them. Although the overall recall was low given the empirical constraints, participants using DIGS identified more implied goals and felt more confident in completing the task. Conclusion: Explicitly providing the additional knowledge for the identification of implied security goals significantly increased the chances of discovering such goals, thereby improving coverage of stakeholder security requirements, even if they are unstated.|None|reject|reject
ESEM|2016|A Controlled Experiment on the Effects of Time-Pressure on Confirmation Bias in Software Testing|iflaah salman,burak turhan,pilar rodriguez marin|Software Quality,Software Testing,Cognitive Biases,Time Pressure,Organisational Factors,Software Psychology,Human Factors,Experiment|Background: Time-pressure is an inevitable reality in software industry in all phases of development. Software engineers who carry out testing activities also work under time-pressure due to project schedules. Research in other disciplines showed that time-pressure could be a trigger of confirmation bias, which is people’s tendency to look for evidence that strengthen their prior believes rather than those that refute those believes. This might in turn negatively impact software testing. Objective: In this study we investigate whether time-pressure is a factor to trigger confirmation bias in software testing context, particularly in generating functional test-cases either to verify or contest a given set of specifications. Method: We performed a controlled experiment with 42 graduate students and assessed their confirmation bias in terms of consistent and inconsistent functional test-cases designed by the participants based on the specifications provided. For this purpose we chose one factor with two treatments between-subjects experimental design. Results: We did not find a significant difference in the confirmation bias measures of time-pressure and no-time-pressure groups. However, difference-of-relative-measures of consistent vs. inconsistent test cases suggests a low coverage in terms of completeness of tests, with a minor absolute difference in favour of no-time-pressure group. Conclusion: We report a negative result in this work regarding the effect of time pressure on confirmation bias in the context of functional software testing. Further replications and investigation of other factors contributing to potential confirmation bias manifestation are required to understand this phenomenon better.|None|reject|reject
ESEM|2016|What Bugs May be Captured and Fixed Before Release? An Empirical Study on Open Source Projects|yonghui huang,feng zhang,ying zou,ahmed e. hassan|bugs,empirical study,early discovered,latency,regression,testing|Bugs can be discovered sooner or later after the release of a software product. Among the early discovered bugs, some bugs may be fixed quickly and some other bugs require a longer time to fix. The bugs that can be discovered early and fixed quickly after a release can have a higher chance to be captured and fixed in the testing phase before the release. In this study, we aim to find the characteristics of the early discovered and quickly fixed bugs. We conduct an empirical study using eight open source projects from Apache repository. In our study, bug reports are first divided into two groups (i.e., the early and the late  discovered bugs) using the median bug discovery time. Then we further separate the early discovered bugs into two subgroups (i.e., the quickly and the slowly fixed bugs) using the median bug fixing time. We characterize bugs using eight metrics that measure the bug fixing process from four perspectives (i.e., time aspect, discussion, code change and testing), and compare the characteristics of bugs across groups. Our findings suggest developers focus on the code change that involves fewer developers and possibly introduces any of the four problems, such as Configuration, Documentation, Feature request and Incompatibility. These insights could help developers identify a subset of code changes for testing before the release.|None|reject|reject
ESEM|2016|Metamodeling applied to a Measurement System|karine valenca,edna dias canedo,ricardo a. d. kosloski|Measurement,Metamodeling,Database|Measurement processes, specifically in software engineering, are of great importance to organizations' success. Important data for cost and time estimates only can be obtained by systematic measurement. However, to measurements get good results, it is important collect data from the organization itself. These data should be recorded in a database. In this sense, this paper will present a proposal for metamodeling that allows the user define their owns measurement metrics without changing the application source code.|None|reject|reject
ESEM|2016|Clustering Mobile Apps Based on Mined Textual Descriptions|afnan al-subaihin,federica sarro,sue black,licia capra,mark harman,yue jia,yuanyuan zhang|App Store Analysis,Information Retrieval,Mining Software Repositories,Software Categorisation,Natural Language Processing|Context: Categorising software systems according to their functionality yields many benefits to both users and developers. Objective: In order to uncover the latent clustering of mobile apps in app stores, we propose a novel technique that measures app similarity based on claimed behaviour. Method: Features are extracted using information retrieval augmented with ontological analysis and used as attributes to characterise apps. These attributes are then used to cluster the apps using agglomerative hierarchical clustering. We empirically evaluate our approach on 17,877 apps mined from the BlackBerry and Google app stores in 2014. Results: The results show that our approach dramatically improves the existing categorisation quality for both Blackberry (from 0.02 to 0.41 on average) and Google (from 0.03 to 0.21 on average) stores. We also find a strong Spearman rank correlation (rho = 0.96 for Google and rho = 0.99 for BlackBerry) between the number of apps and the ideal granularity within each category, indicating that ideal granularity increases with category size, as expected. Conclusions: Current categorisation in the app stores studied do not exhibit a good classification quality in terms of the claimed feature space. However, a better quality can be achieved using a good feature extraction technique and a traditional clustering method.|None|accept|accept
ESEM|2016|Predicting Defectiveness of Software Patches|behjat soltanifar,atakan erdem,ayse bener|Code review,Code Review Quality,Software Patch Defectiveness,Defect Prediction|Software code review, as an engineering best practice, refers to the inspection of the code change in order to find possible defects and ensure change quality. Code review has become a standard process in both open source and industrial software projects. Code reviews, however, may not guarantee finding the defects. Thus, there is a risk for a defective code change in a given patch, to pass the review process and be submitted. In this research, we aim to apply machine learning algorithms in order to predict the defectiveness of a patch after being reviewed, at the time of its submission. Predicting defectiveness of change code is beneficial for code reviewers in making patch release decisions. Our empirical results show that, Bayesian Networks that consider different factors involved in review process, in terms of Product, Process and People (3P) is able to predict the defectiveness of the changed code with 76\% accuracy.|None|reject|reject
ESEM|2016|Software structure evolution and relation to subgraph defectiveness|ana vrankovic,tihana galinac grbac|Software defect prediction,network graph,subgraphs,evolving complex systems|Background. Network analysis has been successfully applied in many applications to understand structure effects. In our previous study, we found that the software structure continuously evolve over the system releases and there is a correlation between system defectiveness and subgraph occurrence frequency in the software network graph. Aims. We aim to extend previous study to analyze the relation of structure evolution and defectiveness of subgraphs present in the software network graph. Method. We used basic statistic methods to study subgraph defectiveness across versions of system evolution and across different subgraph types. Results. Software versions have similar behavior in terms of average subgraph type defectiveness and subgraph frequency distributions. However, different subgraph types behave differently in respect to defectiveness distributions. Subgraph defectiveness are good predictors of the number of defects in system version. Conclusion. We found that subgraph defectiveness are good measure for predicting number of defects in system version.|None|reject|reject
ESEM|2016|An Empirical Study on Performance Bugs for Highly Configurable Software Systems|xue han,tingting yu|performance bugs,configurations,characteristics study|Modern computer systems are highly-configurable, complicating the testing and debugging process. The sheer size of the configuration space makes the quality of software even harder to achieve. Performance is one of the key aspects of non-functional qualities, where performance bugs can cause significant performance degradation and lead to poor user experience. However, performance bugs are difficult to expose, primarily because detecting them requires specific inputs, as well as a specific execution environment (e.g., configurations). While researchers have developed techniques to analyze, quantify, detect, and fix performance bugs, we conjecture that many of these techniques may not be effective in highly-configurable systems. In this paper, we study the challenges that configurability creates for handling performance bugs. We study 113 real-world performance bugs, randomly sampled from three highly-configurable open-source projects: Apache, MySQL and Firefox. The findings of this study provide a set of lessons learned and guidance to aid practitioners and researchers to better handle performance bugs in highly-configurable software systems.|None|accept|accept
ESEM|2016|Pitfalls and Challenges on Surveying Evidence in the Software Engineering Technical Literature: a Tertiary Study|talita ribeiro,jobson da silva,guilherme travassos|Systematic literature review,tertiary study,evidence based software engineering,methodology.|BACKGROUND: The number of novice researchers adopting systematic literature reviews (SLRs) as a research strategy to acquire evidence in Software Engineering (SE) has continuously grown. A SLR is intended to present a meticulous and fair research protocol, expecting more rigor in its process definition and execution in order to allow audit and consistency of results when replicated. Therefore, many guidelines have been proposed as a mean of minimizing researchers’ bias, providing guidance regarding its planning and accomplishment, and contributing to increase the confidence in the results. However, these positive initiatives are not enough to guarantee the consistency of results since different outcomes have been observed in SLR executions with similar intentions. AIMS: To investigate the planning, execution and generated outcomes of SLR research protocols when dealing with the same research question and performed by similar research teams of novice researchers along the time. Next, to characterize pitfalls and challenges on executing SLRs by novices in SE. METHOD: To qualitative evaluate and compare (using Jaccard similarity and Kappa agreement coefficients) same goal SLR research protocols and outcomes undertook by similar research teams along the time. RESULTS: From seven research protocols regarding quality attributes for use cases executed in 2010 and 2012, unexpected differences in the planning and execution were observed in all of them. Even when some agreement had been reached, the outcomes ended up different. Six main causes were observed. CONCLUSIONS: Researchers’ inexperience, lack of a common terminology regarding the problem domain, and lack of research protocol and outcomes verification (procedures and answers to the research question crosschecking) represent relevant pitfalls to use SLRs in SE. Hence, more attention has to be given when novices are undertaking SLRs in SE without the presence of a senior researcher. Besides, an in-depth understanding of the terminology used in the topic under investigation is mandatory, once it can support the organization of appropriate search strings (regarding their sensibility and precision) and the reporting of results.|None|reject|reject
ESEM|2016|Controlled Experiments with Student Participants in Software Engineering: A Systematic Mapping Study|marian daun,carolin hubscher,thorsten weyer|Controlled Experiments,Student Experiments,Student Participants|[Context] In software engineering research, emphasis is given to sound evaluations of new approaches. While industry surveys or industrial case studies are preferred to evaluate industrial applicability, controlled experiments with student participants are commonly used to determine measurements such as effectiveness and efficiency of a proposed approach. [Objectives] In this paper, we elaborate on the current state of the art of controlled experiments using student participants. As threats regarding the generalizability are quite obvious, we want to determine how widespread controlled experiments with student participants are and in which settings they are used. [Methods] This paper reports on a systematic mapping study using high-quality journals and conferences from the software engineering field as data sources. We scanned all papers published in the last five years and investigated all papers reporting student experiments in detail. [Results] From 2788 papers under investigation 175 report results from controlled experiments. 109 (62.3%) of these experiments have been conducted with student participants. Most experiments used undergraduate student participants, recruited students on a voluntary basis, and set them tasks to measure their comprehension. However, many experiments lack information regarding the students’ recruitment and other important factors. [Conclusions] Student participation in software engineering experiments can be seen as a common evaluation approach. In contrast, there seems to be little knowledge about the threats to validity in student experiments, as major drivers such as the recruitment are not reported at all.|None|reject|reject
ESEM|2016|Evidence Briefings: Towards a Medium to Transfer Knowledge from Systematic Reviews to Practitioners|bruno cartaxo,gustavo pinto,elton vieira,sergio soares|Software Engineering,Evidence-Based Software Engineering,Systematic Reviews,Knowledge Transfer,Evidence Briefings|Context: Integrate research evidence with practice is one of the main goals of evidence-based software engineering. However, recent studies show that the connection between systematic reviews and practitioners has not fully established. Goal: This paper presents the first steps towards a medium to transfer knowledge acquired from systematic reviews to practitioners. Method: We selected a set of systematic reviews identified by a tertiary study and extracted their findings to generate one-page Evidence Briefings to serve as mediums. A design specialist defined the briefings structure based on information design and gestalt principles. To evaluate the format and content of the briefings we conducted personal opinion surveys based on two groups: StackExchange users that posted questions in topics related to the reviews, and the authors of the selected reviews themselves. The former had a response rate of 21.9% (32 out 146) and the latter 31.8% (7 out of 22). Results: Practitioners rarely use systematic review research papers as mediums to acquire knowledge, since just 9% have told to do so. Both researchers and practitioners positively evaluated the evidence briefings, since 71% and 82% of the StackExchange users and systematic review authors, respectively, agreed or strongly agreed that the briefings' interface is clear. Conclusions: Researchers and practitioners were positive about the content and format of the evidence briefings we proposed. It is also possible to say that there is a gap between practitioners and systematic reviews due to the low percentage of practitioners that consume systematic reviews. The good reception of the evidence briefings from both sides show a possible route to reduce that gap.|None|accept|accept
ESEM|2016|How Systematic Reviews Cover Practitioners’ Needs: A Study on StackExchange Communities|bruno cartaxo,gustavo pinto,fernando kenji kamei,sergio soares|Software Engineering,Evidence-Based Software Engineering,Systematic Reviews,Practitioners,StackExchange,Q&A|Context: One of the main motivations behind Evidence-Based Software Engineering (EBSE) is to leverage the best evidence from research to practice. However, some studies suggest this goal is not being fully accomplished. Objective: This paper presents an assessment on how systematic reviews cover practitioners' needs. Method: We selected 24 systematic reviews identified by a comprehensive tertiary study. Using the search strings of the identified systematic review, we queried the most relevant questions of five active StackExchange communities related to software engineering. After a manual examination of the 1,860 returned questions, created by 1,288 users, we investigated whether our selection of systematic reviews could be used to answer those questions. Results: 505 (27.15%) out of 1,860 returned questions were considered in fact related to the systematic reviews, the others were false-positives. Our analysis showed that the systematic reviews can partially or totally answer 25.74% (130) of these questions. Conclusions: The low ratio of questions that could be answered by the systematic reviews suggests low coverage of practitioners' needs. Qualitative analysis on questions that could not be answered showed that one who wants to conduct a systematic review more connected with practitioners' needs should focus on (1) emerging software development topics and (2) improve the search strings. We also believe that researchers could analyze StackExchange questions on early stages of systematic review planning. It reveals how the research relates to practitioners' needs and also promotes insights that can be included on the systematic reviews' scope of investigation. We also created a list of suggestions that can increase the ratio of answered questions.|None|reject|reject
ESEM|2016|Software Project Managers’ Perceptions of Productivity Factors: Findings from a Qualitative Study|edson oliveira,marco cristo,emilia mendes,tayana conte|Perception,Productivity,Qualitative Study|Background – Despite that productivity plays an important role in numerous software development organizations, to date there is no standard way to measure developers’ productivity. In addition, the management of such human capital is mainly based on how project managers perceive productivity. Therefore, it is important to investigate what these perceptions are, and how they affect a software development organization. Aim – In this study, our main goal is to identify the perception of project managers about the productivity of developers and how such perception is applied in practice. Method – We employed a qualitative research methodology in which we interviewed 12 managers from three software development organizations in the city of Manaus (Brazil). We used semi-structured interviews for data collection and Grounded Theory-based procedures for data analysis. Results – We identified that the managers’ perceptions about developer productivity are influenced by four different factors: (1) tasks delivered on time, (2) produced artifacts that do not need rework, (3) products that meet stakeholders’ expectations, and (4) personal behavior such as focus and proactivity. Managers consider these perceptions mainly at three different situations: (a) when allocating developers to teams; (b) when aiming at improving the developers’ productivity; and (c) to support management decisions related to the developers’ careers. Conclusions – This qualitative study shows a perception of productivity different from the ones portrayed in other research papers. This suggests that the use of Grounded Theory-based procedures may provide a way to enrich our understanding of the phenomenon under investigation.|None|reject|reject
ESEM|2016|Group Learning and Performance in a Large-scale Software Project: Results and lessons learned|ricardo britto,darja smite,lars-ola damm|Large-scale software development,Global software engineering,Group learning,Team performance|Background: Research on teams originated from the social sciences and brought a number of new topics into the repertoire of software engineering. Teams and teamwork are recognized for the promised benefits of increased performance, innovation, and employee satisfaction. Existing research from related disciplines suggests that performance is improved through experience, and along with individual learning, members of a team support each others learning in a number of ways, resulting in what is recognized as group learning. Aims: In this paper we report our attempt to study the relationship between group learning and performance in a large-scale software project. Method: We conducted an exploratory case study using as a case an on-going project that has been conducted for over fifteen years in Ericsson. The investigated data was collected through archival research and both unstructured and semi-structured interviews. We analyzed the collected data using descriptive statistics, charts and regression analysis. Results: Our results show that the investigated teams failed to improve their performance over time (measured herein as productivity), i.e. the accumulated experience is not leading to performance improvements. We made an attempt to identify possible factors that are associated with the absence of performance improvements, however the approach employed herein led us to inconclusive results. Conclusions: Although the results are inconclusive, our hunch is that the fact that the members of the investigated teams did not work together must of the time led them to failure to improve their performance over time. In addition, the fact that they worked simultaneously in different work items could have hinder their performance. However, we believe that further research must be conducted to provide a stronger evidence about the identified result. Based on the experience acquired by conducting this study, we report some lessons learned that can support researchers and practitioners when investigating the topic addressed in this paper.|None|reject|reject
ESEM|2016|Software Instability Analysis Based on Afferent and Efferent Coupling Measures|danilo batista dos santos,antonio maria pereira de resende,eudes de castro lima,andre freire|software instability analysis,afferent coupling,efferent coupling|Software instability measures indicate the degree of susceptibility of needing to make modifications in given entities caused by changes in other related software entities. If there is low instability, then there is evidence that the analyzed entity has little dependence on others. Otherwise, there is evidence that the analyzed entity is sensi-tive to changes occurring in other entities. In the latter case, soft-ware reconstruction could be necessary. Consequently, the higher the value of instability in an entity the more vulnerable it is to unex-pected changes, even if the entity does not suffer direct changes in its code. This article presents an instability measure analysis ob-serving reference values published in scientific articles, and refer-ence values practiced in the open source market. This article adopts the instability definition of Martin [2] that depends on the afferent (Ca) and efferent (Ce) coupling metrics. After conducting a system-atic literature review (SLR), it was noted that there is a shortage of reference values in scientific articles. The authors performed a statistical analysis of instability measures in 107 free software products, involving three different versions of each, totaling 321 product versions. A little less than half (48%) of software products had high instability equal to 1, the maximum value allowed, and the instability average obtained was 0.7. Based on results of this paper, we conclude that several of software have a high instability and keep it high through the releases analyzed. More analysis is neces-sary to confirm this behavior about instability of software through time.|None|reject|reject
ESEM|2016|An Empirical-Based Model for Effort Estimation in Software Reengineering Projects|silvia nunes das dores,duncan dubugras alcoba ruiz|Case Study,Effort Estimation,Empirical Study,Software Reengineering|Background. Effort estimation is one of the core activities of a software development project because it is used for many purposes, such as cost estimation, scheduling, resource allocation and software investment. In the specific context of software reengineering projects, there is a lack of literature of related work and little is known about how the effort estimation is done in practice. Goal. We aim to identify the factors that impact on effort estimation in software reengineering projects, and apply these factors to improve the estimation process. Method. We performed an empirical study in a set of organizations that carry out software reengineering. This approach included an interview study and a case study. Results. We propose a model to support effort estimation in software reengineering projects. This model is based on related literature, best practices, challenges and lessons learned identified in the empirical study. The model includes steps for estimation planning, implementation, monitoring, and continuous improvement. Conclusions. Our findings bring important contribution to the industry and academy. Practitioners can be aware of what may impact the effort estimation in reengineering projects and what can be done to minimize the impact of faced challenges, while researchers can acquire further knowledge of the topic by replicating our study and by providing the industry with processes and tools to improve effort estimation practices.|None|reject|reject
ESEM|2016|Reviewing the Postulates of Agile Methods from the perspective of Argyris’ Theories of Action|diana pereira,fabio q. b. da silva,cesar franca|Management,Agile,Human Factors|Context: Agile methods became popular since its Manifesto publication in 2001, which consolidates a set of propositions built around principles based on trust and self-organizing teams. However, the Agile Manifesto does not suggest an explicit way to achieve it. Background: Argyris has long studied organizations, and proposed a very consistent framework to explain how people can relate to each other in order to be trustful and productive, to build healthy organizations. Objective: this article aims to investigate similarities and divergences between the principles behind the Agile methods and the Argyris’ Theories of Action, as a first step towards an adaptation of Argyris’ framework as a tool to facilitate the construction of truly agile organizations. Method: this article follows a subset of the steps of a meta-ethnographic method to synthesize studies on both subjects. Results: We were able to match codes and categories involving both field, with a total of 17 categories and 4  central ones which seem to be the essential correlation between both. Conclusion: We have found that there are clear intersections between the Agile principles and the Theories of Action, on subjects such as categories named access to information, and clarity of information, which are the bases of good communication process. There are also found some divergent categories when comparing one theory to another, but it is also important to phrase that they seem to be internal inconsistency on the subject focused for Agile postulates.|None|reject|reject
ESEM|2016|Effectiveness Assessment of an Early Testing Technique using Mutation Testing|maria fernanda granda,nelly condori-fernandez,tanja e. j. vos,oscar pastor|Test Suite effectivenes,Effectiveness Assessment,Mutation Testing,Conceptual Schemas testing,Class Diagram mutation|While modern software development technologies enhance the capabilities of model-based/driven development, they introduce challenges for testers such as how to perform an early testing at model level to ensure the quality of the model. In this paper, we present an early testing technique supported by the CoSTest tool, which is empirically evaluated with respect to its effectiveness in terms of its fault detection ability in Conceptual Schemas (CS) and test suites adequacy. This evaluation is carried out by mutation testing with seven Conceptual Schemas that represent the functionality of different software systems. Our findings show that (1) CoSTest effectiveness for detecting faults and the adequacy of the test suites are affected by the mutant type where is executed, achieving better effectiveness in detecting fault types and test suite adequacy in a high order mutant, (2) test suites generated by CoSTest are effective at killing a large number of mutants, (3) there are fault types that our test suites cannot detect.|None|reject|reject
ESEM|2016|Understanding confounding variables in experimental replications for comparing Security tactics and patterns|gilberto pedraza garcia,rene noel,santiago matalonga,hernan astudillo,eduardo b. fernandez|Tactics,Security Patterns,Experimental Software Engineering,Secure Software Design|Context: Security Patterns and Architectural Tactics have been proposed to design secure software systems.  Both approaches systematize design decisions selection for security threats mitigation, but there is no empirical evidence of their comparative impact on products’ quality. Aims: Comparing techniques for threat identification and mitigation, regarding efficacy (quality) and efficiency (effort).  Method and results: An initial experimental study to compare security patterns and security tactics yield no difference in threat mitigation, but relevant difference in threat identification quality. A first replication focused on threat identification and compared two techniques (MUA and MAST), finding no significant difference in quality and effort. A second replication compared both techniques against an ad-hoc approach, and found significant evidence that the latter finds more threats.  Conclusions: Several confounding factors may lead to this counterintuitive result: training, subjects’ experience, and time boxing. We also discuss how the changes in the experimental design and redesign process helped us to understand the phenomena under study.|None|reject|reject
ESEM|2016|Towards a Substantive Theory of Decision-Making in Software Project Management: Preliminary Findings from a Qualitative Study|jose adson da cunha,fabio q. b. da silva,hermano de moura,francisco vasconcellos|Software Project Management,Decision-Making,Qualitative Study|Background: In software project management, the decision-making process is a complex set of tasks largely based on human relations and on specific knowledge and individual cultural background. The factors that affect the decisions of the software project managers (SPMs) as well as their potential consequences require attention because project delays and failures are usually related to a series of poor decisions. Aims: To understand how SPMs make decisions by how they interpret their experiences in the workplace. Further, to identify antecedents and consequences of those decisions towards increasing the effectiveness of project management. We also aim to refine the research design for future investigations. Method: Semi-structured interviews were carried out with SPMs from a Brazilian large governmental organization. From the initial analysis we found that the phenomenon is context dependent and conducted interviews with SPMs from a Brazilian large private organization. Results: We found that decision-making in software project management is based on knowledge sharing in which the SPM acts as a facilitator before making decisions. This phenomenon is influenced by individual factors, such as experience, organizational ability, communication, negotiation, and systemic view of the project and by contextual factors such as the autonomy of the SPM and team members' technical competence. Also, these factors are mediated by cognitive biases. Conclusions: Due to the uncertainty and dynamism inherent to software projects, the SPMs focus on making, monitoring and adjusting decisions in an argument-driven way. From the initial relationships among the identified factors, the research design was refined.|None|accept|accept
ESEM|2016|Software Requirements Specification in Agile Projects: an Empirical Study|juliana medeiros,alexandre vasconcelos,carla silva,amanda sarmento|Requirements Specification,Agile Methods,Empirical studies|Background: Requirements specification is considered as one of the challenges that have compromised the adoption of agile methods. Empirical studies in industry point out that user stories have proved to be superficial specifications. Inadequate specification increases the effort required for coding, testing, and maintenance. Aims: The objective of this study was to evaluate a process called Requirements Agile Process (RAP) that uses a new technique to specify requirements in agile projects. This technique is called User Rules Specification (URS). Method: A mix method research composed of a systematic literature review (SLR) and an industrial case study were used to analyze the phenomenon of requirements specification in agile projects. The SLR analyzed 24 primary studies, from an initial set of 2.852 articles. The case study was conducted over 10 months, involving 7 projects and 20 participants. It was investigated the correlation between quality attributes, non-conformities and volatility of the URS. Results: The results pointed out five correlations statistically significant. The URS elaborated presented a partial compliance with the ISO-IEEE 830. The URS showed to be an objective specification, satisfying the needs of the developers and it did not compromise the agility of the process. Conclusions: The results obtained provided explanatory power about the requirements specification in agile projects using RAP. Some limitations related to the lack of automated support and the representation of non-functional requirements impair the adoption of RAP.|None|reject|reject
ESEM|2016|Understanding the Contribution of Non-source Documents in Improving Missing Link Recovery: An Empirical Study|yan sun,qing wang|Missing Link Recovery,Non-Source Documents,Mining Software Repositories,Software Maintenance|Background: Links between issue reports and their fixing commits play an important role in software maintenance. Such link data are often missing in practice and many approaches have been proposed in order to recover them automatically. Most of these approaches focus on comparing log messages and source code files in commits with issues reports. Besides the two kinds of data in commits, non-source documents (NSDs) such as change logs usually record the fixing activities and sometimes share similar texts as those in issue reports. However, few discussions have been made on the role of NSDs in designing link recovery approaches. Aims: This paper aims at understanding whether and how NSDs affect the performance of link recovery approaches. Method: An empirical study is conducted to evaluate the role of NSDs in link recovery approaches in 18 open source projects with 6370 issues and 22761 commits. Results: With the inclusion of NSDs, link recovery approaches can get an average increase in F-Measure ranging from 2.76% - 25.63%. Further examinations show NSDs contribute to the performance improvement in 15 projects and have exceptions in 3 projects. The performance improvement in the 15 projects is mainly from the filtering of noisy links. On average, 23.59% - 76.30% false links can be excluded by exploiting NSDs in the link recovery approach. We also analyze the 3 projects in which NSDs cannot improve the performance. Our finding shows sophisticated data selection in NSDs is necessary. Conclusions: Our preliminary findings demonstrate that involving NSDs can improve the performance of link recovery approaches in most cases.|None|accept|accept
ESEM|2016|Predicting Crashing Releases of Mobile Applications|xin xia,emad shihab,yasutaka kamei,david lo,xinyu wang|Mobile Applications,Crash Release,Prediction Model|The quality of mobile applications has a vital impact on their user's experience, ratings and ultimately overall success. Given the high competition in the mobile application market, i.e., many mobile applications perform the same or similar functionality, users of mobile apps tend to be less tolerant to quality issues. Prior work has shown that crashing mobile app updates, i.e., releases, tend to have a large negative impact on an app's user perceived quality. Therefore, identifying these crashing releases early on so that they can be avoided will help mobile app developers keep their user base and ensure the overall success of their apps. To help mobile developers, we use machine learning techniques to effectively predict mobile app releases that are more likely to cause crashes, i.e., crashing releases. To perform our prediction, we mine and use a number of factors about the mobile releases, that are grouped into six unique dimensions: complexity, time, code, diffusion, commit, and text. We perform an empirical study on 10 open source mobile applications containing a total of 2,638 releases from the F-Droid repository. On average, our approach can achieve F1 and AUC scores that improve over a baseline (random) predictor by 50% and 28%, respectively.  We also find that factors related to text extracted from the commit logs prior to a release are the best predictors of crashing releases and have the largest effect.|None|accept|accept
ESEM|2016|Relationship of Personality Traits with Contribution and Context – An Analysis of Software Development in GitHub|ayushi rastogi,nachiappan nagappan|Empirical Software Engineering,Personality Traits,Contribution,Software Development|Gauging personality traits is essential to comprehend the true nature of online, distributed software development. Such explorations help to understand the implications of contributors' behavior on software development activities and vice-versa, thereby facilitating preemptive measures. In this study, we empirically analyze the relationship of the contributors' personality traits with their level of contribution and the context of the software development in GitHub. Analyses of 243 popular, actively discussed projects suggest that contributors with different levels of contribution have different personality profiles. We find that unlike commercial projects, neuroticism increases with the level of contribution and reputation in GitHub. Analyses of the personality traits of 423 contributors, who actively engage in discussions in different contexts of software development, show marked differences. These differences in personality traits arise based on their role and reputation in the project, the platform to which they contribute and with time. We find that contributors as reviewers are less open to new ideas compared to contributors as reporters.|None|reject|reject
ESEM|2016|The Impact of Task Granularity on Co-evolution Analyses|keisuke miura,shane mcintosh,yasutaka kamei,ahmed e. hassan,naoyasu ubayashi|Work item,Task granularity,Co-change analyses|Substantial research in the software evolution field aims to recover knowledge about development from the project history that is archived in repositories, such as a Version Control System (VCS). However, the data that is archived in these repositories can be analyzed at different levels of granularity. Although software evolution is a well-studied phenomenon at the revision-level, revisions may be too fine-grained to accurately represent development tasks. In this paper, we set out to empirically study the characteristics of work items (i.e., logical groups of revisions that address a single issue) to understand the impact that fine-grained revision-level analyses have on co-change analyses. Through analysis of the VCSs of 14 open source projects that are developed under the Apache Software Foundation, we find that work item grouping has the potential to impact co-change activity, since 5-62 of work items consist of 2 or more revisions. A deeper quantitative analysis shows that: (1) 1-42 of largest work items are entirely composed of small revisions, which traditional approaches to filter or analyze large changes would like miss, (2) 48-97 of revisions that co-change under a single work item cannot be detected using the typical configuration of the sliding time window technique and (3) 14-57 of work items that involve multiple developers, which is a form of collaborative development that cannot be detected at the revision-level. Since the work item granularity is the natural means that practitioners use to separate development tasks, future software evolution studies, especially co-change analyses, should be conducted at the work item level.|None|accept|accept
ESEM|2016|A Controlled Experiment on the Impact of Requirement Granularity on the Effects of Test-driven Development|itir karac,burak turhan,natalia juristo juzgado,rahul mohanani|test-driven development,controlled experiment,requirement granularity,software quality,developer productivity|Context: Test-driven Development (TDD) is an iterative software development process characterized by test-code- refactor cycle. In each cycle, a micro feature is implemented by first writing the unit tests, then adding only necessary code to pass the tests and refactoring. Inexperienced devel- opers may not fully benefit from TDD if they cannot de- compose the task into small sub-tasks and work in micro development cycles. Objective: We aim to investigate the impact of granularity of the requirements on software quality and amount of work done by novice software developers in the context of TDD. Method: We conducted a one-factor crossover experiment with 48 students in the context of a graduate level academic course. Each subject implemented two similar tasks using TDD, described with coarse-grained and fine-grained user stories respectively. Resulting artifacts were evaluated with acceptance test suits and we analyzed the collected data us- ing linear mixed models. Results: Requirement granularity and task are found to be significant factors affecting software quality and amount of work delivered, whereas sequencing of treatments did not have any significant effect. Conclusion: The level of detail the tasks are presented is important with respect to the quality of the products and the work delivered by developers. The ability to have divide- and-conquer skills to break specifications into smaller work items is an important practical skill, at least in the con- text of TDD. Analysis and interpretation of the results of experiments should account for the potential impact of the experimental tasks.|None|reject|reject
ESEM|2016|Bottom-up Adoption of Continuous Delivery in a Stage-gate Managed Software Organization|eero laukkanen,timo o.a. lehtinen,juha itkonen,maria paasivaara,casper lassenius|continuous delivery,stage-gate process,case study|Background: Continuous delivery (CD) is a development practice for decreasing the time-to-market in modern software organizations by keeping software releasable all the time. However, the underlying problems related to CD adoption within a stage-gate development process have not been studied previously. Aims: We investigate the perceived problems of CD adoption in a large global software development unit at Nokia Networks. The adoption was bottom-up: driven by the unit, not by the product management. We especially focus on the stage-gate managed development process used in the unit and how it affects the adoption. Method: The overall research approach is a qualitative single case study on one of the several geographical sites of the development unit. We organized two 2-hour root cause analysis workshops with altogether 15 participants to discover the adoption problems and to explain why and how they occurred. Results: The stage-gate managed development process reduced the time available for the bottom-up CD adoption with three mechanisms: time pressure, process overhead and increased complexity. Time pressure was caused by tight scheduling, process overhead by gate requirements and increased complexity by the use of multiple version control branches for different stages in the process. Due to the lack of time, the adoption was not proceeding as well as expected. In addition, the process limited the available hardware resources and caused delayed integration. Conclusions: Adopting CD in a development organization that needs to conform to a stage-gate managed development process is challenging. Practitioners should either gain support from the management to relax the required process or reduce their expectations on what can be achieved while conforming to the process. To simplify the development process, the use of multiple version control branches could be replaced with feature toggles.|None|accept|accept
ESEM|2016|An Exploratory Study of The Bug and Fix Syntactic Patterns in Java Systems|haidar osman,mircea lungu,oscar nierstrasz|Software Evolution,Automatic Bug Detection,Mining Software Repositories,Bugg Patterns|Bug fixing is a key activity during software maintenance. Analyzing bug fixes is necessary to understand where, how, and when these bugs arise in the code and how programmers usually fix them. In this paper, we focus on finding out the file types that are fixed the most, the size of fixes, and recurrent bug-fix patterns by analyzing two separate corpora: the Java Popular projects corpus (717 projects) and the Eclipse Ecosystem corpus (756 projects). The results show that most bug fixes involve one line of code and the most frequent bugs are missing null checks and missing invocations. We show also that although most fixes affect Java files in Java systems as expected, a significant number of fixes are made to other types of files such as XML and JavaScript files.|None|reject|reject
ESEM|2016|Diagram Size vs. Layout Flaws: Understanding Quality Factors of UML Diagrams|harald storrle|UML Diagrams,Model Understanding,Diagram Size,Diagram Layout Quality|BACKGROUND: Previously, we have defined the notion of diagram size and studied its impact the understanding of design-level UML diagrams. Subsequently, questions have been raised regarding the reliability and generality of our findings. Also, new questions arose regarding how the quality of diagrams could be defined, and how size and quality interact. OBJECTIVE: We aim to improve our previous results in three ways. First, we increase the validity by repeating our analysis on a substantially larger and more diverse data set. Second, we broaden the scope by including more diagram types in our study. Third, we expand our understanding of the factors responsible for the understandability of models. METHOD: We re-analyze modeler performance data from previous experiments (14,000 data points, n = 156 participants). The experiments were based on 60 diagrams of the five most commonly used UML diagram types. We carefully study the structure of our diagram samples and determine their sizes and number of layout faults in an objective way. RESULTS: We confirm and refine earlier findings, and extend their scope to two new diagram types. We show that diagram size and diagram layout flaws are distinct factors, and quantify their impact on diagram understanding. These factors are necessary, but not sufficient in defining diagram quality, as we show by example. As by-products, we provide an improved definition of diagram size, and a (preliminary) notion of diagram quality in terms of layout faults.|None|accept|accept
ESEM|2016|A systematic Review of Scheduling Approaches on Multi-tenancy Cloud-Based Systems|jia ru,jacky w. keung|Multi-tenancy,Scheduling policies,Cloud computing,Resource allocation,SaaS|Multi-tenancy brings different scheduling approaches for cloud systems. Tenants have different requirements of hosted applications, including service function, response time, throughput, requiring different scheduling capabilities and occurred different resource consumption and competition. Different multi-tenancy scheduling approaches have been developed for different service models, such as Software as a Service (SaaS), Platform as a service (PaaS), Infrastructure as a Service (IaaS), and the new momentum Database as a Service (DBaaS). This paper provides a comprehensive review of research studies related to multi-tenancy scheduling on cloud systems to determine primary scheduling approaches and the challenge for multi-tenancy scheduling issues. We adopt systematic literature review method to search and review a large number of related journals and conference papers which are to addressed 4 pre-defined research questions on 4 major online databases. Defining inclusion and exclusion criteria was the initial step before extracting data from selected papers and derived answers addressing our inquires. Based on the inclusion and exclusion criteria, 24 papers were selected, of which 33 approaches are identified. Most of these methods are developed for SaaS (42.42%) and most resource provision approaches are employed on PaaS (36.36%). The results have demonstrated that most of multi-tenancy scheduling solutions are employed on SaaS (application layer) with concern of many aspects, such as data management, data placement, and some other approaches focus on datacenter layer, such as building database or data partition. With the difference of tenants' requirements and functionalities, the choice of cloud systems are changed. Based on our study, designing a multi-tenancy scheduling framework should consider the following 3 factors: computing, QoS and storage resource. Multi-tenancy scheduling approaches mostly consider load balancing problem and QoS parameters, such as response time through negotiating Service Level Agreements (SLA) violations and system performance. This study provides a comprehensive overview of different scheduling approaches dedicated multi-tenant based cloud systems, which is important for future development of multi-tenant based applications for cloud systems.|None|reject|reject
ESEM|2016|Understanding UML Diagrams: An Eye-Tracking study|harald storrle,nick baltsen,henrik christoffersen,anja maier|UML Diagrams,Diagram Understanding,Eye-Tracking|Background: Diagrams are widely used in Software Engineering. Yet, surprisingly little is known about the impact of diagram layout to model understanding. We have previously studied this relationship and presented evidence that layout quality, diagram size, and expertise level are relevant impact factors, while diagram type is not. Objective: Now that we have established the effect as such, we progress to study its cause: the behavior of modelers while reading diagrams. We hope to find which reading strategies for diagrams are most effective, which may or may not depend on diagram type or layout pattern. We hope this will also shed light on the cognitive process of diagram understanding. Method: We conduct an eye tracking study with 28 participants, sub-sampling the diagrams used in previous experiments. We record several objective and subjective performance indicators as well as eye movement and pupil dilation. Results: We discover behavioral regularities and aggregate them into reading strategies which vary with expertise levels, but not with layout quality. Diagram flaws such as line crossings impose the same amount of cognitive load as "regular" diagram elements. Conclusions: Modelers exhibit specific strategies of diagram understanding. Experts employ different strategies than novices, which explains performance differences irrespective of layout quality. These findings are relevant both for reading and creating diagrams and should be adopted by all modelers. These results likely apply to any visual notation in software engineering.|None|reject|reject
ESEM|2016|Building an Ensemble for Software Defect Prediction Based on Diversity Selection|jean petric,david bowes,tracy hall,bruce christianson,nathan baddoo|software defect prediction,software faults,ensembles of learning machines,stacking,diversity|Background: Ensemble techniques have gained attention in various scientific fields. Defect prediction researchers have investigated many state-of-the-art ensemble models and concluded that in many cases these outperform standard single classifier techniques. Almost all previous work using ensemble techniques in defect prediction rely on the majority voting scheme for combining prediction outputs, and on the implicit diversity among single classifiers. Aim: Investigate whether defect prediction can be improved using an explicit diversity technique with stacking ensemble, given the fact that different classifiers identify different sets of defects. Method: We used four different families of classifiers and the weighted accuracy diversity (WAD) technique to exploit diversity amongst classifiers. To combine individual predictions, we used the stacking ensemble technique. We used state-of-the-art knowledge in software defect prediction to build our ensemble models, and tested their prediction abilities against 8 publicly available data sets. Conclusion: The results show performance improvement using stacking ensembles compared to other defect prediction models. Diversity amongst classifiers used for building ensembles is essential to achieving these performance improvements.|None|accept|accept
ESEM|2016|Assessing Trends in Developers’ Collaboration Networks: A Large Scale Study of Open Source Software Projects|alma orucevic-alagic,nicklas johansson,christian tenggren,martin host|Open Source Software,Apache Software Foundation,Social network analysis,Network metrics,Committer|Software development is a collaborative effort and application of network analysis on developers' collaboration networks has been one approach used to study it.  However, previous studies have not applied the approach on a greater scale, i.e., on a large set of mature and industry used software projects of varying sizes and domains, for the purpose of uncovering possible patterns in the collaboration metrics. We argue that existence of such patterns in the network metrics can be indicative of preferred development collaboration topologies, which could be used to asses any software development effort. For the purpose of this study, 228 source code repositories hosted under the Apache Software Foundation have been mined in order to construct and study developers' collaboration networks based on the source code contributions made by the developers. We show that several interesting trends can be seen, such as the ones relating the size of the project to developers' betweenness and closeness centralities, and low variability of average clustering coefficients across all of the projects.  The results also show a high specialization of the majority of developers on a particular, smaller part, of the system, and the existence of very few expert developers that bind larger parts of system together across all of the projects, irrespective of their sizes and domains. Hence, the uncovered patterns might also be indicative of the most efficient way to develop software in an industry software development context and, thus, could be used to asses any development collaboration topology.|None|reject|reject
ESEM|2016|Relationship between Code Reading Speed and Programmers’ Age|yukasa murakami,masateru tsunoda,masahide nakamura|developer's performance,code reading,mental simulation|In the software development, some people point out that retirement age of the programmer is about 35. However, there is no quantitative analysis about the influence of the age to programmers’ ability. Also, if it affects to the ability, it is not clear which ability (e.g. memory) is affected by the age. In this study, we focus on the relationship between the age and the memory. For example, if there is a program which needs programmers’ memory to understand, older programmers need more time to understand it than younger programmer. In the experiment, we prepared two programs which need more memory and less memory to understand them. We measured time for younger subjects and older subjects to read them. The result suggested that when a program needs more memory to understand it, older subjects needs more time to understand it.|None|reject|reject
ESEM|2016|An investigation on engineering the software quality requirements in an empirical context|reza mirsalari,jacques masson,sarah gibson,pierre n. robillard|Software Engineering,Quality assurance,ISO/IEC 25000,Survey|The main objective of this study is to investigate the feasibility of the elicitation and quantification of users’ quality requirements using the model presented in ISO/IEC 25000. For this purpose, we established two working questionnaires for end users and power users. The questionnaires were filled out by the users of a software product, which was in the development phase. The answers were collected and analyzed. The results reveal that both end users and power users have the same perspective on the importance of certain quality factors. The quality profiles which are the results of this investigation empower the software project and product managers to devote attention to the quality characteristics that are important from the users’ point of view. This study suggests the need for a crosscheck between quality profiles and required artifacts during the development process to validate that the activities to support the users’ quality requirements derived from quality profiles are appropriate.|None|reject|reject
ESEM|2016|Using Object-Oriented Metrics to Predict OSGi Component-Based Applications Defects|salma hamza,salah sadou,regis fleurquin,lucas bueno r. oliveira|Software Component,Software Quality,Defect Prediction Model,Object-Oriented Metrics|Component-Based Development (CBD) has been claimed as a step toward quality in large software systems. However, unlike the traditional Object-Oriented (OO) development, CBD paradigm still lacks of internal metrics able to control and predict quality characteristics. In this paper, we show that some well-known OO metrics can be used to build defect prediction models for component-based software systems. Thus, we applied consolidated OO code metrics on six large open-source systems developed using OSGi framework in order to create models able to predict defects. These prediction models were created using Principal Component Analysis and Multivariate Logistic Regression. Results of our study pointed out that the built models can predict 80% to 92% of frequently defective components with recall ranges between 89% and 98%, depending on the evaluated project. By the way, it is possible to conclude that the considered OO metrics are able to successfully predict some external quality properties in component-based systems.|None|reject|reject
